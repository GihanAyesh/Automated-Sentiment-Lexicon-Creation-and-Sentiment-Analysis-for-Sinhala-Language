{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment_Tagger_slstm_with_fasttext_with_save_crossVal.ipynb","provenance":[],"collapsed_sections":["W1kYrhsN6xEs","B4wf4YZ9RUW-","0n2fJGeq63I7","6w_fbBM2Qp1O","XeFsOVyYQ7y6","BZa7J54XRvwG","ddbrDIW8SsV4"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"t48aRicQOinf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"28f5efc5-e481-43f6-ab96-d8452b6e8378","executionInfo":{"status":"ok","timestamp":1583748602757,"user_tz":-330,"elapsed":1992,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bG9mz-zbS8Rt","colab_type":"code","colab":{}},"source":["path='/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/data_set'\n","vector_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/fasttext_vectors'\n","# run from lahiru1st@gmail.com\n","# path='/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/data_set'\n","# vector_path = '/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/fasttext_vectors'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwA4weGRP9OQ","colab_type":"text"},"source":["# Imports"]},{"cell_type":"markdown","metadata":{"id":"W1kYrhsN6xEs","colab_type":"text"},"source":["## Standard Imports"]},{"cell_type":"code","metadata":{"id":"OXwBif125t0Y","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","# import six.moves.cPickle as pickle\n","# from collections import OrderedDict\n","import sys\n","import time\n","import numpy as np\n","import tensorflow as tf\n","# import read_data\n","# from random import shuffle\n","# import random\n","import pickle\n","# import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import confusion_matrix\n","# from general_utils import Progbar\n","# import tensorflow.contrib.slim as slim\n","# from sst_config import Config\n","from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0T3y_TelRPlY","colab_type":"text"},"source":["## config"]},{"cell_type":"code","metadata":{"id":"kiZjosbeRMm-","colab_type":"code","colab":{}},"source":["class Config(object):\n","    vocab_size=15000\n","    max_grad_norm = 5\n","    init_scale = 0.05\n","    hidden_size = 300\n","    lr_decay = 0.95\n","    valid_portion=0.0\n","    batch_size=5\n","    keep_prob = 0.5\n","    #0.05\n","    learning_rate = 0.001\n","    max_epoch =2\n","    # max_max_epoch =40\n","    max_max_epoch = 30\n","    num_label=5\n","    attention_iteration=3\n","    random_initialize=False\n","    embedding_trainable=True\n","    l2_beta=0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4wf4YZ9RUW-","colab_type":"text"},"source":["## Progbar"]},{"cell_type":"code","metadata":{"id":"HQxItjCIPrgH","colab_type":"code","colab":{}},"source":["import time\n","import sys\n","import logging\n","import numpy as np\n","\n","\n","class Progbar(object):\n","    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n","\n","    Displays a progress bar.\n","    Small edit : added strict arg to update\n","    # Arguments\n","        target: Total number of steps expected.\n","        interval: Minimum visual progress update interval (in seconds).\n","    \"\"\"\n","\n","    def __init__(self, target, width=30, verbose=0):\n","        self.width = width\n","        self.target = target\n","        self.sum_values = {}\n","        self.unique_values = []\n","        self.start = time.time()\n","        self.total_width = 0\n","        self.seen_so_far = 0\n","        self.verbose = verbose\n","\n","    def update(self, current, values=[], exact=[], strict=[]):\n","        \"\"\"\n","        Updates the progress bar.\n","        # Arguments\n","            current: Index of current step.\n","            values: List of tuples (name, value_for_last_step).\n","                The progress bar will display averages for these values.\n","            exact: List of tuples (name, value_for_last_step).\n","                The progress bar will display these values directly.\n","        \"\"\"\n","\n","        for k, v in values:\n","            if k not in self.sum_values:\n","                self.sum_values[k] = [v * (current - self.seen_so_far),\n","                                      current - self.seen_so_far]\n","                self.unique_values.append(k)\n","            else:\n","                self.sum_values[k][0] += v * (current - self.seen_so_far)\n","                self.sum_values[k][1] += (current - self.seen_so_far)\n","        for k, v in exact:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = [v, 1]\n","\n","        for k, v in strict:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = v\n","\n","        self.seen_so_far = current\n","\n","        now = time.time()\n","        if self.verbose == 1:\n","            prev_total_width = self.total_width\n","            sys.stdout.write(\"\\b\" * prev_total_width)\n","            sys.stdout.write(\"\\r\")\n","\n","            numdigits = int(np.floor(np.log10(self.target))) + 1\n","            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n","            bar = barstr % (current, self.target)\n","            prog = float(current)/self.target\n","            prog_width = int(self.width*prog)\n","            if prog_width > 0:\n","                bar += ('='*(prog_width-1))\n","                if current < self.target:\n","                    bar += '>'\n","                else:\n","                    bar += '='\n","            bar += ('.'*(self.width-prog_width))\n","            bar += ']'\n","            sys.stdout.write(bar)\n","            self.total_width = len(bar)\n","\n","            if current:\n","                time_per_unit = (now - self.start) / current\n","            else:\n","                time_per_unit = 0\n","            eta = time_per_unit*(self.target - current)\n","            info = ''\n","            if current < self.target:\n","                info += ' - ETA: %ds' % eta\n","            else:\n","                info += ' - %ds' % (now - self.start)\n","            for k in self.unique_values:\n","                if type(self.sum_values[k]) is list:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                else:\n","                    info += ' - %s: %s' % (k, self.sum_values[k])\n","\n","            self.total_width += len(info)\n","            if prev_total_width > self.total_width:\n","                info += ((prev_total_width-self.total_width) * \" \")\n","\n","            sys.stdout.write(info)\n","            sys.stdout.flush()\n","\n","            if current >= self.target:\n","                sys.stdout.write(\"\\n\")\n","\n","        if self.verbose == 2:\n","            if current >= self.target:\n","                info = '%ds' % (now - self.start)\n","                for k in self.unique_values:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                sys.stdout.write(info + \"\\n\")\n","\n","    def add(self, n, values=[]):\n","        self.update(self.seen_so_far+n, values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0n2fJGeq63I7","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"code","metadata":{"id":"_pSHY_uE66j1","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","from six.moves import xrange\n","import six.moves.cPickle as pickle\n","import gzip\n","import os\n","import numpy\n","\n","def generate_matrix(seqs, maxlen, lengths):\n","    n_samples = len(seqs)\n","    x= numpy.zeros((n_samples, maxlen)).astype('int64')\n","\n","    for idx, s in enumerate(seqs):\n","        if lengths[idx]>= maxlen:\n","            s=s[:maxlen]\n","        x[idx, :lengths[idx]] = s\n","    return x\n","\n","def prepare_data(seqs, labels):\n","    lengths = [len(s) for s in seqs]\n","    labels = numpy.array(labels).astype('int32')\n","    return [numpy.array(seqs), labels, numpy.array(lengths).astype('int32')]\n","\n","def remove_unk(x, n_words):\n","    return [[1 if w >= n_words else w for w in sen] for sen in x] \n","\n","def load_data(path, n_words):\n","    with open(path, 'rb') as f:\n","        dataset_x, dataset_label= pickle.load(f)\n","        train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","        # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","        test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","    train_set_x = remove_unk(train_set_x, n_words)\n","    # valid_set_x = remove_unk(valid_set_x, n_words)\n","    test_set_x = remove_unk(test_set_x, n_words)\n","\n","    return [train_set_x, train_set_y],[test_set_x, test_set_y]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_1MavSboWVk","colab_type":"code","colab":{}},"source":["def load_data_for_crossVal(path, n_words):\n","  with open(path, 'rb') as f:\n","    dataset_x, dataset_label= pickle.load(f)\n","    train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","    # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","    test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","  train_set_x = remove_unk(train_set_x, n_words)\n","  # valid_set_x = remove_unk(valid_set_x, n_words)\n","  test_set_x = remove_unk(test_set_x, n_words)\n","\n","  \n","\n","  train_set_x.extend(test_set_x)\n","  dataset_label[0].extend( dataset_label[1])\n","\n","  return train_set_x,dataset_label[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEgTqwtlQanA","colab_type":"text"},"source":["# Implementation"]},{"cell_type":"markdown","metadata":{"id":"lQArWlWmQgCQ","colab_type":"text"},"source":["## LSTM Layer"]},{"cell_type":"code","metadata":{"id":"9chxIjuYONVw","colab_type":"code","colab":{}},"source":["def lstm_layer(initial_hidden_states, config, keep_prob, mask):\n","    with tf.variable_scope('forward'):\n","        fw_lstm = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        fw_lstm = tf.contrib.rnn.DropoutWrapper(fw_lstm, output_keep_prob=keep_prob)\n","\n","    with tf.variable_scope('backward'):\n","        bw_lstm = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        bw_lstm = tf.contrib.rnn.DropoutWrapper(bw_lstm, output_keep_prob=keep_prob)\n","\n","    #bidirectional rnn\n","    with tf.variable_scope('bilstm'):\n","        lstm_output=tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, inputs=initial_hidden_states, sequence_length=mask, time_major=False, dtype=tf.float32)\n","        lstm_output=tf.concat(lstm_output[0], 2)\n","\n","    return lstm_output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTmP1ZQbQlTv","colab_type":"text"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"Z3LlPobkOq1_","colab_type":"code","colab":{}},"source":["class Classifer(object):\n","\n","    def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,:-step,:]\n","        #concat padding\n","        return tf.concat([padding, displaced_hidden_states], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,step:,:]\n","        #concat padding\n","        return tf.concat([displaced_hidden_states, padding], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def sum_together(self, l):\n","        combined_state=None\n","        for tensor in l:\n","            if combined_state==None:\n","                combined_state=tensor\n","            else:\n","                combined_state=combined_state+tensor\n","        return combined_state\n","    \n","    def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n","        with tf.name_scope(name_scope_name):\n","            #Word parameters \n","            #forget gate for left \n","            with tf.name_scope(\"f1_gate\"):\n","                #current\n","                Wxf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                #left right\n","                Whf1 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                #initial state\n","                Wif1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                #dummy node\n","                Wdf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for right \n","            with tf.name_scope(\"f2_gate\"):\n","                Wxf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf2 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for inital states     \n","            with tf.name_scope(\"f3_gate\"):\n","                Wxf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf3 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for dummy states     \n","            with tf.name_scope(\"f4_gate\"):\n","                Wxf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf4 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #input gate for current state     \n","            with tf.name_scope(\"i_gate\"):\n","                Wxi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n","                Whi = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n","                Wii = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n","                Wdi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n","            #input gate for output gate\n","            with tf.name_scope(\"o_gate\"):\n","                Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                Who = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","                Wio = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n","                Wdo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n","            #bias for the gates    \n","            with tf.name_scope(\"biases\"):\n","                bi = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                bf1 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n","                bf2 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n","                bf3 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n","                bf4 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n","\n","            #dummy node gated attention parameters\n","            #input gate for dummy state\n","            with tf.name_scope(\"gated_d_gate\"):\n","                gated_Wxd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                gated_Whd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","            #output gate\n","            with tf.name_scope(\"gated_o_gate\"):\n","                gated_Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Who = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #forget gate for states of word\n","            with tf.name_scope(\"gated_f_gate\"):\n","                gated_Wxf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Whf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #biases\n","            with tf.name_scope(\"gated_biases\"):\n","                gated_bd = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                gated_bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                gated_bf = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","\n","        #filters for attention        \n","        mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n","        mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, dim=2)               \n","        #filter invalid steps\n","        sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n","        #filter embedding states\n","        initial_hidden_states=initial_hidden_states*sequence_mask\n","        initial_cell_states=initial_cell_states*sequence_mask\n","        #record shape of the batch\n","        shape=tf.shape(initial_hidden_states)\n","        \n","        #initial embedding states\n","        embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n","        embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","        #randomly initialize the states\n","        if config.random_initialize:\n","            initial_hidden_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            initial_cell_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            #filter it\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","        #inital dummy node states\n","        dummynode_hidden_states=tf.reduce_mean(initial_hidden_states, axis=1)\n","        dummynode_cell_states=tf.reduce_mean(initial_cell_states, axis=1)\n","\n","        for i in range(num_layers):\n","            #update dummy node states\n","            #average states\n","            combined_word_hidden_state=tf.reduce_mean(initial_hidden_states, axis=1)\n","            reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            #copy dummy states for computing forget gate\n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            #input gate\n","            gated_d_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n","            )\n","            #output gate\n","            gated_o_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n","            )\n","            #forget gate for hidden states\n","            gated_f_t = tf.nn.sigmoid(\n","                tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n","            )\n","\n","            #softmax on each hidden dimension \n","            reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n","            gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, dim=1)], axis=1), dim=1)\n","            #split the softmax scores\n","            new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n","            new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n","            #new dummy states\n","            dummy_c_t=tf.reduce_sum(new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n","            dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n","\n","            #update word node states\n","            #get states before\n","            initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n","            initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n","            #get states after\n","            initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_before=self.sum_together(initial_cell_states_before)\n","            initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_after=self.sum_together(initial_cell_states_after)\n","            \n","            #reshape for matmul\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","            #concat before and after hidden states\n","            concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n","\n","            #copy dummy node states \n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","\n","            f1_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n","                tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n","            )\n","\n","            f2_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n","                tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n","            )\n","\n","            f3_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n","                tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n","            )\n","\n","            f4_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n","                tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n","            )\n","            \n","            i_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n","                tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n","            )\n","            \n","            o_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n","                tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n","            )\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n","\n","\n","            five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n","            five_gates=tf.nn.softmax(five_gates, dim=1)\n","            f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n","\n","            c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n","            \n","            h_t = o_t * tf.nn.tanh(c_t)\n","\n","            #update states\n","            initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n","            initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","            dummynode_hidden_states=dummy_h_t\n","            dummynode_cell_states=dummy_c_t\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,self.dropout)\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, self.dropout)\n","\n","        return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n","\n","\n","\n","    def __init__(self, config, session):\n","        #inputs: features, mask, keep_prob, labels\n","        self.input_data = tf.placeholder(tf.int32, [None, None], name=\"inputs\")\n","        self.labels=tf.placeholder(tf.int64, [None,], name=\"labels\")\n","        self.mask=tf.placeholder(tf.int32, [None,], name=\"mask\")\n","        self.dropout=self.keep_prob=keep_prob=tf.placeholder(tf.float32, name=\"keep_prob\")\n","        self.config=config\n","        shape=tf.shape(self.input_data)\n","        #if sys.argv[4]=='lstm':\n","        #    self.dummy_input = tf.placeholder(tf.float32, [None, None], name=\"dummy\")\n","        #embedding\n","        self.embedding=embedding = tf.Variable(tf.random_normal([config.vocab_size, config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"embedding\", trainable=config.embedding_trainable)\n","        #apply embedding\n","        initial_hidden_states=tf.nn.embedding_lookup(embedding, self.input_data)\n","        initial_cell_states=tf.identity(initial_hidden_states)\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,keep_prob)\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, keep_prob)\n","\n","        #create layers \n","        if argument4=='slstm':\n","            new_hidden_states,new_cell_state, dummynode_hidden_states=self.slstm_cell(\"word_slstm\", config.hidden_size,self.mask, initial_hidden_states, initial_cell_states, config.layer)\n","            \n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            #representation=dummynode_hidden_states\n","            representation=tf.reduce_mean(tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n","            \n","            softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","\n","        elif argument4=='lstm':\n","            initial_hidden_states=lstm_layer(initial_hidden_states,config, self.keep_prob, self.mask)\n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            representation=tf.reduce_sum(initial_hidden_states,axis=1)\n","            config.hidden_size_sum=2*config.hidden_size\n","        elif argument4=='cnn':\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, 700, config.hidden_size])            \n","            initial_hidden_states = tf.expand_dims(initial_hidden_states, -1)\n","            pooled_outputs = []\n","            for i, filter_size in enumerate([3]):\n","                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                    # Convolution Layer\n","                    filter_shape = [filter_size, config.hidden_size, 1, config.hidden_size]\n","                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","                    b = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b\")\n","\n","                    W2 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W2\")\n","                    b2 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b2\")\n","\n","                    W3 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W3\")\n","                    b3 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b3\")\n","\n","                    W4 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W4\")\n","                    b4 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b4\")\n","\n","                    conv = tf.nn.conv2d(\n","                        initial_hidden_states,\n","                        W,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv\")\n","                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                    print(h.get_shape())\n","                    h=tf.transpose(h, [0,1,3,2])\n","                    # Apply nonlinearity\n","\n","\n","                    conv2 = tf.nn.conv2d(\n","                        h,\n","                        W2,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv2\")\n","                    h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n","                    print(h2.get_shape())\n","                    h2=tf.transpose(h2, [0,1,3,2])\n","\n","                    conv3 = tf.nn.conv2d(\n","                        h2,\n","                        W3,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv3\")  \n","                    h3 = tf.nn.relu(tf.nn.bias_add(conv3, b3), name=\"relu3\")\n","                    print(h3.get_shape())\n","\n","                    # Max-pooling over the outputs\n","                    pooled = tf.nn.max_pool(\n","                        h3,\n","                        ksize=[1, 700 - 3*filter_size + 3, 1, 1],\n","                        strides=[1, 1, 1, 1],\n","                        padding='VALID',\n","                        name=\"pool\")\n","                    pooled_outputs.append(pooled)\n","            # Combine all the pooled features\n","            num_filters_total = 1 * config.hidden_size\n","            self.h_pool = tf.concat(pooled_outputs, axis=3)\n","            representation = tf.reshape(self.h_pool, [-1, num_filters_total])\n","\n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","\n","            softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","        else:\n","            print(\"Invalid model\")\n","            exit(1)\n","        \n","        self.logits=logits = tf.matmul(representation, softmax_w) + softmax_b\n","        self.to_print=tf.nn.softmax(logits)\n","        #operators for prediction\n","        self.prediction=prediction=tf.argmax(logits,1)\n","        correct_prediction = tf.equal(prediction, self.labels)\n","        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n","        \n","        #cross entropy loss\n","        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits)\n","        self.cost=cost=tf.reduce_mean(loss)+ config.l2_beta*tf.nn.l2_loss(embedding)\n","\n","        #designate training variables\n","        tvars=tf.trainable_variables()\n","        self.lr = tf.Variable(0.0, trainable=False)\n","        grads=tf.gradients(cost, tvars)\n","        grads, _ = tf.clip_by_global_norm(grads,config.max_grad_norm)\n","        self.grads=grads\n","        optimizer = tf.train.AdamOptimizer(config.learning_rate)        \n","        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","    #assign value to learning rate\n","    def assign_lr(self, session, lr_value):\n","        session.run(tf.assign(self.lr, lr_value))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6w_fbBM2Qp1O","colab_type":"text"},"source":["## get_minibatches_idx()"]},{"cell_type":"code","metadata":{"id":"YzblKztaO-OK","colab_type":"code","colab":{}},"source":["def get_minibatches_idx(n, batch_size, shuffle=True):\n","    idx_list = np.arange(n, dtype=\"int32\")\n","\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","\n","    minibatches = []\n","    minibatch_start = 0\n","    for i in range(n // batch_size):\n","        minibatches.append(idx_list[minibatch_start:\n","                                    minibatch_start + batch_size])\n","        minibatch_start += batch_size\n","    if (minibatch_start != n):\n","        # Make a minibatch out of what is left\n","        minibatches.append(idx_list[minibatch_start:])\n","    return minibatches\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxLb_HGZQwM_","colab_type":"text"},"source":["## run_epoch"]},{"cell_type":"code","metadata":{"id":"uhuVbz60PN2n","colab_type":"code","colab":{}},"source":["\n","def run_epoch(session, config, model, data, eval_op, keep_prob, is_training):\n","    n_samples = len(data[0])\n","    print(\"Running %d samples:\"%(n_samples))  \n","    minibatches = get_minibatches_idx(n_samples, config.batch_size, shuffle=False)\n","\n","    predictions = []\n","    correct = 0.\n","    total = 0\n","    total_cost=0\n","    prog = Progbar(target=len(minibatches))\n","    #dummynode_hidden_states_collector=np.array([[0]*config.hidden_size])\n","\n","    to_print_total=np.array([[0]*2])\n","    for i, inds in enumerate(minibatches):\n","        x = data[0][inds]\n","        if argument4=='cnn':\n","            x=pad_sequences(x, maxlen=700, dtype='int32',padding='post', truncating='post', value=0.)\n","        else:\n","            x=pad_sequences(x, maxlen=None, dtype='int32',padding='post', truncating='post', value=0.)\n","        y = data[1][inds]\n","        mask = data[2][inds]\n","\n","\n","\n","        count, _, cost, to_print,prediction= \\\n","        session.run([model.accuracy, eval_op,model.cost, model.to_print,model.prediction],\\\n","            {model.input_data: x, model.labels: y, model.mask:mask, model.keep_prob:keep_prob}) \n","        \n","\n","        if not is_training:\n","            to_print_total=np.concatenate((to_print_total, to_print),axis=0)\n","        correct += count \n","        total += len(inds)\n","        total_cost+=cost\n","        predictions.extend(prediction.tolist())\n","        prog.update(i + 1, [(\"train loss\", cost)])\n","    #if not is_training:\n","    #    print(to_print_total[:, 0].tolist())\n","    #    print(data[1].tolist())\n","    #    print(data[2].tolist())\n","\n","    actual = data[1]\n","\n","    TN, FP, FN, TP = confusion_matrix(actual, predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","    print(\"Total loss:\")\n","    print(total_cost)\n","\n","    accuracy = correct/total\n","\n","    return accuracy,precision,recall,f1, actual, predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnwxV-GHQ2EP","colab_type":"text"},"source":["## train_test_model"]},{"cell_type":"code","metadata":{"id":"kWuJ7_ZwPQlg","colab_type":"code","colab":{}},"source":["def train_test_model(config, i, session, model, train_dataset,test_dataset):\n","  \n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","    #training            \n","    print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\n","    start_time = time.time()\n","    train_acc, train_precision, train_recall, train_f1,actual,predictions = run_epoch(session, config, model, train_dataset, model.train_op, config.keep_prob, True)\n","    print(\"Training Accuracy = %.4f, time = %.3f seconds\\n\"%(train_acc, time.time()-start_time))\n","\n","    #testing\n","    start_time = time.time()\n","    test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    \n","\n","    return test_actual,test_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5GQEegkQ5DA","colab_type":"text"},"source":["## start_epoches"]},{"cell_type":"code","metadata":{"id":"pYlt-2iEPUpE","colab_type":"code","colab":{}},"source":["def start_epoches(config, session,classifier, train_dataset, test_dataset):\n","    all_actual = []\n","    all_predictions = []\n","\n","    for i in range(config.max_max_epoch):\n","      test_actual,test_predictions = train_test_model(config, i, session, classifier, train_dataset, test_dataset)\n","      \n","      all_actual.extend(test_actual)\n","      all_predictions.extend(test_predictions)\n","\n","    return all_actual, all_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XeFsOVyYQ7y6","colab_type":"text"},"source":["## word_to_vec"]},{"cell_type":"markdown","metadata":{"id":"w3kBeQDxRgKl","colab_type":"text"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"dFvOyWeoPYAn","colab_type":"code","colab":{}},"source":["def word_to_vec(matrix, session,config, *args):\n","    \n","    print(\"word2vec shape: \", matrix.shape)\n","    \n","    for model in args:\n","        session.run(tf.assign(model.embedding, matrix))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZa7J54XRvwG","colab_type":"text"},"source":["## configs"]},{"cell_type":"code","metadata":{"id":"ifEZIahRRjEl","colab_type":"code","outputId":"29582b16-767a-4c03-9a93-da9f18f205ee","executionInfo":{"status":"ok","timestamp":1583748605513,"user_tz":-330,"elapsed":4356,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["argument1 = \"7\"\n","argument2 = \"2\"\n","argument3 = \"sinhala_news\"\n","argument4 = \"slstm\"\n","\n","config = Config()\n","config.layer=int(argument1)\n","config.step=int(argument2)\n","config.vocab_size=(18413) # number of words in fastText model\n","print(\"dataset: \"+argument3)\n","print(\"iteration: \"+str(config.layer))\n","print(\"step: \"+str(config.step))\n","print(\"model: \"+str(argument4))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["dataset: sinhala_news\n","iteration: 7\n","step: 2\n","model: slstm\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ddbrDIW8SsV4","colab_type":"text"},"source":["## Word2Vec"]},{"cell_type":"code","metadata":{"id":"K0aqSa1BSvYt","colab_type":"code","outputId":"2463baeb-5a48-4e9c-fad9-e33054db670c","executionInfo":{"status":"ok","timestamp":1583748610985,"user_tz":-330,"elapsed":9815,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["f = open(vector_path, 'rb')\n","matrix= np.array(pickle.load(f))\n","config.vocab_size=matrix.shape[0]\n","print(config.vocab_size)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["18415\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2HH4D-C7V77w","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5XWgKamvxE_","colab_type":"code","colab":{}},"source":["# from gensim.models import word2vec\n","# model = word2vec.Word2Vec.load(vector_path) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3yCllIB6Im-","colab_type":"text"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"id":"seazfYMrhgh5","colab_type":"code","colab":{}},"source":["data_x, data_y = load_data_for_crossVal(path=path,n_words=config.vocab_size)\n","config.num_label= 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7fk98v_6LUT","colab_type":"code","outputId":"98c6a5a0-7019-4ccc-d978-1d7ad5b3d88f","executionInfo":{"status":"ok","timestamp":1583748611602,"user_tz":-330,"elapsed":10362,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","print(\"number label: \"+str(config.num_label))\n","all_dataset = prepare_data(data_x,  data_y)\n","# valid_dataset = prepare_data(valid_dataset[0], valid_dataset[1])\n","# test_dataset = prepare_data(test_dataset[0], test_dataset[1])\n","\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["number label: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYnmpOWR25Lp","colab_type":"code","outputId":"80aee2b4-78f4-4925-b9c1-d566c62df337","executionInfo":{"status":"ok","timestamp":1583748611604,"user_tz":-330,"elapsed":10352,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["print(all_dataset)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["[array([list([133, 105, 445, 19, 207, 2, 6, 4429, 12, 4430, 1533, 375, 77, 160, 33, 7, 10, 423, 17, 2088, 155, 10, 3205]),\n","       list([204, 26]),\n","       list([318, 180, 4431, 303, 7096, 181, 2512, 7097, 7, 4432, 682, 7098, 723, 48, 7099, 7100, 2513, 4433, 1534, 887, 772, 7101, 7102, 1337, 7103, 1338, 7104, 7105, 34, 150, 127]),\n","       ..., list([298, 6382, 3161]), list([313, 6469, 52, 8, 243]),\n","       list([1222, 60, 147, 9, 14, 281, 3, 27, 10, 2, 6, 1132, 6689, 40])],\n","      dtype=object), array([1, 0, 1, ..., 1, 1, 1], dtype=int32), array([23,  2, 31, ...,  3,  5, 14], dtype=int32)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vuF3jIbg4Q4b","colab_type":"text"},"source":["# Train NN"]},{"cell_type":"code","metadata":{"id":"sZn_oDbv7sWV","colab_type":"code","colab":{}},"source":["def train_nn(train_dataset, test_dataset,i) :\n","  all_actual = []\n","  all_predictions = []\n","\n","  with tf.Graph().as_default(), tf.Session() as session:\n","      initializer = tf.random_normal_initializer(0, 0.05)\n","\n","      classifier= Classifer(config=config, session=session)\n","      saver = tf.train.Saver()\n","\n","      # total=0\n","      # #print trainable variables\n","      # for v in tf.trainable_variables():\n","      #     print(v.name)\n","      #     shape=v.get_shape()\n","      #     try:\n","      #         size=shape[0].value*shape[1].value\n","      #     except:\n","      #         size=shape[0].value\n","      #     total+=size\n","      # print(total)\n","\n","      #initialize\n","      init = tf.global_variables_initializer()\n","\n","      session.run(init)\n","      #train test model\n","\n","      # print (\"model_test\",matrix)\n","\n","      word_to_vec(matrix, session,config, classifier)\n","      all_actual, all_predictions = start_epoches(config, session,classifier, train_dataset,test_dataset)\n","\n","      # if(i%10 ==0 ):\n","      #   save_path = saver.save(session, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/cross_validated/1/\")\n","      #   print(\"Model saved in path: %s\" % save_path)\n","\n","  return all_actual, all_predictions \n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljEdTr622OUT","colab_type":"code","outputId":"f18b14ca-6755-4689-f6ca-e1b2409e898a","executionInfo":{"status":"ok","timestamp":1583757580997,"user_tz":-330,"elapsed":7152320,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["kf = KFold(n_splits=10)\n","kf.get_n_splits(all_dataset[0])\n","i = 1\n","\n","final_actual = []\n","final_predictions = []\n","\n","\n","\n","for train_index, test_index in kf.split(all_dataset[0]):\n","  print(\"Cross Validation step : \",i)\n","  \n","  train_data_comments, test_data_comments = all_dataset[0][train_index], all_dataset[0][test_index]\n","  train_data_labels, test_data_labels = all_dataset[1][train_index], all_dataset[1][test_index]\n","  train_lengths, test_lengths = all_dataset[2][train_index], all_dataset[2][test_index]\n"," \n","  train_dataset = [train_data_comments,train_data_labels,train_lengths]\n","  test_dataset = [test_data_comments,test_data_labels,test_lengths]\n","\n","  tf.reset_default_graph()\n","\n","  actual , prediction = train_nn(train_dataset, test_dataset,i)\n","\n","  final_actual.extend(actual)\n","  final_predictions.extend(prediction)\n","\n","  i += 1\n","\n","TN, FP, FN, TP = confusion_matrix(final_actual, final_predictions).ravel()\n","\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","f1 = 2 * precision * recall / (precision + recall)\n","accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","print(\"final Accuracy : \",accuracy)\n","print(\"final precision : \",precision)\n","print(\"final recall : \",recall)\n","print(\"final f1 : \",f1)\n","\n","\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Cross Validation step :  1\n","WARNING:tensorflow:From <ipython-input-26-7fa0982e3379>:256: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use the `axis` argument instead\n","WARNING:tensorflow:From <ipython-input-26-7fa0982e3379>:150: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","dim is deprecated, use axis instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","497.2366236820817\n","Training Accuracy = 0.7758, time = 31.751 seconds\n","\n","Running 501 samples:\n","Total loss:\n","56.64512148872018\n","Test Accuracy = 0.7305, Test Precision = 0.6517, Test Recall = 0.9547, Test F1 = 0.7746\n","\n","Time = 1.561 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","302.08763529197313\n","Training Accuracy = 0.8663, time = 27.399 seconds\n","\n","Running 501 samples:\n","Total loss:\n","38.43437340419041\n","Test Accuracy = 0.8184, Test Precision = 0.7621, Test Recall = 0.9095, Test F1 = 0.8293\n","\n","Time = 1.528 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","166.33400402662008\n","Training Accuracy = 0.9284, time = 27.265 seconds\n","\n","Running 501 samples:\n","Total loss:\n","62.71245579904371\n","Test Accuracy = 0.8343, Test Precision = 0.7899, Test Recall = 0.8971, Test F1 = 0.8401\n","\n","Time = 1.578 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","102.12038813767896\n","Training Accuracy = 0.9599, time = 27.385 seconds\n","\n","Running 501 samples:\n","Total loss:\n","66.30206054914925\n","Test Accuracy = 0.8543, Test Precision = 0.8172, Test Recall = 0.9012, Test F1 = 0.8571\n","\n","Time = 1.633 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","69.93770113353014\n","Training Accuracy = 0.9794, time = 27.195 seconds\n","\n","Running 501 samples:\n","Total loss:\n","69.74177742503821\n","Test Accuracy = 0.8543, Test Precision = 0.8373, Test Recall = 0.8683, Test F1 = 0.8525\n","\n","Time = 1.600 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","56.48009809717764\n","Training Accuracy = 0.9836, time = 27.933 seconds\n","\n","Running 501 samples:\n","Total loss:\n","86.42142487326691\n","Test Accuracy = 0.8343, Test Precision = 0.7817, Test Recall = 0.9136, Test F1 = 0.8425\n","\n","Time = 1.553 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","27.243600279583923\n","Training Accuracy = 0.9914, time = 27.454 seconds\n","\n","Running 501 samples:\n","Total loss:\n","85.40357335601388\n","Test Accuracy = 0.8603, Test Precision = 0.8340, Test Recall = 0.8889, Test F1 = 0.8606\n","\n","Time = 1.641 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","34.04903474788796\n","Training Accuracy = 0.9907, time = 27.241 seconds\n","\n","Running 501 samples:\n","Total loss:\n","105.28189999326409\n","Test Accuracy = 0.8703, Test Precision = 0.8938, Test Recall = 0.8313, Test F1 = 0.8614\n","\n","Time = 1.628 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","27.155084751164267\n","Training Accuracy = 0.9927, time = 27.427 seconds\n","\n","Running 501 samples:\n","Total loss:\n","129.41120464050383\n","Test Accuracy = 0.8643, Test Precision = 0.8302, Test Recall = 0.9053, Test F1 = 0.8661\n","\n","Time = 1.552 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","26.188197028785687\n","Training Accuracy = 0.9927, time = 27.272 seconds\n","\n","Running 501 samples:\n","Total loss:\n","125.68153894217181\n","Test Accuracy = 0.8663, Test Precision = 0.8636, Test Recall = 0.8601, Test F1 = 0.8619\n","\n","Time = 1.713 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","15.377779576643626\n","Training Accuracy = 0.9958, time = 27.114 seconds\n","\n","Running 501 samples:\n","Total loss:\n","107.19719946905596\n","Test Accuracy = 0.8703, Test Precision = 0.8618, Test Recall = 0.8724, Test F1 = 0.8671\n","\n","Time = 1.648 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","14.320122004871266\n","Training Accuracy = 0.9960, time = 27.528 seconds\n","\n","Running 501 samples:\n","Total loss:\n","133.68342328583597\n","Test Accuracy = 0.8802, Test Precision = 0.8996, Test Recall = 0.8477, Test F1 = 0.8729\n","\n","Time = 1.711 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","26.283564091325058\n","Training Accuracy = 0.9945, time = 27.094 seconds\n","\n","Running 501 samples:\n","Total loss:\n","95.17714680042354\n","Test Accuracy = 0.8762, Test Precision = 0.8694, Test Recall = 0.8765, Test F1 = 0.8730\n","\n","Time = 1.554 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","13.213213423739525\n","Training Accuracy = 0.9969, time = 27.351 seconds\n","\n","Running 501 samples:\n","Total loss:\n","115.6393675915707\n","Test Accuracy = 0.8782, Test Precision = 0.8991, Test Recall = 0.8436, Test F1 = 0.8705\n","\n","Time = 1.651 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","14.732062023294073\n","Training Accuracy = 0.9962, time = 27.140 seconds\n","\n","Running 501 samples:\n","Total loss:\n","124.83751875801772\n","Test Accuracy = 0.8802, Test Precision = 0.8927, Test Recall = 0.8560, Test F1 = 0.8739\n","\n","Time = 1.629 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","8.674781836121797\n","Training Accuracy = 0.9984, time = 27.137 seconds\n","\n","Running 501 samples:\n","Total loss:\n","158.29955398852067\n","Test Accuracy = 0.8743, Test Precision = 0.8689, Test Recall = 0.8724, Test F1 = 0.8706\n","\n","Time = 1.630 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","14.684153032477116\n","Training Accuracy = 0.9971, time = 27.574 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.28100656325171\n","Test Accuracy = 0.8782, Test Precision = 0.8760, Test Recall = 0.8724, Test F1 = 0.8742\n","\n","Time = 1.568 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","5.610765769458553\n","Training Accuracy = 0.9987, time = 27.445 seconds\n","\n","Running 501 samples:\n","Total loss:\n","150.14400579579495\n","Test Accuracy = 0.8663, Test Precision = 0.8465, Test Recall = 0.8848, Test F1 = 0.8652\n","\n","Time = 1.580 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","4.387968751816626\n","Training Accuracy = 0.9993, time = 27.370 seconds\n","\n","Running 501 samples:\n","Total loss:\n","146.97066427707006\n","Test Accuracy = 0.8802, Test Precision = 0.8735, Test Recall = 0.8807, Test F1 = 0.8770\n","\n","Time = 1.705 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","6.037467265500769\n","Training Accuracy = 0.9978, time = 27.049 seconds\n","\n","Running 501 samples:\n","Total loss:\n","193.21396867875666\n","Test Accuracy = 0.8922, Test Precision = 0.9163, Test Recall = 0.8560, Test F1 = 0.8851\n","\n","Time = 1.688 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","6.793122491454298\n","Training Accuracy = 0.9976, time = 27.068 seconds\n","\n","Running 501 samples:\n","Total loss:\n","176.20634703956128\n","Test Accuracy = 0.8762, Test Precision = 0.8606, Test Recall = 0.8889, Test F1 = 0.8745\n","\n","Time = 1.568 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","10.973337868221208\n","Training Accuracy = 0.9982, time = 27.034 seconds\n","\n","Running 501 samples:\n","Total loss:\n","197.9385853996733\n","Test Accuracy = 0.8802, Test Precision = 0.8675, Test Recall = 0.8889, Test F1 = 0.8780\n","\n","Time = 1.702 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","6.451274471451361\n","Training Accuracy = 0.9991, time = 27.245 seconds\n","\n","Running 501 samples:\n","Total loss:\n","198.85416310617853\n","Test Accuracy = 0.8922, Test Precision = 0.9056, Test Recall = 0.8683, Test F1 = 0.8866\n","\n","Time = 1.666 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","12.232178997331363\n","Training Accuracy = 0.9982, time = 26.693 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.71704088267467\n","Test Accuracy = 0.8663, Test Precision = 0.8438, Test Recall = 0.8889, Test F1 = 0.8657\n","\n","Time = 1.697 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","10.570095551032816\n","Training Accuracy = 0.9984, time = 26.953 seconds\n","\n","Running 501 samples:\n","Total loss:\n","151.20105518368894\n","Test Accuracy = 0.8723, Test Precision = 0.8594, Test Recall = 0.8807, Test F1 = 0.8699\n","\n","Time = 1.558 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","7.567712166281179\n","Training Accuracy = 0.9982, time = 27.011 seconds\n","\n","Running 501 samples:\n","Total loss:\n","141.1798380838336\n","Test Accuracy = 0.8523, Test Precision = 0.8213, Test Recall = 0.8889, Test F1 = 0.8538\n","\n","Time = 1.651 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","4.397173953553786\n","Training Accuracy = 0.9989, time = 27.150 seconds\n","\n","Running 501 samples:\n","Total loss:\n","169.80149497667205\n","Test Accuracy = 0.8762, Test Precision = 0.8918, Test Recall = 0.8477, Test F1 = 0.8692\n","\n","Time = 1.722 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","2.7869669322039172\n","Training Accuracy = 0.9984, time = 26.808 seconds\n","\n","Running 501 samples:\n","Total loss:\n","203.8555859644166\n","Test Accuracy = 0.8743, Test Precision = 0.8689, Test Recall = 0.8724, Test F1 = 0.8706\n","\n","Time = 1.707 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","7.936105629288949\n","Training Accuracy = 0.9991, time = 26.970 seconds\n","\n","Running 501 samples:\n","Total loss:\n","155.63196211277435\n","Test Accuracy = 0.8663, Test Precision = 0.9000, Test Recall = 0.8148, Test F1 = 0.8553\n","\n","Time = 1.667 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","7.082469459267891\n","Training Accuracy = 0.9989, time = 26.965 seconds\n","\n","Running 501 samples:\n","Total loss:\n","232.46803447369592\n","Test Accuracy = 0.8483, Test Precision = 0.8127, Test Recall = 0.8930, Test F1 = 0.8510\n","\n","Time = 1.667 seconds\n","\n","Cross Validation step :  2\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","479.03032134845853\n","Training Accuracy = 0.7818, time = 30.792 seconds\n","\n","Running 501 samples:\n","Total loss:\n","42.5698576325085\n","Test Accuracy = 0.8144, Test Precision = 0.7578, Test Recall = 0.9421, Test F1 = 0.8399\n","\n","Time = 1.553 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","296.6608740860829\n","Training Accuracy = 0.8678, time = 27.380 seconds\n","\n","Running 501 samples:\n","Total loss:\n","48.030081185293966\n","Test Accuracy = 0.8204, Test Precision = 0.7553, Test Recall = 0.9653, Test F1 = 0.8475\n","\n","Time = 1.663 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","175.14903335345662\n","Training Accuracy = 0.9293, time = 27.245 seconds\n","\n","Running 501 samples:\n","Total loss:\n","55.226139909398626\n","Test Accuracy = 0.8363, Test Precision = 0.7740, Test Recall = 0.9653, Test F1 = 0.8591\n","\n","Time = 1.703 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","102.99737729052163\n","Training Accuracy = 0.9603, time = 27.553 seconds\n","\n","Running 501 samples:\n","Total loss:\n","76.80806095695135\n","Test Accuracy = 0.8443, Test Precision = 0.7929, Test Recall = 0.9459, Test F1 = 0.8627\n","\n","Time = 1.660 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","71.34494531945097\n","Training Accuracy = 0.9760, time = 27.522 seconds\n","\n","Running 501 samples:\n","Total loss:\n","76.77448670371729\n","Test Accuracy = 0.8463, Test Precision = 0.8138, Test Recall = 0.9112, Test F1 = 0.8597\n","\n","Time = 1.691 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","48.39039272094367\n","Training Accuracy = 0.9851, time = 27.624 seconds\n","\n","Running 501 samples:\n","Total loss:\n","89.14836350583298\n","Test Accuracy = 0.8503, Test Precision = 0.8309, Test Recall = 0.8919, Test F1 = 0.8603\n","\n","Time = 1.653 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","32.603644360259906\n","Training Accuracy = 0.9889, time = 27.702 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.28105722045717\n","Test Accuracy = 0.8623, Test Precision = 0.8862, Test Recall = 0.8417, Test F1 = 0.8634\n","\n","Time = 1.716 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","36.81694439836542\n","Training Accuracy = 0.9896, time = 27.582 seconds\n","\n","Running 501 samples:\n","Total loss:\n","79.93321522381727\n","Test Accuracy = 0.8802, Test Precision = 0.9163, Test Recall = 0.8456, Test F1 = 0.8795\n","\n","Time = 1.692 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","27.20335707962872\n","Training Accuracy = 0.9920, time = 27.665 seconds\n","\n","Running 501 samples:\n","Total loss:\n","96.02143286318706\n","Test Accuracy = 0.8723, Test Precision = 0.8736, Test Recall = 0.8803, Test F1 = 0.8769\n","\n","Time = 1.720 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","20.24292627710414\n","Training Accuracy = 0.9942, time = 27.445 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.39212072486569\n","Test Accuracy = 0.8623, Test Precision = 0.8598, Test Recall = 0.8764, Test F1 = 0.8681\n","\n","Time = 1.686 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","18.074467825652498\n","Training Accuracy = 0.9958, time = 27.426 seconds\n","\n","Running 501 samples:\n","Total loss:\n","106.57065917222545\n","Test Accuracy = 0.8703, Test Precision = 0.8464, Test Recall = 0.9151, Test F1 = 0.8794\n","\n","Time = 1.745 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","17.847870303810875\n","Training Accuracy = 0.9953, time = 27.426 seconds\n","\n","Running 501 samples:\n","Total loss:\n","181.62736019699997\n","Test Accuracy = 0.8423, Test Precision = 0.8020, Test Recall = 0.9228, Test F1 = 0.8582\n","\n","Time = 1.692 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","19.26650683584338\n","Training Accuracy = 0.9953, time = 27.251 seconds\n","\n","Running 501 samples:\n","Total loss:\n","177.77331842442186\n","Test Accuracy = 0.8443, Test Precision = 0.8027, Test Recall = 0.9266, Test F1 = 0.8602\n","\n","Time = 1.556 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","13.64065893571808\n","Training Accuracy = 0.9980, time = 27.314 seconds\n","\n","Running 501 samples:\n","Total loss:\n","138.03236533175723\n","Test Accuracy = 0.8523, Test Precision = 0.8364, Test Recall = 0.8880, Test F1 = 0.8614\n","\n","Time = 1.688 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","15.394232875927802\n","Training Accuracy = 0.9962, time = 27.437 seconds\n","\n","Running 501 samples:\n","Total loss:\n","159.5929909503745\n","Test Accuracy = 0.8463, Test Precision = 0.8116, Test Recall = 0.9151, Test F1 = 0.8603\n","\n","Time = 1.754 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","5.2381953348389665\n","Training Accuracy = 0.9980, time = 27.290 seconds\n","\n","Running 501 samples:\n","Total loss:\n","187.4861086010584\n","Test Accuracy = 0.8623, Test Precision = 0.8800, Test Recall = 0.8494, Test F1 = 0.8644\n","\n","Time = 1.676 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","11.708493640066067\n","Training Accuracy = 0.9973, time = 27.265 seconds\n","\n","Running 501 samples:\n","Total loss:\n","124.40526424684967\n","Test Accuracy = 0.8543, Test Precision = 0.8298, Test Recall = 0.9035, Test F1 = 0.8651\n","\n","Time = 1.612 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","5.7606012472244466\n","Training Accuracy = 0.9982, time = 27.961 seconds\n","\n","Running 501 samples:\n","Total loss:\n","182.79388344984892\n","Test Accuracy = 0.8583, Test Precision = 0.8431, Test Recall = 0.8919, Test F1 = 0.8668\n","\n","Time = 1.785 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","10.208595498111585\n","Training Accuracy = 0.9987, time = 27.291 seconds\n","\n","Running 501 samples:\n","Total loss:\n","196.7112133914169\n","Test Accuracy = 0.8523, Test Precision = 0.8491, Test Recall = 0.8687, Test F1 = 0.8588\n","\n","Time = 1.732 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","6.217154767400466\n","Training Accuracy = 0.9989, time = 27.236 seconds\n","\n","Running 501 samples:\n","Total loss:\n","207.93604068016802\n","Test Accuracy = 0.8523, Test Precision = 0.8491, Test Recall = 0.8687, Test F1 = 0.8588\n","\n","Time = 1.765 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","11.8507925805423\n","Training Accuracy = 0.9982, time = 27.174 seconds\n","\n","Running 501 samples:\n","Total loss:\n","126.54346412639478\n","Test Accuracy = 0.8483, Test Precision = 0.8506, Test Recall = 0.8571, Test F1 = 0.8538\n","\n","Time = 1.738 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","6.142804065042185\n","Training Accuracy = 0.9980, time = 27.222 seconds\n","\n","Running 501 samples:\n","Total loss:\n","183.7182212772967\n","Test Accuracy = 0.8503, Test Precision = 0.8566, Test Recall = 0.8533, Test F1 = 0.8549\n","\n","Time = 1.710 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","10.337437273873345\n","Training Accuracy = 0.9982, time = 27.221 seconds\n","\n","Running 501 samples:\n","Total loss:\n","211.25414251833178\n","Test Accuracy = 0.8323, Test Precision = 0.7946, Test Recall = 0.9112, Test F1 = 0.8489\n","\n","Time = 1.723 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","10.186373501705152\n","Training Accuracy = 0.9989, time = 27.315 seconds\n","\n","Running 501 samples:\n","Total loss:\n","159.07740582148864\n","Test Accuracy = 0.8583, Test Precision = 0.8760, Test Recall = 0.8456, Test F1 = 0.8605\n","\n","Time = 1.717 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","5.6081157788860345\n","Training Accuracy = 0.9991, time = 27.315 seconds\n","\n","Running 501 samples:\n","Total loss:\n","145.84110897176797\n","Test Accuracy = 0.8483, Test Precision = 0.8233, Test Recall = 0.8996, Test F1 = 0.8598\n","\n","Time = 1.775 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.4335151983639882\n","Training Accuracy = 1.0000, time = 27.376 seconds\n","\n","Running 501 samples:\n","Total loss:\n","196.91842552357616\n","Test Accuracy = 0.8663, Test Precision = 0.8609, Test Recall = 0.8842, Test F1 = 0.8724\n","\n","Time = 1.690 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","4.477062637831722\n","Training Accuracy = 0.9993, time = 27.182 seconds\n","\n","Running 501 samples:\n","Total loss:\n","199.4980980585529\n","Test Accuracy = 0.8603, Test Precision = 0.8735, Test Recall = 0.8533, Test F1 = 0.8633\n","\n","Time = 1.735 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","10.494576170963027\n","Training Accuracy = 0.9978, time = 27.323 seconds\n","\n","Running 501 samples:\n","Total loss:\n","168.05444459208584\n","Test Accuracy = 0.8643, Test Precision = 0.8745, Test Recall = 0.8610, Test F1 = 0.8677\n","\n","Time = 1.747 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","9.411653076442988\n","Training Accuracy = 0.9987, time = 28.060 seconds\n","\n","Running 501 samples:\n","Total loss:\n","164.7827583282758\n","Test Accuracy = 0.8563, Test Precision = 0.8555, Test Recall = 0.8687, Test F1 = 0.8621\n","\n","Time = 1.765 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","2.201190614906942\n","Training Accuracy = 0.9993, time = 27.502 seconds\n","\n","Running 501 samples:\n","Total loss:\n","218.67606636700367\n","Test Accuracy = 0.8483, Test Precision = 0.8453, Test Recall = 0.8649, Test F1 = 0.8550\n","\n","Time = 1.640 seconds\n","\n","Cross Validation step :  3\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","486.0378381125629\n","Training Accuracy = 0.7709, time = 31.265 seconds\n","\n","Running 501 samples:\n","Total loss:\n","55.32585580274463\n","Test Accuracy = 0.7665, Test Precision = 0.6925, Test Recall = 0.9431, Test F1 = 0.7986\n","\n","Time = 1.730 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","284.67660670238547\n","Training Accuracy = 0.8789, time = 26.932 seconds\n","\n","Running 501 samples:\n","Total loss:\n","51.930325992638245\n","Test Accuracy = 0.8204, Test Precision = 0.7806, Test Recall = 0.8821, Test F1 = 0.8282\n","\n","Time = 1.689 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","169.93951354546152\n","Training Accuracy = 0.9337, time = 27.180 seconds\n","\n","Running 501 samples:\n","Total loss:\n","68.09083945512975\n","Test Accuracy = 0.8164, Test Precision = 0.7711, Test Recall = 0.8902, Test F1 = 0.8264\n","\n","Time = 1.731 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","105.2116805920835\n","Training Accuracy = 0.9605, time = 27.424 seconds\n","\n","Running 501 samples:\n","Total loss:\n","84.27782381477263\n","Test Accuracy = 0.8343, Test Precision = 0.8221, Test Recall = 0.8455, Test F1 = 0.8337\n","\n","Time = 1.704 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","76.77867967045712\n","Training Accuracy = 0.9754, time = 26.799 seconds\n","\n","Running 501 samples:\n","Total loss:\n","81.4796453326361\n","Test Accuracy = 0.8503, Test Precision = 0.8202, Test Recall = 0.8902, Test F1 = 0.8538\n","\n","Time = 1.737 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","54.76165501323687\n","Training Accuracy = 0.9818, time = 27.041 seconds\n","\n","Running 501 samples:\n","Total loss:\n","131.91254040476213\n","Test Accuracy = 0.8383, Test Precision = 0.7936, Test Recall = 0.9065, Test F1 = 0.8463\n","\n","Time = 1.740 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","31.655954061823735\n","Training Accuracy = 0.9896, time = 27.050 seconds\n","\n","Running 501 samples:\n","Total loss:\n","143.1217516094867\n","Test Accuracy = 0.8623, Test Precision = 0.8471, Test Recall = 0.8780, Test F1 = 0.8623\n","\n","Time = 1.727 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","33.06990548642475\n","Training Accuracy = 0.9909, time = 26.921 seconds\n","\n","Running 501 samples:\n","Total loss:\n","166.40959603849544\n","Test Accuracy = 0.8643, Test Precision = 0.8477, Test Recall = 0.8821, Test F1 = 0.8645\n","\n","Time = 1.608 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","27.00432589951796\n","Training Accuracy = 0.9938, time = 27.705 seconds\n","\n","Running 501 samples:\n","Total loss:\n","148.11317902185988\n","Test Accuracy = 0.8503, Test Precision = 0.8065, Test Recall = 0.9146, Test F1 = 0.8571\n","\n","Time = 1.774 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","25.87695268775097\n","Training Accuracy = 0.9936, time = 27.143 seconds\n","\n","Running 501 samples:\n","Total loss:\n","150.73285615295143\n","Test Accuracy = 0.8643, Test Precision = 0.8477, Test Recall = 0.8821, Test F1 = 0.8645\n","\n","Time = 1.645 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","22.06041522499516\n","Training Accuracy = 0.9938, time = 27.084 seconds\n","\n","Running 501 samples:\n","Total loss:\n","111.8280595930666\n","Test Accuracy = 0.8743, Test Precision = 0.8645, Test Recall = 0.8821, Test F1 = 0.8732\n","\n","Time = 1.693 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","25.694906989482757\n","Training Accuracy = 0.9931, time = 26.940 seconds\n","\n","Running 501 samples:\n","Total loss:\n","119.97702654779283\n","Test Accuracy = 0.8603, Test Precision = 0.8465, Test Recall = 0.8740, Test F1 = 0.8600\n","\n","Time = 1.716 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","9.365194747653813\n","Training Accuracy = 0.9971, time = 27.240 seconds\n","\n","Running 501 samples:\n","Total loss:\n","152.3008818546135\n","Test Accuracy = 0.8723, Test Precision = 0.8922, Test Recall = 0.8415, Test F1 = 0.8661\n","\n","Time = 1.723 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","9.55365145728518\n","Training Accuracy = 0.9973, time = 27.282 seconds\n","\n","Running 501 samples:\n","Total loss:\n","187.40878193595074\n","Test Accuracy = 0.8543, Test Precision = 0.8289, Test Recall = 0.8862, Test F1 = 0.8566\n","\n","Time = 1.756 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","13.36373450293075\n","Training Accuracy = 0.9978, time = 27.415 seconds\n","\n","Running 501 samples:\n","Total loss:\n","150.39716686620662\n","Test Accuracy = 0.8762, Test Precision = 0.8802, Test Recall = 0.8659, Test F1 = 0.8730\n","\n","Time = 1.804 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","8.981707770366935\n","Training Accuracy = 0.9982, time = 27.209 seconds\n","\n","Running 501 samples:\n","Total loss:\n","207.00916586949265\n","Test Accuracy = 0.8623, Test Precision = 0.8933, Test Recall = 0.8171, Test F1 = 0.8535\n","\n","Time = 1.731 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","13.421611032128311\n","Training Accuracy = 0.9976, time = 27.168 seconds\n","\n","Running 501 samples:\n","Total loss:\n","219.78028007570978\n","Test Accuracy = 0.8723, Test Precision = 0.8856, Test Recall = 0.8496, Test F1 = 0.8672\n","\n","Time = 1.745 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","19.97234292988378\n","Training Accuracy = 0.9971, time = 26.986 seconds\n","\n","Running 501 samples:\n","Total loss:\n","108.9627434236088\n","Test Accuracy = 0.8343, Test Precision = 0.7942, Test Recall = 0.8943, Test F1 = 0.8413\n","\n","Time = 1.800 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","10.236676316577267\n","Training Accuracy = 0.9978, time = 26.806 seconds\n","\n","Running 501 samples:\n","Total loss:\n","139.1239421220309\n","Test Accuracy = 0.8603, Test Precision = 0.8729, Test Recall = 0.8374, Test F1 = 0.8548\n","\n","Time = 1.643 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","5.773202938086271\n","Training Accuracy = 0.9982, time = 27.549 seconds\n","\n","Running 501 samples:\n","Total loss:\n","203.7315407968262\n","Test Accuracy = 0.8543, Test Precision = 0.8240, Test Recall = 0.8943, Test F1 = 0.8577\n","\n","Time = 1.772 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","11.043816807419196\n","Training Accuracy = 0.9982, time = 27.425 seconds\n","\n","Running 501 samples:\n","Total loss:\n","235.85711261362061\n","Test Accuracy = 0.8543, Test Precision = 0.8366, Test Recall = 0.8740, Test F1 = 0.8549\n","\n","Time = 1.722 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","10.46302059785073\n","Training Accuracy = 0.9982, time = 27.071 seconds\n","\n","Running 501 samples:\n","Total loss:\n","234.19151744315704\n","Test Accuracy = 0.8762, Test Precision = 0.9071, Test Recall = 0.8333, Test F1 = 0.8686\n","\n","Time = 1.794 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","7.6072802577448435\n","Training Accuracy = 0.9989, time = 27.057 seconds\n","\n","Running 501 samples:\n","Total loss:\n","197.7728915088587\n","Test Accuracy = 0.8343, Test Precision = 0.8007, Test Recall = 0.8821, Test F1 = 0.8395\n","\n","Time = 1.735 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","4.162298721307465\n","Training Accuracy = 0.9987, time = 26.947 seconds\n","\n","Running 501 samples:\n","Total loss:\n","228.7093185407424\n","Test Accuracy = 0.8363, Test Precision = 0.7993, Test Recall = 0.8902, Test F1 = 0.8423\n","\n","Time = 1.720 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","10.044146554767973\n","Training Accuracy = 0.9987, time = 27.114 seconds\n","\n","Running 501 samples:\n","Total loss:\n","145.4194685210402\n","Test Accuracy = 0.8683, Test Precision = 0.8719, Test Recall = 0.8577, Test F1 = 0.8648\n","\n","Time = 1.765 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","8.150902852631098\n","Training Accuracy = 0.9984, time = 27.402 seconds\n","\n","Running 501 samples:\n","Total loss:\n","175.4086334960509\n","Test Accuracy = 0.8782, Test Precision = 0.9004, Test Recall = 0.8455, Test F1 = 0.8721\n","\n","Time = 1.727 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","5.7461392334412125\n","Training Accuracy = 0.9993, time = 26.811 seconds\n","\n","Running 501 samples:\n","Total loss:\n","213.26883441708534\n","Test Accuracy = 0.8603, Test Precision = 0.8667, Test Recall = 0.8455, Test F1 = 0.8560\n","\n","Time = 1.750 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","3.4556726161741764\n","Training Accuracy = 0.9989, time = 27.077 seconds\n","\n","Running 501 samples:\n","Total loss:\n","246.11414914569764\n","Test Accuracy = 0.8483, Test Precision = 0.8373, Test Recall = 0.8577, Test F1 = 0.8474\n","\n","Time = 1.788 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","7.950248776302306\n","Training Accuracy = 0.9987, time = 27.244 seconds\n","\n","Running 501 samples:\n","Total loss:\n","183.2622492124615\n","Test Accuracy = 0.8723, Test Precision = 0.8856, Test Recall = 0.8496, Test F1 = 0.8672\n","\n","Time = 1.709 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","10.294553893027762\n","Training Accuracy = 0.9987, time = 27.699 seconds\n","\n","Running 501 samples:\n","Total loss:\n","166.20219139794062\n","Test Accuracy = 0.8503, Test Precision = 0.8301, Test Recall = 0.8740, Test F1 = 0.8515\n","\n","Time = 1.746 seconds\n","\n","Cross Validation step :  4\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","482.1935617523268\n","Training Accuracy = 0.7835, time = 30.797 seconds\n","\n","Running 501 samples:\n","Total loss:\n","51.24521349044517\n","Test Accuracy = 0.7545, Test Precision = 0.6677, Test Recall = 0.9534, Test F1 = 0.7853\n","\n","Time = 1.692 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","286.35197850118857\n","Training Accuracy = 0.8782, time = 27.580 seconds\n","\n","Running 501 samples:\n","Total loss:\n","51.30018941278104\n","Test Accuracy = 0.8144, Test Precision = 0.7344, Test Recall = 0.9492, Test F1 = 0.8281\n","\n","Time = 1.676 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","182.2063143202431\n","Training Accuracy = 0.9257, time = 27.678 seconds\n","\n","Running 501 samples:\n","Total loss:\n","67.85270317261711\n","Test Accuracy = 0.8024, Test Precision = 0.7246, Test Recall = 0.9364, Test F1 = 0.8170\n","\n","Time = 1.662 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","106.47616618616988\n","Training Accuracy = 0.9587, time = 27.418 seconds\n","\n","Running 501 samples:\n","Total loss:\n","64.5028840968938\n","Test Accuracy = 0.8403, Test Precision = 0.7806, Test Recall = 0.9195, Test F1 = 0.8444\n","\n","Time = 1.714 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","57.96869554149166\n","Training Accuracy = 0.9776, time = 27.606 seconds\n","\n","Running 501 samples:\n","Total loss:\n","98.30465825081879\n","Test Accuracy = 0.8583, Test Precision = 0.8185, Test Recall = 0.8983, Test F1 = 0.8566\n","\n","Time = 1.618 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","47.03856668860291\n","Training Accuracy = 0.9858, time = 27.697 seconds\n","\n","Running 501 samples:\n","Total loss:\n","90.12601559310741\n","Test Accuracy = 0.8643, Test Precision = 0.8360, Test Recall = 0.8856, Test F1 = 0.8601\n","\n","Time = 1.580 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","33.83558998851363\n","Training Accuracy = 0.9882, time = 27.664 seconds\n","\n","Running 501 samples:\n","Total loss:\n","105.59072117181395\n","Test Accuracy = 0.8643, Test Precision = 0.8443, Test Recall = 0.8729, Test F1 = 0.8583\n","\n","Time = 1.700 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","32.84685347749348\n","Training Accuracy = 0.9905, time = 27.403 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.42547218768371\n","Test Accuracy = 0.8643, Test Precision = 0.8360, Test Recall = 0.8856, Test F1 = 0.8601\n","\n","Time = 1.673 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","29.683798981607534\n","Training Accuracy = 0.9925, time = 27.726 seconds\n","\n","Running 501 samples:\n","Total loss:\n","110.76163582761507\n","Test Accuracy = 0.8802, Test Precision = 0.8492, Test Recall = 0.9068, Test F1 = 0.8770\n","\n","Time = 1.695 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","14.594708092913091\n","Training Accuracy = 0.9971, time = 27.915 seconds\n","\n","Running 501 samples:\n","Total loss:\n","127.05670277796534\n","Test Accuracy = 0.8743, Test Precision = 0.8650, Test Recall = 0.8686, Test F1 = 0.8668\n","\n","Time = 1.686 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","18.332647459026806\n","Training Accuracy = 0.9958, time = 27.910 seconds\n","\n","Running 501 samples:\n","Total loss:\n","160.45319675474838\n","Test Accuracy = 0.8723, Test Precision = 0.8839, Test Recall = 0.8390, Test F1 = 0.8609\n","\n","Time = 1.714 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","24.189615563087507\n","Training Accuracy = 0.9940, time = 27.612 seconds\n","\n","Running 501 samples:\n","Total loss:\n","116.32544726585989\n","Test Accuracy = 0.8683, Test Precision = 0.8664, Test Recall = 0.8517, Test F1 = 0.8590\n","\n","Time = 1.759 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","25.235914471398946\n","Training Accuracy = 0.9942, time = 27.761 seconds\n","\n","Running 501 samples:\n","Total loss:\n","83.99296842477257\n","Test Accuracy = 0.8603, Test Precision = 0.8374, Test Recall = 0.8729, Test F1 = 0.8548\n","\n","Time = 1.745 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","4.6707290213437425\n","Training Accuracy = 0.9989, time = 27.438 seconds\n","\n","Running 501 samples:\n","Total loss:\n","164.30447668044525\n","Test Accuracy = 0.8563, Test Precision = 0.8333, Test Recall = 0.8686, Test F1 = 0.8506\n","\n","Time = 1.668 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","20.25962856056419\n","Training Accuracy = 0.9960, time = 27.477 seconds\n","\n","Running 501 samples:\n","Total loss:\n","131.64494063523915\n","Test Accuracy = 0.8663, Test Precision = 0.8165, Test Recall = 0.9237, Test F1 = 0.8668\n","\n","Time = 1.700 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","12.444049828925234\n","Training Accuracy = 0.9965, time = 27.274 seconds\n","\n","Running 501 samples:\n","Total loss:\n","137.09205056992127\n","Test Accuracy = 0.8703, Test Precision = 0.8548, Test Recall = 0.8729, Test F1 = 0.8637\n","\n","Time = 1.718 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","5.433446733708866\n","Training Accuracy = 0.9991, time = 27.580 seconds\n","\n","Running 501 samples:\n","Total loss:\n","173.82143383921272\n","Test Accuracy = 0.8723, Test Precision = 0.8525, Test Recall = 0.8814, Test F1 = 0.8667\n","\n","Time = 1.713 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","6.439464843684604\n","Training Accuracy = 0.9991, time = 27.484 seconds\n","\n","Running 501 samples:\n","Total loss:\n","156.97400304819038\n","Test Accuracy = 0.8583, Test Precision = 0.8261, Test Recall = 0.8856, Test F1 = 0.8548\n","\n","Time = 1.713 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","3.836831133199098\n","Training Accuracy = 0.9993, time = 27.654 seconds\n","\n","Running 501 samples:\n","Total loss:\n","171.4219758072691\n","Test Accuracy = 0.8643, Test Precision = 0.8652, Test Recall = 0.8432, Test F1 = 0.8541\n","\n","Time = 1.728 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","15.2055726277461\n","Training Accuracy = 0.9965, time = 27.624 seconds\n","\n","Running 501 samples:\n","Total loss:\n","167.50449706798045\n","Test Accuracy = 0.8743, Test Precision = 0.8681, Test Recall = 0.8644, Test F1 = 0.8662\n","\n","Time = 1.695 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","3.6807144251133224\n","Training Accuracy = 0.9991, time = 27.867 seconds\n","\n","Running 501 samples:\n","Total loss:\n","164.84328753968327\n","Test Accuracy = 0.8643, Test Precision = 0.8387, Test Recall = 0.8814, Test F1 = 0.8595\n","\n","Time = 1.707 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","9.449798480434819\n","Training Accuracy = 0.9976, time = 27.569 seconds\n","\n","Running 501 samples:\n","Total loss:\n","178.4415494120725\n","Test Accuracy = 0.8603, Test Precision = 0.8843, Test Recall = 0.8093, Test F1 = 0.8451\n","\n","Time = 1.784 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","8.811679797057089\n","Training Accuracy = 0.9987, time = 27.454 seconds\n","\n","Running 501 samples:\n","Total loss:\n","144.77059316454418\n","Test Accuracy = 0.8703, Test Precision = 0.8608, Test Recall = 0.8644, Test F1 = 0.8626\n","\n","Time = 1.702 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","3.2980444062072767\n","Training Accuracy = 0.9989, time = 27.558 seconds\n","\n","Running 501 samples:\n","Total loss:\n","208.31786225592592\n","Test Accuracy = 0.8762, Test Precision = 0.8783, Test Recall = 0.8559, Test F1 = 0.8670\n","\n","Time = 1.704 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","10.792105401542539\n","Training Accuracy = 0.9984, time = 27.360 seconds\n","\n","Running 501 samples:\n","Total loss:\n","152.81823809608167\n","Test Accuracy = 0.8503, Test Precision = 0.8233, Test Recall = 0.8686, Test F1 = 0.8454\n","\n","Time = 1.747 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","10.950406399172248\n","Training Accuracy = 0.9980, time = 27.425 seconds\n","\n","Running 501 samples:\n","Total loss:\n","157.82793459172672\n","Test Accuracy = 0.8623, Test Precision = 0.8646, Test Recall = 0.8390, Test F1 = 0.8516\n","\n","Time = 1.717 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","6.979457445334752\n","Training Accuracy = 0.9989, time = 27.569 seconds\n","\n","Running 501 samples:\n","Total loss:\n","178.02529104303534\n","Test Accuracy = 0.8643, Test Precision = 0.8717, Test Recall = 0.8347, Test F1 = 0.8528\n","\n","Time = 1.714 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","0.23371494630128353\n","Training Accuracy = 1.0000, time = 27.579 seconds\n","\n","Running 501 samples:\n","Total loss:\n","211.1323165162636\n","Test Accuracy = 0.8703, Test Precision = 0.8608, Test Recall = 0.8644, Test F1 = 0.8626\n","\n","Time = 1.747 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","1.2692858244942933\n","Training Accuracy = 0.9996, time = 27.782 seconds\n","\n","Running 501 samples:\n","Total loss:\n","247.81409433507346\n","Test Accuracy = 0.8623, Test Precision = 0.8523, Test Recall = 0.8559, Test F1 = 0.8541\n","\n","Time = 1.779 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","10.085971228764233\n","Training Accuracy = 0.9984, time = 27.800 seconds\n","\n","Running 501 samples:\n","Total loss:\n","134.67289084653675\n","Test Accuracy = 0.8583, Test Precision = 0.8367, Test Recall = 0.8686, Test F1 = 0.8524\n","\n","Time = 1.722 seconds\n","\n","Cross Validation step :  5\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","479.62250504642725\n","Training Accuracy = 0.7700, time = 31.000 seconds\n","\n","Running 501 samples:\n","Total loss:\n","42.915616766526455\n","Test Accuracy = 0.7944, Test Precision = 0.7394, Test Recall = 0.9349, Test F1 = 0.8257\n","\n","Time = 1.663 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","292.82028992503183\n","Training Accuracy = 0.8738, time = 27.892 seconds\n","\n","Running 501 samples:\n","Total loss:\n","44.71835932851536\n","Test Accuracy = 0.8024, Test Precision = 0.7440, Test Recall = 0.9464, Test F1 = 0.8331\n","\n","Time = 1.647 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","184.8574688449844\n","Training Accuracy = 0.9257, time = 27.712 seconds\n","\n","Running 501 samples:\n","Total loss:\n","46.29793728542609\n","Test Accuracy = 0.8403, Test Precision = 0.7929, Test Recall = 0.9387, Test F1 = 0.8596\n","\n","Time = 1.728 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","103.12465000633158\n","Training Accuracy = 0.9619, time = 27.675 seconds\n","\n","Running 501 samples:\n","Total loss:\n","48.32658908986741\n","Test Accuracy = 0.8463, Test Precision = 0.8286, Test Recall = 0.8889, Test F1 = 0.8577\n","\n","Time = 1.682 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","64.56166002274797\n","Training Accuracy = 0.9772, time = 27.602 seconds\n","\n","Running 501 samples:\n","Total loss:\n","90.96490965444579\n","Test Accuracy = 0.8523, Test Precision = 0.8281, Test Recall = 0.9042, Test F1 = 0.8645\n","\n","Time = 1.616 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","43.786065329123694\n","Training Accuracy = 0.9876, time = 27.848 seconds\n","\n","Running 501 samples:\n","Total loss:\n","95.49831195043662\n","Test Accuracy = 0.8623, Test Precision = 0.8504, Test Recall = 0.8927, Test F1 = 0.8710\n","\n","Time = 1.720 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","36.63272551073871\n","Training Accuracy = 0.9907, time = 27.823 seconds\n","\n","Running 501 samples:\n","Total loss:\n","98.49283586169818\n","Test Accuracy = 0.8523, Test Precision = 0.8502, Test Recall = 0.8697, Test F1 = 0.8598\n","\n","Time = 1.707 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","32.63970649322924\n","Training Accuracy = 0.9891, time = 27.974 seconds\n","\n","Running 501 samples:\n","Total loss:\n","108.60014285865842\n","Test Accuracy = 0.8603, Test Precision = 0.9099, Test Recall = 0.8123, Test F1 = 0.8583\n","\n","Time = 1.734 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","29.026565441845822\n","Training Accuracy = 0.9931, time = 27.806 seconds\n","\n","Running 501 samples:\n","Total loss:\n","115.62964946189831\n","Test Accuracy = 0.8563, Test Precision = 0.8621, Test Recall = 0.8621, Test F1 = 0.8621\n","\n","Time = 1.668 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","16.192629207579913\n","Training Accuracy = 0.9951, time = 27.689 seconds\n","\n","Running 501 samples:\n","Total loss:\n","146.17732343185884\n","Test Accuracy = 0.8343, Test Precision = 0.8201, Test Recall = 0.8736, Test F1 = 0.8460\n","\n","Time = 1.720 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","21.41782696379655\n","Training Accuracy = 0.9938, time = 28.010 seconds\n","\n","Running 501 samples:\n","Total loss:\n","119.29636949816913\n","Test Accuracy = 0.8523, Test Precision = 0.8327, Test Recall = 0.8966, Test F1 = 0.8635\n","\n","Time = 1.753 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","16.280009273184163\n","Training Accuracy = 0.9962, time = 28.017 seconds\n","\n","Running 501 samples:\n","Total loss:\n","147.22732172734806\n","Test Accuracy = 0.8343, Test Precision = 0.8134, Test Recall = 0.8851, Test F1 = 0.8477\n","\n","Time = 1.742 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","12.850320507172707\n","Training Accuracy = 0.9960, time = 27.671 seconds\n","\n","Running 501 samples:\n","Total loss:\n","167.45081454902774\n","Test Accuracy = 0.8463, Test Precision = 0.8407, Test Recall = 0.8697, Test F1 = 0.8550\n","\n","Time = 1.783 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","8.284468855690587\n","Training Accuracy = 0.9978, time = 27.769 seconds\n","\n","Running 501 samples:\n","Total loss:\n","159.8851661888339\n","Test Accuracy = 0.8463, Test Precision = 0.8710, Test Recall = 0.8276, Test F1 = 0.8487\n","\n","Time = 1.769 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","14.44051570220499\n","Training Accuracy = 0.9967, time = 27.980 seconds\n","\n","Running 501 samples:\n","Total loss:\n","139.0887569649665\n","Test Accuracy = 0.8583, Test Precision = 0.8598, Test Recall = 0.8697, Test F1 = 0.8648\n","\n","Time = 1.759 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","12.081367943270559\n","Training Accuracy = 0.9978, time = 28.044 seconds\n","\n","Running 501 samples:\n","Total loss:\n","158.0437221297532\n","Test Accuracy = 0.8583, Test Precision = 0.8800, Test Recall = 0.8429, Test F1 = 0.8611\n","\n","Time = 1.791 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","16.907527477185354\n","Training Accuracy = 0.9967, time = 27.693 seconds\n","\n","Running 501 samples:\n","Total loss:\n","165.78434950138322\n","Test Accuracy = 0.8463, Test Precision = 0.8217, Test Recall = 0.9004, Test F1 = 0.8592\n","\n","Time = 1.726 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","11.187254524760554\n","Training Accuracy = 0.9965, time = 27.985 seconds\n","\n","Running 501 samples:\n","Total loss:\n","139.63996846534093\n","Test Accuracy = 0.8563, Test Precision = 0.8826, Test Recall = 0.8352, Test F1 = 0.8583\n","\n","Time = 1.704 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","6.411827545214278\n","Training Accuracy = 0.9982, time = 27.994 seconds\n","\n","Running 501 samples:\n","Total loss:\n","145.32131841993888\n","Test Accuracy = 0.8583, Test Precision = 0.8831, Test Recall = 0.8391, Test F1 = 0.8605\n","\n","Time = 1.764 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","8.036888731391144\n","Training Accuracy = 0.9976, time = 27.651 seconds\n","\n","Running 501 samples:\n","Total loss:\n","156.65337843827822\n","Test Accuracy = 0.8523, Test Precision = 0.8725, Test Recall = 0.8391, Test F1 = 0.8555\n","\n","Time = 1.756 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","8.295401893095697\n","Training Accuracy = 0.9978, time = 27.816 seconds\n","\n","Running 501 samples:\n","Total loss:\n","197.6794189694078\n","Test Accuracy = 0.8603, Test Precision = 0.8835, Test Recall = 0.8429, Test F1 = 0.8627\n","\n","Time = 1.772 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","8.484162012577368\n","Training Accuracy = 0.9987, time = 28.149 seconds\n","\n","Running 501 samples:\n","Total loss:\n","145.2913309604162\n","Test Accuracy = 0.8563, Test Precision = 0.8857, Test Recall = 0.8314, Test F1 = 0.8577\n","\n","Time = 1.758 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","9.450231147880924\n","Training Accuracy = 0.9982, time = 27.887 seconds\n","\n","Running 501 samples:\n","Total loss:\n","181.26761180187117\n","Test Accuracy = 0.8523, Test Precision = 0.8582, Test Recall = 0.8582, Test F1 = 0.8582\n","\n","Time = 1.610 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","2.5767975138229957\n","Training Accuracy = 0.9998, time = 28.082 seconds\n","\n","Running 501 samples:\n","Total loss:\n","195.41310992498876\n","Test Accuracy = 0.8383, Test Precision = 0.8214, Test Recall = 0.8812, Test F1 = 0.8503\n","\n","Time = 1.748 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","8.416601338337598\n","Training Accuracy = 0.9984, time = 27.729 seconds\n","\n","Running 501 samples:\n","Total loss:\n","202.17676391975277\n","Test Accuracy = 0.8463, Test Precision = 0.8485, Test Recall = 0.8582, Test F1 = 0.8533\n","\n","Time = 1.788 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","2.2275742857343452\n","Training Accuracy = 0.9991, time = 27.899 seconds\n","\n","Running 501 samples:\n","Total loss:\n","237.0864376061005\n","Test Accuracy = 0.8363, Test Precision = 0.8482, Test Recall = 0.8352, Test F1 = 0.8417\n","\n","Time = 1.801 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","8.088618005965813\n","Training Accuracy = 0.9987, time = 27.939 seconds\n","\n","Running 501 samples:\n","Total loss:\n","173.45808312900596\n","Test Accuracy = 0.8383, Test Precision = 0.8285, Test Recall = 0.8697, Test F1 = 0.8486\n","\n","Time = 1.645 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","5.411766321953131\n","Training Accuracy = 0.9989, time = 27.656 seconds\n","\n","Running 501 samples:\n","Total loss:\n","192.79770653401692\n","Test Accuracy = 0.8463, Test Precision = 0.8433, Test Recall = 0.8659, Test F1 = 0.8544\n","\n","Time = 1.725 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.24745142773555528\n","Training Accuracy = 1.0000, time = 27.588 seconds\n","\n","Running 501 samples:\n","Total loss:\n","249.38639460942508\n","Test Accuracy = 0.8363, Test Precision = 0.8140, Test Recall = 0.8889, Test F1 = 0.8498\n","\n","Time = 1.760 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","14.753241923853082\n","Training Accuracy = 0.9980, time = 27.895 seconds\n","\n","Running 501 samples:\n","Total loss:\n","123.1117802922264\n","Test Accuracy = 0.8443, Test Precision = 0.8894, Test Recall = 0.8008, Test F1 = 0.8427\n","\n","Time = 1.773 seconds\n","\n","Cross Validation step :  6\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","480.48351763468236\n","Training Accuracy = 0.7784, time = 30.692 seconds\n","\n","Running 501 samples:\n","Total loss:\n","59.857126984745264\n","Test Accuracy = 0.7106, Test Precision = 0.6496, Test Recall = 0.9414, Test F1 = 0.7687\n","\n","Time = 1.677 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","289.90541832719464\n","Training Accuracy = 0.8720, time = 27.989 seconds\n","\n","Running 501 samples:\n","Total loss:\n","53.67353997938335\n","Test Accuracy = 0.8044, Test Precision = 0.7516, Test Recall = 0.9219, Test F1 = 0.8281\n","\n","Time = 1.531 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","174.19311648824606\n","Training Accuracy = 0.9257, time = 27.402 seconds\n","\n","Running 501 samples:\n","Total loss:\n","65.46529698497034\n","Test Accuracy = 0.8144, Test Precision = 0.7655, Test Recall = 0.9180, Test F1 = 0.8348\n","\n","Time = 1.717 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","107.91439400380148\n","Training Accuracy = 0.9616, time = 27.557 seconds\n","\n","Running 501 samples:\n","Total loss:\n","76.62078847640441\n","Test Accuracy = 0.8104, Test Precision = 0.7622, Test Recall = 0.9141, Test F1 = 0.8313\n","\n","Time = 1.655 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","64.72143996144985\n","Training Accuracy = 0.9763, time = 27.629 seconds\n","\n","Running 501 samples:\n","Total loss:\n","99.3925258286622\n","Test Accuracy = 0.8204, Test Precision = 0.7730, Test Recall = 0.9180, Test F1 = 0.8393\n","\n","Time = 1.699 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","54.17415531585036\n","Training Accuracy = 0.9838, time = 27.269 seconds\n","\n","Running 501 samples:\n","Total loss:\n","104.18893642233701\n","Test Accuracy = 0.8523, Test Precision = 0.8583, Test Recall = 0.8516, Test F1 = 0.8549\n","\n","Time = 1.662 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","37.459841691132794\n","Training Accuracy = 0.9885, time = 27.645 seconds\n","\n","Running 501 samples:\n","Total loss:\n","134.8728113668982\n","Test Accuracy = 0.8603, Test Precision = 0.8941, Test Recall = 0.8242, Test F1 = 0.8577\n","\n","Time = 1.726 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","34.01897476036103\n","Training Accuracy = 0.9916, time = 27.868 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.13389039569716\n","Test Accuracy = 0.8523, Test Precision = 0.8370, Test Recall = 0.8828, Test F1 = 0.8593\n","\n","Time = 1.563 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","16.615264108642485\n","Training Accuracy = 0.9947, time = 27.625 seconds\n","\n","Running 501 samples:\n","Total loss:\n","188.4051694225093\n","Test Accuracy = 0.8603, Test Precision = 0.8690, Test Recall = 0.8555, Test F1 = 0.8622\n","\n","Time = 1.725 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","24.458454860462332\n","Training Accuracy = 0.9942, time = 27.704 seconds\n","\n","Running 501 samples:\n","Total loss:\n","158.7149815368537\n","Test Accuracy = 0.8523, Test Precision = 0.8611, Test Recall = 0.8477, Test F1 = 0.8543\n","\n","Time = 1.710 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","23.835999981935736\n","Training Accuracy = 0.9931, time = 27.503 seconds\n","\n","Running 501 samples:\n","Total loss:\n","122.95571745893355\n","Test Accuracy = 0.8503, Test Precision = 0.8549, Test Recall = 0.8516, Test F1 = 0.8532\n","\n","Time = 1.738 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","17.198225241130878\n","Training Accuracy = 0.9956, time = 28.139 seconds\n","\n","Running 501 samples:\n","Total loss:\n","148.5607876804392\n","Test Accuracy = 0.8563, Test Precision = 0.8932, Test Recall = 0.8164, Test F1 = 0.8531\n","\n","Time = 1.733 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","14.373243964173941\n","Training Accuracy = 0.9965, time = 27.869 seconds\n","\n","Running 501 samples:\n","Total loss:\n","126.25688544467224\n","Test Accuracy = 0.8523, Test Precision = 0.8583, Test Recall = 0.8516, Test F1 = 0.8549\n","\n","Time = 1.726 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","17.678348162659418\n","Training Accuracy = 0.9962, time = 27.502 seconds\n","\n","Running 501 samples:\n","Total loss:\n","173.87277523172176\n","Test Accuracy = 0.8443, Test Precision = 0.8225, Test Recall = 0.8867, Test F1 = 0.8534\n","\n","Time = 1.722 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","16.088004871068517\n","Training Accuracy = 0.9960, time = 27.665 seconds\n","\n","Running 501 samples:\n","Total loss:\n","144.4403701027948\n","Test Accuracy = 0.8503, Test Precision = 0.8694, Test Recall = 0.8320, Test F1 = 0.8503\n","\n","Time = 1.752 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","7.9007859394467985\n","Training Accuracy = 0.9978, time = 27.546 seconds\n","\n","Running 501 samples:\n","Total loss:\n","201.11677452367005\n","Test Accuracy = 0.8463, Test Precision = 0.8683, Test Recall = 0.8242, Test F1 = 0.8457\n","\n","Time = 1.697 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","8.875492417859652\n","Training Accuracy = 0.9976, time = 27.623 seconds\n","\n","Running 501 samples:\n","Total loss:\n","193.38946317981635\n","Test Accuracy = 0.8483, Test Precision = 0.8629, Test Recall = 0.8359, Test F1 = 0.8492\n","\n","Time = 1.582 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","15.327247883454618\n","Training Accuracy = 0.9969, time = 27.638 seconds\n","\n","Running 501 samples:\n","Total loss:\n","216.60692127086247\n","Test Accuracy = 0.8543, Test Precision = 0.8645, Test Recall = 0.8477, Test F1 = 0.8560\n","\n","Time = 1.787 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","4.099489010958987\n","Training Accuracy = 0.9989, time = 27.638 seconds\n","\n","Running 501 samples:\n","Total loss:\n","193.28561025805024\n","Test Accuracy = 0.8523, Test Precision = 0.8730, Test Recall = 0.8320, Test F1 = 0.8520\n","\n","Time = 1.716 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","12.86148653904364\n","Training Accuracy = 0.9969, time = 27.997 seconds\n","\n","Running 501 samples:\n","Total loss:\n","170.22171341078993\n","Test Accuracy = 0.8423, Test Precision = 0.8290, Test Recall = 0.8711, Test F1 = 0.8495\n","\n","Time = 1.714 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","9.926372125594073\n","Training Accuracy = 0.9980, time = 27.828 seconds\n","\n","Running 501 samples:\n","Total loss:\n","216.98574981239832\n","Test Accuracy = 0.8483, Test Precision = 0.8814, Test Recall = 0.8125, Test F1 = 0.8455\n","\n","Time = 1.737 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","1.7273074783560975\n","Training Accuracy = 0.9996, time = 27.502 seconds\n","\n","Running 501 samples:\n","Total loss:\n","226.69801259751205\n","Test Accuracy = 0.8623, Test Precision = 0.8755, Test Recall = 0.8516, Test F1 = 0.8634\n","\n","Time = 1.769 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","13.663323370741765\n","Training Accuracy = 0.9984, time = 27.989 seconds\n","\n","Running 501 samples:\n","Total loss:\n","148.57218546234222\n","Test Accuracy = 0.8643, Test Precision = 0.8730, Test Recall = 0.8594, Test F1 = 0.8661\n","\n","Time = 1.749 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","11.994392193074901\n","Training Accuracy = 0.9980, time = 27.444 seconds\n","\n","Running 501 samples:\n","Total loss:\n","209.49444156664134\n","Test Accuracy = 0.8543, Test Precision = 0.8588, Test Recall = 0.8555, Test F1 = 0.8571\n","\n","Time = 1.745 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","2.897712806428295\n","Training Accuracy = 0.9991, time = 27.384 seconds\n","\n","Running 501 samples:\n","Total loss:\n","222.062425563691\n","Test Accuracy = 0.8523, Test Precision = 0.8473, Test Recall = 0.8672, Test F1 = 0.8571\n","\n","Time = 1.767 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","6.011111116180425\n","Training Accuracy = 0.9987, time = 27.594 seconds\n","\n","Running 501 samples:\n","Total loss:\n","189.38783173385585\n","Test Accuracy = 0.8583, Test Precision = 0.8903, Test Recall = 0.8242, Test F1 = 0.8560\n","\n","Time = 1.601 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","10.093446750886663\n","Training Accuracy = 0.9982, time = 27.715 seconds\n","\n","Running 501 samples:\n","Total loss:\n","156.7370780635207\n","Test Accuracy = 0.8543, Test Precision = 0.8704, Test Recall = 0.8398, Test F1 = 0.8549\n","\n","Time = 1.713 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","0.530396076148623\n","Training Accuracy = 1.0000, time = 27.749 seconds\n","\n","Running 501 samples:\n","Total loss:\n","214.67956783900405\n","Test Accuracy = 0.8523, Test Precision = 0.8824, Test Recall = 0.8203, Test F1 = 0.8502\n","\n","Time = 1.777 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","4.839156540909766\n","Training Accuracy = 0.9996, time = 27.581 seconds\n","\n","Running 501 samples:\n","Total loss:\n","199.9469110112114\n","Test Accuracy = 0.8563, Test Precision = 0.8802, Test Recall = 0.8320, Test F1 = 0.8554\n","\n","Time = 1.740 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.9471147838470095\n","Training Accuracy = 0.9998, time = 27.919 seconds\n","\n","Running 501 samples:\n","Total loss:\n","259.1400245543317\n","Test Accuracy = 0.8543, Test Precision = 0.8617, Test Recall = 0.8516, Test F1 = 0.8566\n","\n","Time = 1.736 seconds\n","\n","Cross Validation step :  7\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","487.8711352501996\n","Training Accuracy = 0.7640, time = 31.404 seconds\n","\n","Running 501 samples:\n","Total loss:\n","49.03513878211379\n","Test Accuracy = 0.7625, Test Precision = 0.6677, Test Recall = 0.9283, Test F1 = 0.7767\n","\n","Time = 1.707 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","290.11892354954034\n","Training Accuracy = 0.8718, time = 27.451 seconds\n","\n","Running 501 samples:\n","Total loss:\n","52.46424790099263\n","Test Accuracy = 0.7884, Test Precision = 0.6997, Test Recall = 0.9193, Test F1 = 0.7946\n","\n","Time = 1.791 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","176.57816808405732\n","Training Accuracy = 0.9273, time = 28.116 seconds\n","\n","Running 501 samples:\n","Total loss:\n","56.41076162981335\n","Test Accuracy = 0.8204, Test Precision = 0.7529, Test Recall = 0.8879, Test F1 = 0.8148\n","\n","Time = 1.743 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","113.03778075984894\n","Training Accuracy = 0.9599, time = 27.941 seconds\n","\n","Running 501 samples:\n","Total loss:\n","56.64432815870532\n","Test Accuracy = 0.8283, Test Precision = 0.7708, Test Recall = 0.8744, Test F1 = 0.8193\n","\n","Time = 1.738 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","71.91458070812453\n","Training Accuracy = 0.9743, time = 27.679 seconds\n","\n","Running 501 samples:\n","Total loss:\n","63.82580569768106\n","Test Accuracy = 0.8523, Test Precision = 0.8225, Test Recall = 0.8520, Test F1 = 0.8370\n","\n","Time = 1.703 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","42.72950344826994\n","Training Accuracy = 0.9845, time = 27.675 seconds\n","\n","Running 501 samples:\n","Total loss:\n","109.87515575662701\n","Test Accuracy = 0.8044, Test Precision = 0.7224, Test Recall = 0.9103, Test F1 = 0.8056\n","\n","Time = 1.736 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","40.9509923150043\n","Training Accuracy = 0.9858, time = 27.865 seconds\n","\n","Running 501 samples:\n","Total loss:\n","101.72433812490272\n","Test Accuracy = 0.8423, Test Precision = 0.7880, Test Recall = 0.8834, Test F1 = 0.8330\n","\n","Time = 1.714 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","37.56533229733558\n","Training Accuracy = 0.9878, time = 27.746 seconds\n","\n","Running 501 samples:\n","Total loss:\n","141.3009511565593\n","Test Accuracy = 0.8244, Test Precision = 0.7547, Test Recall = 0.8969, Test F1 = 0.8197\n","\n","Time = 1.785 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","20.89032563387415\n","Training Accuracy = 0.9942, time = 28.025 seconds\n","\n","Running 501 samples:\n","Total loss:\n","116.4204219458219\n","Test Accuracy = 0.8623, Test Precision = 0.8468, Test Recall = 0.8430, Test F1 = 0.8449\n","\n","Time = 1.758 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","18.4803049223062\n","Training Accuracy = 0.9942, time = 27.946 seconds\n","\n","Running 501 samples:\n","Total loss:\n","143.44020244572582\n","Test Accuracy = 0.8563, Test Precision = 0.8159, Test Recall = 0.8744, Test F1 = 0.8442\n","\n","Time = 1.601 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","19.471950590410465\n","Training Accuracy = 0.9949, time = 27.856 seconds\n","\n","Running 501 samples:\n","Total loss:\n","127.82603203013598\n","Test Accuracy = 0.8523, Test Precision = 0.7945, Test Recall = 0.9013, Test F1 = 0.8445\n","\n","Time = 1.761 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","19.047678510115375\n","Training Accuracy = 0.9951, time = 27.750 seconds\n","\n","Running 501 samples:\n","Total loss:\n","103.74766023869617\n","Test Accuracy = 0.8643, Test Precision = 0.8326, Test Recall = 0.8700, Test F1 = 0.8509\n","\n","Time = 1.754 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","13.183039118307537\n","Training Accuracy = 0.9967, time = 27.885 seconds\n","\n","Running 501 samples:\n","Total loss:\n","129.22472959372305\n","Test Accuracy = 0.8643, Test Precision = 0.8270, Test Recall = 0.8789, Test F1 = 0.8522\n","\n","Time = 1.693 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","15.141326396320657\n","Training Accuracy = 0.9967, time = 27.140 seconds\n","\n","Running 501 samples:\n","Total loss:\n","146.11359601985663\n","Test Accuracy = 0.8503, Test Precision = 0.8033, Test Recall = 0.8789, Test F1 = 0.8394\n","\n","Time = 1.743 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","12.086483460179803\n","Training Accuracy = 0.9971, time = 27.362 seconds\n","\n","Running 501 samples:\n","Total loss:\n","174.58226073337485\n","Test Accuracy = 0.8603, Test Precision = 0.8341, Test Recall = 0.8565, Test F1 = 0.8451\n","\n","Time = 1.733 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","12.275636597528951\n","Training Accuracy = 0.9969, time = 27.777 seconds\n","\n","Running 501 samples:\n","Total loss:\n","151.81092684356008\n","Test Accuracy = 0.8483, Test Precision = 0.8182, Test Recall = 0.8475, Test F1 = 0.8326\n","\n","Time = 1.808 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","20.605085356941842\n","Training Accuracy = 0.9949, time = 27.620 seconds\n","\n","Running 501 samples:\n","Total loss:\n","119.49508466476323\n","Test Accuracy = 0.8543, Test Precision = 0.8205, Test Recall = 0.8610, Test F1 = 0.8403\n","\n","Time = 1.779 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","6.909282358249737\n","Training Accuracy = 0.9984, time = 27.711 seconds\n","\n","Running 501 samples:\n","Total loss:\n","173.8338243758548\n","Test Accuracy = 0.8503, Test Precision = 0.8304, Test Recall = 0.8341, Test F1 = 0.8322\n","\n","Time = 1.746 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","7.484686011218447\n","Training Accuracy = 0.9980, time = 27.843 seconds\n","\n","Running 501 samples:\n","Total loss:\n","160.35965805367607\n","Test Accuracy = 0.8643, Test Precision = 0.8539, Test Recall = 0.8386, Test F1 = 0.8462\n","\n","Time = 1.653 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","10.410171690432495\n","Training Accuracy = 0.9982, time = 27.837 seconds\n","\n","Running 501 samples:\n","Total loss:\n","171.68194848338047\n","Test Accuracy = 0.8583, Test Precision = 0.8455, Test Recall = 0.8341, Test F1 = 0.8397\n","\n","Time = 1.806 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","8.420651496180124\n","Training Accuracy = 0.9978, time = 27.548 seconds\n","\n","Running 501 samples:\n","Total loss:\n","218.42531269501046\n","Test Accuracy = 0.8543, Test Precision = 0.8151, Test Recall = 0.8700, Test F1 = 0.8416\n","\n","Time = 1.757 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","11.791359848342232\n","Training Accuracy = 0.9984, time = 27.840 seconds\n","\n","Running 501 samples:\n","Total loss:\n","122.31830888175362\n","Test Accuracy = 0.8543, Test Precision = 0.8205, Test Recall = 0.8610, Test F1 = 0.8403\n","\n","Time = 1.814 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","2.982775573324851\n","Training Accuracy = 0.9989, time = 28.070 seconds\n","\n","Running 501 samples:\n","Total loss:\n","191.71716200553425\n","Test Accuracy = 0.8663, Test Precision = 0.8514, Test Recall = 0.8475, Test F1 = 0.8494\n","\n","Time = 1.834 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","10.894717138787431\n","Training Accuracy = 0.9978, time = 27.911 seconds\n","\n","Running 501 samples:\n","Total loss:\n","193.1664113642394\n","Test Accuracy = 0.8583, Test Precision = 0.8248, Test Recall = 0.8655, Test F1 = 0.8446\n","\n","Time = 1.739 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","6.090785949949549\n","Training Accuracy = 0.9991, time = 27.634 seconds\n","\n","Running 501 samples:\n","Total loss:\n","189.24476382335763\n","Test Accuracy = 0.8603, Test Precision = 0.8430, Test Recall = 0.8430, Test F1 = 0.8430\n","\n","Time = 1.779 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","7.162139936436169\n","Training Accuracy = 0.9987, time = 27.667 seconds\n","\n","Running 501 samples:\n","Total loss:\n","169.96215055896133\n","Test Accuracy = 0.8443, Test Precision = 0.7912, Test Recall = 0.8834, Test F1 = 0.8347\n","\n","Time = 1.765 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","1.905557395651627\n","Training Accuracy = 0.9993, time = 27.677 seconds\n","\n","Running 501 samples:\n","Total loss:\n","246.5301970459492\n","Test Accuracy = 0.8643, Test Precision = 0.8444, Test Recall = 0.8520, Test F1 = 0.8482\n","\n","Time = 1.812 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","8.420502076869802\n","Training Accuracy = 0.9982, time = 27.763 seconds\n","\n","Running 501 samples:\n","Total loss:\n","177.80356897856228\n","Test Accuracy = 0.8743, Test Precision = 0.8670, Test Recall = 0.8475, Test F1 = 0.8571\n","\n","Time = 1.827 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","13.727974017886648\n","Training Accuracy = 0.9976, time = 27.600 seconds\n","\n","Running 501 samples:\n","Total loss:\n","136.11830230605108\n","Test Accuracy = 0.8762, Test Precision = 0.8852, Test Recall = 0.8296, Test F1 = 0.8565\n","\n","Time = 1.812 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","7.296038757622199\n","Training Accuracy = 0.9982, time = 27.590 seconds\n","\n","Running 501 samples:\n","Total loss:\n","148.36856085390815\n","Test Accuracy = 0.8563, Test Precision = 0.8386, Test Recall = 0.8386, Test F1 = 0.8386\n","\n","Time = 1.764 seconds\n","\n","Cross Validation step :  8\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","481.53192874230444\n","Training Accuracy = 0.7862, time = 30.971 seconds\n","\n","Running 501 samples:\n","Total loss:\n","41.68260043533519\n","Test Accuracy = 0.8124, Test Precision = 0.7660, Test Recall = 0.8852, Test F1 = 0.8213\n","\n","Time = 1.617 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","296.2805439524236\n","Training Accuracy = 0.8769, time = 27.434 seconds\n","\n","Running 501 samples:\n","Total loss:\n","42.22625406016596\n","Test Accuracy = 0.8224, Test Precision = 0.7860, Test Recall = 0.8730, Test F1 = 0.8272\n","\n","Time = 1.732 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","168.57752785492812\n","Training Accuracy = 0.9352, time = 27.831 seconds\n","\n","Running 501 samples:\n","Total loss:\n","52.46974850999504\n","Test Accuracy = 0.8184, Test Precision = 0.7844, Test Recall = 0.8648, Test F1 = 0.8226\n","\n","Time = 1.721 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","100.74351555129257\n","Training Accuracy = 0.9596, time = 27.341 seconds\n","\n","Running 501 samples:\n","Total loss:\n","80.71639782451632\n","Test Accuracy = 0.8224, Test Precision = 0.7903, Test Recall = 0.8648, Test F1 = 0.8258\n","\n","Time = 1.706 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","63.04745438030988\n","Training Accuracy = 0.9798, time = 27.739 seconds\n","\n","Running 501 samples:\n","Total loss:\n","91.91340230409259\n","Test Accuracy = 0.8124, Test Precision = 0.7799, Test Recall = 0.8566, Test F1 = 0.8164\n","\n","Time = 1.700 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","50.778770497168935\n","Training Accuracy = 0.9829, time = 27.404 seconds\n","\n","Running 501 samples:\n","Total loss:\n","108.13456971781503\n","Test Accuracy = 0.8483, Test Precision = 0.8281, Test Recall = 0.8689, Test F1 = 0.8480\n","\n","Time = 1.744 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","35.83430984760998\n","Training Accuracy = 0.9882, time = 27.331 seconds\n","\n","Running 501 samples:\n","Total loss:\n","99.96414887803786\n","Test Accuracy = 0.8483, Test Precision = 0.8500, Test Recall = 0.8361, Test F1 = 0.8430\n","\n","Time = 1.745 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","31.311664827893964\n","Training Accuracy = 0.9909, time = 27.436 seconds\n","\n","Running 501 samples:\n","Total loss:\n","112.87877851395517\n","Test Accuracy = 0.8623, Test Precision = 0.8788, Test Recall = 0.8320, Test F1 = 0.8547\n","\n","Time = 1.709 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","23.420352042577647\n","Training Accuracy = 0.9933, time = 27.375 seconds\n","\n","Running 501 samples:\n","Total loss:\n","123.07038600578285\n","Test Accuracy = 0.8603, Test Precision = 0.8625, Test Recall = 0.8484, Test F1 = 0.8554\n","\n","Time = 1.735 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","17.796582292032262\n","Training Accuracy = 0.9956, time = 27.687 seconds\n","\n","Running 501 samples:\n","Total loss:\n","155.32091390859642\n","Test Accuracy = 0.8343, Test Precision = 0.8426, Test Recall = 0.8115, Test F1 = 0.8267\n","\n","Time = 1.714 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","14.464062046457954\n","Training Accuracy = 0.9960, time = 27.424 seconds\n","\n","Running 501 samples:\n","Total loss:\n","173.625202433673\n","Test Accuracy = 0.8663, Test Precision = 0.9078, Test Recall = 0.8074, Test F1 = 0.8547\n","\n","Time = 1.770 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","20.252237614361015\n","Training Accuracy = 0.9960, time = 27.418 seconds\n","\n","Running 501 samples:\n","Total loss:\n","190.56558314889676\n","Test Accuracy = 0.8403, Test Precision = 0.8333, Test Recall = 0.8402, Test F1 = 0.8367\n","\n","Time = 1.776 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","17.106153678847416\n","Training Accuracy = 0.9956, time = 27.336 seconds\n","\n","Running 501 samples:\n","Total loss:\n","119.5693887346043\n","Test Accuracy = 0.8583, Test Precision = 0.8914, Test Recall = 0.8074, Test F1 = 0.8473\n","\n","Time = 1.598 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","14.30965775596892\n","Training Accuracy = 0.9960, time = 27.807 seconds\n","\n","Running 501 samples:\n","Total loss:\n","155.36363829245013\n","Test Accuracy = 0.8603, Test Precision = 0.8955, Test Recall = 0.8074, Test F1 = 0.8491\n","\n","Time = 1.733 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","14.395402017929356\n","Training Accuracy = 0.9967, time = 27.218 seconds\n","\n","Running 501 samples:\n","Total loss:\n","160.5644608626507\n","Test Accuracy = 0.8543, Test Precision = 0.8977, Test Recall = 0.7910, Test F1 = 0.8410\n","\n","Time = 1.756 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","9.087249771500012\n","Training Accuracy = 0.9960, time = 27.266 seconds\n","\n","Running 501 samples:\n","Total loss:\n","195.4589735931583\n","Test Accuracy = 0.8483, Test Precision = 0.8360, Test Recall = 0.8566, Test F1 = 0.8462\n","\n","Time = 1.737 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","16.220414610984633\n","Training Accuracy = 0.9965, time = 27.579 seconds\n","\n","Running 501 samples:\n","Total loss:\n","169.49016461826065\n","Test Accuracy = 0.8463, Test Precision = 0.8465, Test Recall = 0.8361, Test F1 = 0.8412\n","\n","Time = 1.603 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","15.365519718785368\n","Training Accuracy = 0.9969, time = 27.490 seconds\n","\n","Running 501 samples:\n","Total loss:\n","202.2891743237332\n","Test Accuracy = 0.8463, Test Precision = 0.8523, Test Recall = 0.8279, Test F1 = 0.8399\n","\n","Time = 1.800 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","8.798553494524072\n","Training Accuracy = 0.9976, time = 27.188 seconds\n","\n","Running 501 samples:\n","Total loss:\n","198.56418637577593\n","Test Accuracy = 0.8603, Test Precision = 0.8919, Test Recall = 0.8115, Test F1 = 0.8498\n","\n","Time = 1.739 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","4.055572302740078\n","Training Accuracy = 0.9991, time = 27.840 seconds\n","\n","Running 501 samples:\n","Total loss:\n","175.2856013458772\n","Test Accuracy = 0.8443, Test Precision = 0.8430, Test Recall = 0.8361, Test F1 = 0.8395\n","\n","Time = 1.751 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","8.800790200025327\n","Training Accuracy = 0.9984, time = 27.308 seconds\n","\n","Running 501 samples:\n","Total loss:\n","264.7341931002681\n","Test Accuracy = 0.8423, Test Precision = 0.8287, Test Recall = 0.8525, Test F1 = 0.8404\n","\n","Time = 1.783 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","13.162859759437797\n","Training Accuracy = 0.9978, time = 27.563 seconds\n","\n","Running 501 samples:\n","Total loss:\n","166.11694818620455\n","Test Accuracy = 0.8643, Test Precision = 0.8894, Test Recall = 0.8238, Test F1 = 0.8553\n","\n","Time = 1.750 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","5.782774241656444\n","Training Accuracy = 0.9984, time = 27.481 seconds\n","\n","Running 501 samples:\n","Total loss:\n","231.98330630188065\n","Test Accuracy = 0.8563, Test Precision = 0.8909, Test Recall = 0.8033, Test F1 = 0.8448\n","\n","Time = 1.774 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","6.475086060948028\n","Training Accuracy = 0.9989, time = 27.658 seconds\n","\n","Running 501 samples:\n","Total loss:\n","193.28179391572488\n","Test Accuracy = 0.8523, Test Precision = 0.8864, Test Recall = 0.7992, Test F1 = 0.8405\n","\n","Time = 1.758 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","2.4836137902879756\n","Training Accuracy = 0.9991, time = 27.696 seconds\n","\n","Running 501 samples:\n","Total loss:\n","284.7348839194516\n","Test Accuracy = 0.8583, Test Precision = 0.8914, Test Recall = 0.8074, Test F1 = 0.8473\n","\n","Time = 1.769 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","11.710769692319971\n","Training Accuracy = 0.9984, time = 27.275 seconds\n","\n","Running 501 samples:\n","Total loss:\n","277.5004229087185\n","Test Accuracy = 0.8483, Test Precision = 0.8925, Test Recall = 0.7828, Test F1 = 0.8341\n","\n","Time = 1.798 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","11.560995891368933\n","Training Accuracy = 0.9984, time = 27.504 seconds\n","\n","Running 501 samples:\n","Total loss:\n","216.44336266709072\n","Test Accuracy = 0.8463, Test Precision = 0.8646, Test Recall = 0.8115, Test F1 = 0.8372\n","\n","Time = 1.787 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","2.0518444428077736\n","Training Accuracy = 0.9991, time = 27.532 seconds\n","\n","Running 501 samples:\n","Total loss:\n","275.2833977080105\n","Test Accuracy = 0.8503, Test Precision = 0.8627, Test Recall = 0.8238, Test F1 = 0.8428\n","\n","Time = 1.756 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","12.689244840895022\n","Training Accuracy = 0.9984, time = 27.388 seconds\n","\n","Running 501 samples:\n","Total loss:\n","176.38789692736384\n","Test Accuracy = 0.8483, Test Precision = 0.8529, Test Recall = 0.8320, Test F1 = 0.8423\n","\n","Time = 1.779 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","2.663762757405866\n","Training Accuracy = 0.9996, time = 27.692 seconds\n","\n","Running 501 samples:\n","Total loss:\n","191.41531602835119\n","Test Accuracy = 0.8523, Test Precision = 0.8512, Test Recall = 0.8443, Test F1 = 0.8477\n","\n","Time = 1.661 seconds\n","\n","Cross Validation step :  9\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","480.5780076831579\n","Training Accuracy = 0.7793, time = 30.893 seconds\n","\n","Running 501 samples:\n","Total loss:\n","58.68932950674207\n","Test Accuracy = 0.7325, Test Precision = 0.6649, Test Recall = 0.9730, Test F1 = 0.7900\n","\n","Time = 1.654 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","307.6141922605457\n","Training Accuracy = 0.8678, time = 27.649 seconds\n","\n","Running 501 samples:\n","Total loss:\n","38.12066145332301\n","Test Accuracy = 0.8343, Test Precision = 0.7876, Test Recall = 0.9305, Test F1 = 0.8531\n","\n","Time = 1.574 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","175.28958366879033\n","Training Accuracy = 0.9312, time = 27.630 seconds\n","\n","Running 501 samples:\n","Total loss:\n","46.992739532724954\n","Test Accuracy = 0.8563, Test Precision = 0.8169, Test Recall = 0.9305, Test F1 = 0.8700\n","\n","Time = 1.693 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","105.10197749932868\n","Training Accuracy = 0.9623, time = 27.652 seconds\n","\n","Running 501 samples:\n","Total loss:\n","67.52300178455698\n","Test Accuracy = 0.8483, Test Precision = 0.8040, Test Recall = 0.9344, Test F1 = 0.8643\n","\n","Time = 1.780 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","66.62788317953597\n","Training Accuracy = 0.9792, time = 27.845 seconds\n","\n","Running 501 samples:\n","Total loss:\n","62.03844890760057\n","Test Accuracy = 0.8782, Test Precision = 0.9057, Test Recall = 0.8533, Test F1 = 0.8787\n","\n","Time = 1.713 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","60.495099688203894\n","Training Accuracy = 0.9796, time = 27.671 seconds\n","\n","Running 501 samples:\n","Total loss:\n","92.6164397807035\n","Test Accuracy = 0.8543, Test Precision = 0.8419, Test Recall = 0.8842, Test F1 = 0.8625\n","\n","Time = 1.699 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","37.422418881568404\n","Training Accuracy = 0.9889, time = 27.493 seconds\n","\n","Running 501 samples:\n","Total loss:\n","83.048298637512\n","Test Accuracy = 0.8942, Test Precision = 0.9187, Test Recall = 0.8726, Test F1 = 0.8950\n","\n","Time = 1.725 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","37.74665692530767\n","Training Accuracy = 0.9885, time = 27.655 seconds\n","\n","Running 501 samples:\n","Total loss:\n","89.21662796770346\n","Test Accuracy = 0.8723, Test Precision = 0.8764, Test Recall = 0.8764, Test F1 = 0.8764\n","\n","Time = 1.715 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","26.393540149058563\n","Training Accuracy = 0.9916, time = 27.683 seconds\n","\n","Running 501 samples:\n","Total loss:\n","107.14825545646292\n","Test Accuracy = 0.8842, Test Precision = 0.8911, Test Recall = 0.8842, Test F1 = 0.8876\n","\n","Time = 1.762 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","19.294689809245444\n","Training Accuracy = 0.9958, time = 27.276 seconds\n","\n","Running 501 samples:\n","Total loss:\n","105.47020533871309\n","Test Accuracy = 0.8902, Test Precision = 0.8984, Test Recall = 0.8880, Test F1 = 0.8932\n","\n","Time = 1.652 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","21.13368938967989\n","Training Accuracy = 0.9931, time = 28.057 seconds\n","\n","Running 501 samples:\n","Total loss:\n","90.46609894123036\n","Test Accuracy = 0.8882, Test Precision = 0.9109, Test Recall = 0.8687, Test F1 = 0.8893\n","\n","Time = 1.772 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","14.052770231208292\n","Training Accuracy = 0.9962, time = 27.259 seconds\n","\n","Running 501 samples:\n","Total loss:\n","136.8759790722242\n","Test Accuracy = 0.8822, Test Precision = 0.8597, Test Recall = 0.9228, Test F1 = 0.8901\n","\n","Time = 1.665 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","27.218091773505357\n","Training Accuracy = 0.9940, time = 27.316 seconds\n","\n","Running 501 samples:\n","Total loss:\n","84.83189192854934\n","Test Accuracy = 0.8842, Test Precision = 0.8736, Test Recall = 0.9073, Test F1 = 0.8902\n","\n","Time = 1.757 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","7.7164220883577705\n","Training Accuracy = 0.9980, time = 27.363 seconds\n","\n","Running 501 samples:\n","Total loss:\n","141.77120909113873\n","Test Accuracy = 0.8862, Test Precision = 0.9106, Test Recall = 0.8649, Test F1 = 0.8871\n","\n","Time = 1.654 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","16.114681464616716\n","Training Accuracy = 0.9969, time = 28.129 seconds\n","\n","Running 501 samples:\n","Total loss:\n","146.9545870890513\n","Test Accuracy = 0.8822, Test Precision = 0.8788, Test Recall = 0.8958, Test F1 = 0.8872\n","\n","Time = 1.751 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","18.386276944187546\n","Training Accuracy = 0.9960, time = 27.405 seconds\n","\n","Running 501 samples:\n","Total loss:\n","108.53847761653313\n","Test Accuracy = 0.8743, Test Precision = 0.8712, Test Recall = 0.8880, Test F1 = 0.8795\n","\n","Time = 1.768 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","7.609120366205509\n","Training Accuracy = 0.9984, time = 27.869 seconds\n","\n","Running 501 samples:\n","Total loss:\n","100.40938660820028\n","Test Accuracy = 0.8703, Test Precision = 0.9110, Test Recall = 0.8301, Test F1 = 0.8687\n","\n","Time = 1.744 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","5.903816368009974\n","Training Accuracy = 0.9976, time = 27.459 seconds\n","\n","Running 501 samples:\n","Total loss:\n","139.8949995643733\n","Test Accuracy = 0.8782, Test Precision = 0.9091, Test Recall = 0.8494, Test F1 = 0.8782\n","\n","Time = 1.773 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","7.048046690123979\n","Training Accuracy = 0.9984, time = 27.863 seconds\n","\n","Running 501 samples:\n","Total loss:\n","150.84467241912935\n","Test Accuracy = 0.8743, Test Precision = 0.8858, Test Recall = 0.8687, Test F1 = 0.8772\n","\n","Time = 1.600 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","9.866039309616017\n","Training Accuracy = 0.9973, time = 27.700 seconds\n","\n","Running 501 samples:\n","Total loss:\n","161.87943691216194\n","Test Accuracy = 0.8842, Test Precision = 0.8941, Test Recall = 0.8803, Test F1 = 0.8872\n","\n","Time = 1.696 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","1.7325987172983375\n","Training Accuracy = 0.9996, time = 27.652 seconds\n","\n","Running 501 samples:\n","Total loss:\n","219.67660403630683\n","Test Accuracy = 0.8882, Test Precision = 0.8919, Test Recall = 0.8919, Test F1 = 0.8919\n","\n","Time = 1.782 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","7.936190896795285\n","Training Accuracy = 0.9982, time = 27.626 seconds\n","\n","Running 501 samples:\n","Total loss:\n","193.15065378655785\n","Test Accuracy = 0.8743, Test Precision = 0.8475, Test Recall = 0.9228, Test F1 = 0.8835\n","\n","Time = 1.747 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","19.12433485368018\n","Training Accuracy = 0.9960, time = 27.335 seconds\n","\n","Running 501 samples:\n","Total loss:\n","107.68948051843577\n","Test Accuracy = 0.8882, Test Precision = 0.8830, Test Recall = 0.9035, Test F1 = 0.8931\n","\n","Time = 1.625 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","8.732764710504824\n","Training Accuracy = 0.9982, time = 27.581 seconds\n","\n","Running 501 samples:\n","Total loss:\n","110.67900767502377\n","Test Accuracy = 0.8762, Test Precision = 0.9227, Test Recall = 0.8301, Test F1 = 0.8740\n","\n","Time = 1.737 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","6.016502359452616\n","Training Accuracy = 0.9989, time = 27.745 seconds\n","\n","Running 501 samples:\n","Total loss:\n","127.8534121770154\n","Test Accuracy = 0.8902, Test Precision = 0.8923, Test Recall = 0.8958, Test F1 = 0.8940\n","\n","Time = 1.763 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","1.6291322585812829\n","Training Accuracy = 0.9996, time = 27.671 seconds\n","\n","Running 501 samples:\n","Total loss:\n","174.86614927120402\n","Test Accuracy = 0.8882, Test Precision = 0.8773, Test Recall = 0.9112, Test F1 = 0.8939\n","\n","Time = 1.835 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","0.29321029234926854\n","Training Accuracy = 1.0000, time = 27.996 seconds\n","\n","Running 501 samples:\n","Total loss:\n","202.60066584576538\n","Test Accuracy = 0.8942, Test Precision = 0.9087, Test Recall = 0.8842, Test F1 = 0.8963\n","\n","Time = 1.827 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","5.096929007766718\n","Training Accuracy = 0.9987, time = 27.584 seconds\n","\n","Running 501 samples:\n","Total loss:\n","217.17302059496325\n","Test Accuracy = 0.8782, Test Precision = 0.8667, Test Recall = 0.9035, Test F1 = 0.8847\n","\n","Time = 1.571 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","8.350135179417645\n","Training Accuracy = 0.9991, time = 27.382 seconds\n","\n","Running 501 samples:\n","Total loss:\n","187.27400808166942\n","Test Accuracy = 0.8842, Test Precision = 0.9004, Test Recall = 0.8726, Test F1 = 0.8863\n","\n","Time = 1.760 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","7.9839868499265485\n","Training Accuracy = 0.9993, time = 27.298 seconds\n","\n","Running 501 samples:\n","Total loss:\n","151.4001262253077\n","Test Accuracy = 0.8862, Test Precision = 0.8915, Test Recall = 0.8880, Test F1 = 0.8897\n","\n","Time = 1.735 seconds\n","\n","Cross Validation step :  10\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","479.5705322828144\n","Training Accuracy = 0.7713, time = 30.732 seconds\n","\n","Running 501 samples:\n","Total loss:\n","37.366675842233235\n","Test Accuracy = 0.8583, Test Precision = 0.9103, Test Recall = 0.8099, Test F1 = 0.8571\n","\n","Time = 1.735 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","295.43019174381334\n","Training Accuracy = 0.8707, time = 27.408 seconds\n","\n","Running 501 samples:\n","Total loss:\n","41.22597309061666\n","Test Accuracy = 0.8383, Test Precision = 0.9027, Test Recall = 0.7757, Test F1 = 0.8344\n","\n","Time = 1.698 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","177.5105658433822\n","Training Accuracy = 0.9299, time = 27.584 seconds\n","\n","Running 501 samples:\n","Total loss:\n","46.225884028099244\n","Test Accuracy = 0.8663, Test Precision = 0.8984, Test Recall = 0.8403, Test F1 = 0.8684\n","\n","Time = 1.722 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","94.18846834366789\n","Training Accuracy = 0.9674, time = 27.313 seconds\n","\n","Running 501 samples:\n","Total loss:\n","54.58879907060145\n","Test Accuracy = 0.8723, Test Precision = 0.8842, Test Recall = 0.8707, Test F1 = 0.8774\n","\n","Time = 1.679 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","75.24733460526622\n","Training Accuracy = 0.9727, time = 27.726 seconds\n","\n","Running 501 samples:\n","Total loss:\n","82.85549006104694\n","Test Accuracy = 0.8683, Test Precision = 0.8774, Test Recall = 0.8707, Test F1 = 0.8740\n","\n","Time = 1.712 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","51.85806875014169\n","Training Accuracy = 0.9836, time = 27.671 seconds\n","\n","Running 501 samples:\n","Total loss:\n","108.1856234355053\n","Test Accuracy = 0.8683, Test Precision = 0.8717, Test Recall = 0.8783, Test F1 = 0.8750\n","\n","Time = 1.683 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","34.34772897217896\n","Training Accuracy = 0.9907, time = 27.437 seconds\n","\n","Running 501 samples:\n","Total loss:\n","139.69555962858738\n","Test Accuracy = 0.8423, Test Precision = 0.9381, Test Recall = 0.7490, Test F1 = 0.8330\n","\n","Time = 1.723 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","38.892111772849994\n","Training Accuracy = 0.9887, time = 27.445 seconds\n","\n","Running 501 samples:\n","Total loss:\n","105.42003740286084\n","Test Accuracy = 0.8743, Test Precision = 0.8846, Test Recall = 0.8745, Test F1 = 0.8795\n","\n","Time = 1.697 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","25.7255798254835\n","Training Accuracy = 0.9922, time = 27.341 seconds\n","\n","Running 501 samples:\n","Total loss:\n","111.21822684363084\n","Test Accuracy = 0.8782, Test Precision = 0.8855, Test Recall = 0.8821, Test F1 = 0.8838\n","\n","Time = 1.737 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","10.088226042806138\n","Training Accuracy = 0.9969, time = 27.372 seconds\n","\n","Running 501 samples:\n","Total loss:\n","163.13558738548346\n","Test Accuracy = 0.8782, Test Precision = 0.9106, Test Recall = 0.8517, Test F1 = 0.8802\n","\n","Time = 1.703 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","21.602663360065176\n","Training Accuracy = 0.9942, time = 27.607 seconds\n","\n","Running 501 samples:\n","Total loss:\n","126.56071663255395\n","Test Accuracy = 0.8563, Test Precision = 0.8604, Test Recall = 0.8669, Test F1 = 0.8636\n","\n","Time = 1.727 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","14.114388077459118\n","Training Accuracy = 0.9969, time = 26.712 seconds\n","\n","Running 501 samples:\n","Total loss:\n","164.2828242976178\n","Test Accuracy = 0.8822, Test Precision = 0.9048, Test Recall = 0.8669, Test F1 = 0.8854\n","\n","Time = 1.589 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","17.641091045478223\n","Training Accuracy = 0.9953, time = 25.842 seconds\n","\n","Running 501 samples:\n","Total loss:\n","142.2347755590309\n","Test Accuracy = 0.8663, Test Precision = 0.8740, Test Recall = 0.8707, Test F1 = 0.8724\n","\n","Time = 1.575 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","12.402876564894903\n","Training Accuracy = 0.9976, time = 25.602 seconds\n","\n","Running 501 samples:\n","Total loss:\n","163.82046648258293\n","Test Accuracy = 0.8802, Test Precision = 0.9143, Test Recall = 0.8517, Test F1 = 0.8819\n","\n","Time = 1.554 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","11.906263331502128\n","Training Accuracy = 0.9976, time = 25.790 seconds\n","\n","Running 501 samples:\n","Total loss:\n","203.37757945921322\n","Test Accuracy = 0.8782, Test Precision = 0.9139, Test Recall = 0.8479, Test F1 = 0.8797\n","\n","Time = 1.559 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","12.618914543605754\n","Training Accuracy = 0.9982, time = 26.467 seconds\n","\n","Running 501 samples:\n","Total loss:\n","146.31134809974512\n","Test Accuracy = 0.8663, Test Precision = 0.8889, Test Recall = 0.8517, Test F1 = 0.8699\n","\n","Time = 1.572 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","14.582582889283028\n","Training Accuracy = 0.9976, time = 26.014 seconds\n","\n","Running 501 samples:\n","Total loss:\n","159.42363312293702\n","Test Accuracy = 0.8782, Test Precision = 0.9316, Test Recall = 0.8289, Test F1 = 0.8773\n","\n","Time = 1.556 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","22.721652372163792\n","Training Accuracy = 0.9956, time = 26.287 seconds\n","\n","Running 501 samples:\n","Total loss:\n","114.1611122937828\n","Test Accuracy = 0.8523, Test Precision = 0.8677, Test Recall = 0.8479, Test F1 = 0.8577\n","\n","Time = 1.563 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","10.413456526057018\n","Training Accuracy = 0.9978, time = 26.113 seconds\n","\n","Running 501 samples:\n","Total loss:\n","163.49537344444352\n","Test Accuracy = 0.8703, Test Precision = 0.9195, Test Recall = 0.8251, Test F1 = 0.8697\n","\n","Time = 1.551 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","14.181793865012708\n","Training Accuracy = 0.9967, time = 25.499 seconds\n","\n","Running 501 samples:\n","Total loss:\n","120.43174720810214\n","Test Accuracy = 0.8683, Test Precision = 0.9053, Test Recall = 0.8365, Test F1 = 0.8696\n","\n","Time = 1.568 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","3.3071187817075938\n","Training Accuracy = 0.9984, time = 25.649 seconds\n","\n","Running 501 samples:\n","Total loss:\n","190.13996038765382\n","Test Accuracy = 0.8782, Test Precision = 0.9139, Test Recall = 0.8479, Test F1 = 0.8797\n","\n","Time = 1.545 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","4.4610769685639795\n","Training Accuracy = 0.9989, time = 25.718 seconds\n","\n","Running 501 samples:\n","Total loss:\n","191.0406967740498\n","Test Accuracy = 0.8822, Test Precision = 0.9146, Test Recall = 0.8555, Test F1 = 0.8841\n","\n","Time = 1.562 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","13.276482896492851\n","Training Accuracy = 0.9978, time = 25.557 seconds\n","\n","Running 501 samples:\n","Total loss:\n","92.57585157974694\n","Test Accuracy = 0.8802, Test Precision = 0.8889, Test Recall = 0.8821, Test F1 = 0.8855\n","\n","Time = 1.550 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","8.763468556343557\n","Training Accuracy = 0.9989, time = 25.804 seconds\n","\n","Running 501 samples:\n","Total loss:\n","144.90644835857705\n","Test Accuracy = 0.8862, Test Precision = 0.9328, Test Recall = 0.8441, Test F1 = 0.8862\n","\n","Time = 1.570 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","9.953169544472669\n","Training Accuracy = 0.9984, time = 25.838 seconds\n","\n","Running 501 samples:\n","Total loss:\n","131.79625845980885\n","Test Accuracy = 0.8683, Test Precision = 0.8689, Test Recall = 0.8821, Test F1 = 0.8755\n","\n","Time = 1.553 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","6.673857038941822\n","Training Accuracy = 0.9991, time = 25.594 seconds\n","\n","Running 501 samples:\n","Total loss:\n","129.81878950747495\n","Test Accuracy = 0.8683, Test Precision = 0.9020, Test Recall = 0.8403, Test F1 = 0.8701\n","\n","Time = 1.555 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","0.8876215064369326\n","Training Accuracy = 0.9998, time = 25.563 seconds\n","\n","Running 501 samples:\n","Total loss:\n","192.66021631398195\n","Test Accuracy = 0.8743, Test Precision = 0.9425, Test Recall = 0.8099, Test F1 = 0.8712\n","\n","Time = 1.584 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","5.972368722516773\n","Training Accuracy = 0.9984, time = 25.775 seconds\n","\n","Running 501 samples:\n","Total loss:\n","233.63998702178884\n","Test Accuracy = 0.8703, Test Precision = 0.9342, Test Recall = 0.8099, Test F1 = 0.8676\n","\n","Time = 1.559 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","4.518897033397565\n","Training Accuracy = 0.9991, time = 25.427 seconds\n","\n","Running 501 samples:\n","Total loss:\n","227.76832527857076\n","Test Accuracy = 0.8762, Test Precision = 0.9136, Test Recall = 0.8441, Test F1 = 0.8775\n","\n","Time = 1.568 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","4.9693037093142856\n","Training Accuracy = 0.9991, time = 26.007 seconds\n","\n","Running 501 samples:\n","Total loss:\n","155.49868493449065\n","Test Accuracy = 0.8762, Test Precision = 0.9136, Test Recall = 0.8441, Test F1 = 0.8775\n","\n","Time = 1.551 seconds\n","\n","final Accuracy :  0.8549035262807718\n","final precision :  0.8445104476056485\n","final recall :  0.8678447121820616\n","final f1 :  0.8560185918766174\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H7toABPhbeV5","colab_type":"text"},"source":["### Maximum accuracy fasttext without removing puncuations **85.66**\n","\n","Results from fast text after removing puncuations\n","final Accuracy :  0.8738522954091816\n","final precision :  0.9021142508639968\n","final recall :  0.850095785440613\n","final f1 :  0.8753328730644049"]},{"cell_type":"code","metadata":{"id":"NGpAcB9E7GN1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoYNLo9pOZBk","colab_type":"text"},"source":["# Cross Validation"]},{"cell_type":"code","metadata":{"id":"ccCXP2m9NJwf","colab_type":"code","colab":{}},"source":["def start_test_epoches(config, session,classifier, train_dataset, test_dataset):\n","    #record max\n","    #max_val_acc=-1\n","    #max_test_acc=-1\n"," \n","    all_actual = []\n","    all_predictions = []\n","    for i in range(config.max_max_epoch):\n","        actual, prediction = test_model(config, i, session, classifier, train_dataset, test_dataset)\n","        all_actual.extend(actual)\n","        all_predictions.extend(prediction)\n","\n","\n","    TN, FP, FN, TP = confusion_matrix(all_actual, all_predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","    accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","    print(\"final Accuracy : \",accuracy)\n","    print(\"final precision : \",precision)\n","    print(\"final recall : \",recall)\n","    print(\"final f1 : \",f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqeU8wHGNbeR","colab_type":"code","colab":{}},"source":["def test_model(config, i, session, model, train_dataset,test_dataset):\n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","\n","    #testing\n","    start_time = time.time()\n","    test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    #return valid_acc, test_acc\n","\n","    return test_actual,test_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH-o5b7dwH9G","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","\n","with tf.Graph().as_default(), tf.Session() as session:\n","  #initializer = tf.random_normal_initializer(0, 0.05)\n","\n","  classifier= Classifer(config=config, session=session)\n","  saver = tf.train.Saver()\n","  \n","  init = tf.global_variables_initializer()\n","  session.run(init)\n","\n","  saver = tf.train.Saver()\n","\n","  tf.train.Saver().restore(session,tf.train.latest_checkpoint(\"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/100epochs/\") )\n","  print(\"Model restored.\")\n","\n","  word_to_vec(matrix, session,config, classifier)\n","  start_test_epoches(config, session,classifier, train_dataset, test_dataset)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"249mSY2c7IbK","colab_type":"text"},"source":["# Testing Scripts"]},{"cell_type":"code","metadata":{"id":"FWcl31Qd7M38","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n","\n","i = 1\n","kf = KFold(n_splits=5)\n","kf.get_n_splits(all_dataset[0])\n","\n","for train_index, test_index in kf.split(all_dataset[0]):\n","\n","  train_data_comments, test_data_comments = all_dataset[0][train_index], all_dataset[0][test_index]\n","  train_data_labels, test_data_labels = all_dataset[1][train_index], all_dataset[1][test_index]\n","  train_lengths, test_lengths = all_dataset[2][train_index], all_dataset[2][test_index]\n"," \n","  train_data_set = [train_data_comments,train_data_labels,train_lengths]\n","  test_data_set = [test_data_comments,test_data_labels,test_lengths]\n","  print(len(test_data_set))\n","\n","\n","\n","  \n","  break\n","  i +=1\n","  print(\"Iteration : \",i)\n"],"execution_count":0,"outputs":[]}]}