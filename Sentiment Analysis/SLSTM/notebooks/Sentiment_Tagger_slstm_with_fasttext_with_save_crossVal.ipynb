{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment_Tagger_slstm_with_fasttext_with_save_crossVal.ipynb","provenance":[],"collapsed_sections":["W1kYrhsN6xEs","B4wf4YZ9RUW-","0n2fJGeq63I7","6w_fbBM2Qp1O","XeFsOVyYQ7y6","BZa7J54XRvwG","ddbrDIW8SsV4"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"t48aRicQOinf","colab_type":"code","outputId":"b57789ef-f031-4537-cb1b-e80fccd086d1","executionInfo":{"status":"ok","timestamp":1584545204995,"user_tz":-330,"elapsed":22005,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bG9mz-zbS8Rt","colab_type":"code","colab":{}},"source":["path='/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/SLSTM/parsed_data/from_fasttext/data_set'\n","vector_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/SLSTM/parsed_data/from_fasttext/fasttext_vectors'\n","# run from lahiru1st@gmail.com\n","# path='/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/data_set'\n","# vector_path = '/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/fasttext_vectors'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwA4weGRP9OQ","colab_type":"text"},"source":["# Imports"]},{"cell_type":"markdown","metadata":{"id":"W1kYrhsN6xEs","colab_type":"text"},"source":["## Standard Imports"]},{"cell_type":"code","metadata":{"id":"OXwBif125t0Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"61b1583e-d5ea-48e2-f4fb-835904977492","executionInfo":{"status":"ok","timestamp":1584545217376,"user_tz":-330,"elapsed":6597,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["from __future__ import print_function\n","# import six.moves.cPickle as pickle\n","# from collections import OrderedDict\n","import sys\n","import time\n","import numpy as np\n","import tensorflow as tf\n","# import read_data\n","# from random import shuffle\n","# import random\n","import pickle\n","# import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import confusion_matrix\n","# from general_utils import Progbar\n","# import tensorflow.contrib.slim as slim\n","# from sst_config import Config\n","from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"0T3y_TelRPlY","colab_type":"text"},"source":["## config"]},{"cell_type":"code","metadata":{"id":"kiZjosbeRMm-","colab_type":"code","colab":{}},"source":["class Config(object):\n","    vocab_size=15000\n","    max_grad_norm = 5\n","    init_scale = 0.05\n","    hidden_size = 300\n","    lr_decay = 0.95\n","    valid_portion=0.0\n","    batch_size=5\n","    keep_prob = 0.5\n","    #0.05\n","    learning_rate = 0.001\n","    max_epoch =2\n","    # max_max_epoch =40\n","    max_max_epoch = 30\n","    num_label=5\n","    attention_iteration=3\n","    random_initialize=False\n","    embedding_trainable=True\n","    l2_beta=0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4wf4YZ9RUW-","colab_type":"text"},"source":["## Progbar"]},{"cell_type":"code","metadata":{"id":"HQxItjCIPrgH","colab_type":"code","colab":{}},"source":["import time\n","import sys\n","import logging\n","import numpy as np\n","\n","\n","class Progbar(object):\n","    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n","\n","    Displays a progress bar.\n","    Small edit : added strict arg to update\n","    # Arguments\n","        target: Total number of steps expected.\n","        interval: Minimum visual progress update interval (in seconds).\n","    \"\"\"\n","\n","    def __init__(self, target, width=30, verbose=0):\n","        self.width = width\n","        self.target = target\n","        self.sum_values = {}\n","        self.unique_values = []\n","        self.start = time.time()\n","        self.total_width = 0\n","        self.seen_so_far = 0\n","        self.verbose = verbose\n","\n","    def update(self, current, values=[], exact=[], strict=[]):\n","        \"\"\"\n","        Updates the progress bar.\n","        # Arguments\n","            current: Index of current step.\n","            values: List of tuples (name, value_for_last_step).\n","                The progress bar will display averages for these values.\n","            exact: List of tuples (name, value_for_last_step).\n","                The progress bar will display these values directly.\n","        \"\"\"\n","\n","        for k, v in values:\n","            if k not in self.sum_values:\n","                self.sum_values[k] = [v * (current - self.seen_so_far),\n","                                      current - self.seen_so_far]\n","                self.unique_values.append(k)\n","            else:\n","                self.sum_values[k][0] += v * (current - self.seen_so_far)\n","                self.sum_values[k][1] += (current - self.seen_so_far)\n","        for k, v in exact:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = [v, 1]\n","\n","        for k, v in strict:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = v\n","\n","        self.seen_so_far = current\n","\n","        now = time.time()\n","        if self.verbose == 1:\n","            prev_total_width = self.total_width\n","            sys.stdout.write(\"\\b\" * prev_total_width)\n","            sys.stdout.write(\"\\r\")\n","\n","            numdigits = int(np.floor(np.log10(self.target))) + 1\n","            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n","            bar = barstr % (current, self.target)\n","            prog = float(current)/self.target\n","            prog_width = int(self.width*prog)\n","            if prog_width > 0:\n","                bar += ('='*(prog_width-1))\n","                if current < self.target:\n","                    bar += '>'\n","                else:\n","                    bar += '='\n","            bar += ('.'*(self.width-prog_width))\n","            bar += ']'\n","            sys.stdout.write(bar)\n","            self.total_width = len(bar)\n","\n","            if current:\n","                time_per_unit = (now - self.start) / current\n","            else:\n","                time_per_unit = 0\n","            eta = time_per_unit*(self.target - current)\n","            info = ''\n","            if current < self.target:\n","                info += ' - ETA: %ds' % eta\n","            else:\n","                info += ' - %ds' % (now - self.start)\n","            for k in self.unique_values:\n","                if type(self.sum_values[k]) is list:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                else:\n","                    info += ' - %s: %s' % (k, self.sum_values[k])\n","\n","            self.total_width += len(info)\n","            if prev_total_width > self.total_width:\n","                info += ((prev_total_width-self.total_width) * \" \")\n","\n","            sys.stdout.write(info)\n","            sys.stdout.flush()\n","\n","            if current >= self.target:\n","                sys.stdout.write(\"\\n\")\n","\n","        if self.verbose == 2:\n","            if current >= self.target:\n","                info = '%ds' % (now - self.start)\n","                for k in self.unique_values:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                sys.stdout.write(info + \"\\n\")\n","\n","    def add(self, n, values=[]):\n","        self.update(self.seen_so_far+n, values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0n2fJGeq63I7","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"code","metadata":{"id":"_pSHY_uE66j1","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","from six.moves import xrange\n","import six.moves.cPickle as pickle\n","import gzip\n","import os\n","import numpy\n","\n","def generate_matrix(seqs, maxlen, lengths):\n","    n_samples = len(seqs)\n","    x= numpy.zeros((n_samples, maxlen)).astype('int64')\n","\n","    for idx, s in enumerate(seqs):\n","        if lengths[idx]>= maxlen:\n","            s=s[:maxlen]\n","        x[idx, :lengths[idx]] = s\n","    return x\n","\n","def prepare_data(seqs, labels):\n","    lengths = [len(s) for s in seqs]\n","    labels = numpy.array(labels).astype('int32')\n","    return [numpy.array(seqs), labels, numpy.array(lengths).astype('int32')]\n","\n","def remove_unk(x, n_words):\n","    return [[1 if w >= n_words else w for w in sen] for sen in x] \n","\n","def load_data(path, n_words):\n","    with open(path, 'rb') as f:\n","        dataset_x, dataset_label= pickle.load(f)\n","        train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","        # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","        test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","    train_set_x = remove_unk(train_set_x, n_words)\n","    # valid_set_x = remove_unk(valid_set_x, n_words)\n","    test_set_x = remove_unk(test_set_x, n_words)\n","\n","    return [train_set_x, train_set_y],[test_set_x, test_set_y]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_1MavSboWVk","colab_type":"code","colab":{}},"source":["def load_data_for_crossVal(path, n_words):\n","  with open(path, 'rb') as f:\n","    dataset_x, dataset_label= pickle.load(f)\n","    train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","    # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","    test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","  train_set_x = remove_unk(train_set_x, n_words)\n","  # valid_set_x = remove_unk(valid_set_x, n_words)\n","  test_set_x = remove_unk(test_set_x, n_words)\n","\n","  \n","\n","  train_set_x.extend(test_set_x)\n","  dataset_label[0].extend( dataset_label[1])\n","\n","  return train_set_x,dataset_label[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEgTqwtlQanA","colab_type":"text"},"source":["# Implementation"]},{"cell_type":"markdown","metadata":{"id":"lQArWlWmQgCQ","colab_type":"text"},"source":["## LSTM Layer"]},{"cell_type":"code","metadata":{"id":"9chxIjuYONVw","colab_type":"code","colab":{}},"source":["def lstm_layer(initial_hidden_states, config, keep_prob, mask):\n","    with tf.variable_scope('forward'):\n","        fw_lstm = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        fw_lstm = tf.contrib.rnn.DropoutWrapper(fw_lstm, output_keep_prob=keep_prob)\n","\n","    with tf.variable_scope('backward'):\n","        bw_lstm = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        bw_lstm = tf.contrib.rnn.DropoutWrapper(bw_lstm, output_keep_prob=keep_prob)\n","\n","    #bidirectional rnn\n","    with tf.variable_scope('bilstm'):\n","        lstm_output=tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, inputs=initial_hidden_states, sequence_length=mask, time_major=False, dtype=tf.float32)\n","        lstm_output=tf.concat(lstm_output[0], 2)\n","\n","    return lstm_output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTmP1ZQbQlTv","colab_type":"text"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"Z3LlPobkOq1_","colab_type":"code","colab":{}},"source":["class Classifer(object):\n","\n","    def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,:-step,:]\n","        #concat padding\n","        return tf.concat([padding, displaced_hidden_states], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,step:,:]\n","        #concat padding\n","        return tf.concat([displaced_hidden_states, padding], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def sum_together(self, l):\n","        combined_state=None\n","        for tensor in l:\n","            if combined_state==None:\n","                combined_state=tensor\n","            else:\n","                combined_state=combined_state+tensor\n","        return combined_state\n","    \n","    def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n","        with tf.name_scope(name_scope_name):\n","            #Word parameters \n","            #forget gate for left \n","            with tf.name_scope(\"f1_gate\"):\n","                #current\n","                Wxf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                #left right\n","                Whf1 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                #initial state\n","                Wif1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                #dummy node\n","                Wdf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for right \n","            with tf.name_scope(\"f2_gate\"):\n","                Wxf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf2 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for inital states     \n","            with tf.name_scope(\"f3_gate\"):\n","                Wxf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf3 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for dummy states     \n","            with tf.name_scope(\"f4_gate\"):\n","                Wxf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf4 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #input gate for current state     \n","            with tf.name_scope(\"i_gate\"):\n","                Wxi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n","                Whi = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n","                Wii = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n","                Wdi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n","            #input gate for output gate\n","            with tf.name_scope(\"o_gate\"):\n","                Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                Who = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","                Wio = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n","                Wdo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n","            #bias for the gates    \n","            with tf.name_scope(\"biases\"):\n","                bi = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                bf1 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n","                bf2 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n","                bf3 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n","                bf4 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n","\n","            #dummy node gated attention parameters\n","            #input gate for dummy state\n","            with tf.name_scope(\"gated_d_gate\"):\n","                gated_Wxd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                gated_Whd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","            #output gate\n","            with tf.name_scope(\"gated_o_gate\"):\n","                gated_Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Who = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #forget gate for states of word\n","            with tf.name_scope(\"gated_f_gate\"):\n","                gated_Wxf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Whf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #biases\n","            with tf.name_scope(\"gated_biases\"):\n","                gated_bd = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                gated_bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                gated_bf = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","\n","        #filters for attention        \n","        mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n","        mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, dim=2)               \n","        #filter invalid steps\n","        sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n","        #filter embedding states\n","        initial_hidden_states=initial_hidden_states*sequence_mask\n","        initial_cell_states=initial_cell_states*sequence_mask\n","        #record shape of the batch\n","        shape=tf.shape(initial_hidden_states)\n","        \n","        #initial embedding states\n","        embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n","        embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","        #randomly initialize the states\n","        if config.random_initialize:\n","            initial_hidden_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            initial_cell_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            #filter it\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","        #inital dummy node states\n","        dummynode_hidden_states=tf.reduce_mean(initial_hidden_states, axis=1)\n","        dummynode_cell_states=tf.reduce_mean(initial_cell_states, axis=1)\n","\n","        for i in range(num_layers):\n","            #update dummy node states\n","            #average states\n","            combined_word_hidden_state=tf.reduce_mean(initial_hidden_states, axis=1)\n","            reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            #copy dummy states for computing forget gate\n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            #input gate\n","            gated_d_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n","            )\n","            #output gate\n","            gated_o_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n","            )\n","            #forget gate for hidden states\n","            gated_f_t = tf.nn.sigmoid(\n","                tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n","            )\n","\n","            #softmax on each hidden dimension \n","            reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n","            gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, dim=1)], axis=1), dim=1)\n","            #split the softmax scores\n","            new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n","            new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n","            #new dummy states\n","            dummy_c_t=tf.reduce_sum(new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n","            dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n","\n","            #update word node states\n","            #get states before\n","            initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n","            initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n","            #get states after\n","            initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_before=self.sum_together(initial_cell_states_before)\n","            initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_after=self.sum_together(initial_cell_states_after)\n","            \n","            #reshape for matmul\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","            #concat before and after hidden states\n","            concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n","\n","            #copy dummy node states \n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","\n","            f1_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n","                tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n","            )\n","\n","            f2_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n","                tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n","            )\n","\n","            f3_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n","                tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n","            )\n","\n","            f4_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n","                tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n","            )\n","            \n","            i_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n","                tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n","            )\n","            \n","            o_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n","                tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n","            )\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n","\n","\n","            five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n","            five_gates=tf.nn.softmax(five_gates, dim=1)\n","            f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n","\n","            c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n","            \n","            h_t = o_t * tf.nn.tanh(c_t)\n","\n","            #update states\n","            initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n","            initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","            dummynode_hidden_states=dummy_h_t\n","            dummynode_cell_states=dummy_c_t\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,self.dropout)\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, self.dropout)\n","\n","        return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n","\n","\n","\n","    def __init__(self, config, session):\n","        #inputs: features, mask, keep_prob, labels\n","        self.input_data = tf.placeholder(tf.int32, [None, None], name=\"inputs\")\n","        self.labels=tf.placeholder(tf.int64, [None,], name=\"labels\")\n","        self.mask=tf.placeholder(tf.int32, [None,], name=\"mask\")\n","        self.dropout=self.keep_prob=keep_prob=tf.placeholder(tf.float32, name=\"keep_prob\")\n","        self.config=config\n","        shape=tf.shape(self.input_data)\n","        #if sys.argv[4]=='lstm':\n","        #    self.dummy_input = tf.placeholder(tf.float32, [None, None], name=\"dummy\")\n","        #embedding\n","        self.embedding=embedding = tf.Variable(tf.random_normal([config.vocab_size, config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"embedding\", trainable=config.embedding_trainable)\n","        #apply embedding\n","        initial_hidden_states=tf.nn.embedding_lookup(embedding, self.input_data)\n","        initial_cell_states=tf.identity(initial_hidden_states)\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,keep_prob)\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, keep_prob)\n","\n","        #create layers \n","        if argument4=='slstm':\n","            new_hidden_states,new_cell_state, dummynode_hidden_states=self.slstm_cell(\"word_slstm\", config.hidden_size,self.mask, initial_hidden_states, initial_cell_states, config.layer)\n","            \n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            #representation=dummynode_hidden_states\n","            representation=tf.reduce_mean(tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n","            \n","            softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","\n","        elif argument4=='lstm':\n","            initial_hidden_states=lstm_layer(initial_hidden_states,config, self.keep_prob, self.mask)\n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            representation=tf.reduce_sum(initial_hidden_states,axis=1)\n","            config.hidden_size_sum=2*config.hidden_size\n","        elif argument4=='cnn':\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, 700, config.hidden_size])            \n","            initial_hidden_states = tf.expand_dims(initial_hidden_states, -1)\n","            pooled_outputs = []\n","            for i, filter_size in enumerate([3]):\n","                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                    # Convolution Layer\n","                    filter_shape = [filter_size, config.hidden_size, 1, config.hidden_size]\n","                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","                    b = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b\")\n","\n","                    W2 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W2\")\n","                    b2 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b2\")\n","\n","                    W3 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W3\")\n","                    b3 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b3\")\n","\n","                    W4 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W4\")\n","                    b4 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b4\")\n","\n","                    conv = tf.nn.conv2d(\n","                        initial_hidden_states,\n","                        W,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv\")\n","                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                    print(h.get_shape())\n","                    h=tf.transpose(h, [0,1,3,2])\n","                    # Apply nonlinearity\n","\n","\n","                    conv2 = tf.nn.conv2d(\n","                        h,\n","                        W2,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv2\")\n","                    h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n","                    print(h2.get_shape())\n","                    h2=tf.transpose(h2, [0,1,3,2])\n","\n","                    conv3 = tf.nn.conv2d(\n","                        h2,\n","                        W3,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv3\")  \n","                    h3 = tf.nn.relu(tf.nn.bias_add(conv3, b3), name=\"relu3\")\n","                    print(h3.get_shape())\n","\n","                    # Max-pooling over the outputs\n","                    pooled = tf.nn.max_pool(\n","                        h3,\n","                        ksize=[1, 700 - 3*filter_size + 3, 1, 1],\n","                        strides=[1, 1, 1, 1],\n","                        padding='VALID',\n","                        name=\"pool\")\n","                    pooled_outputs.append(pooled)\n","            # Combine all the pooled features\n","            num_filters_total = 1 * config.hidden_size\n","            self.h_pool = tf.concat(pooled_outputs, axis=3)\n","            representation = tf.reshape(self.h_pool, [-1, num_filters_total])\n","\n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","\n","            softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","        else:\n","            print(\"Invalid model\")\n","            exit(1)\n","        \n","        self.logits=logits = tf.matmul(representation, softmax_w) + softmax_b\n","        self.to_print=tf.nn.softmax(logits)\n","        #operators for prediction\n","        self.prediction=prediction=tf.argmax(logits,1)\n","        correct_prediction = tf.equal(prediction, self.labels)\n","        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n","        \n","        #cross entropy loss\n","        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits)\n","        self.cost=cost=tf.reduce_mean(loss)+ config.l2_beta*tf.nn.l2_loss(embedding)\n","\n","        #designate training variables\n","        tvars=tf.trainable_variables()\n","        self.lr = tf.Variable(0.0, trainable=False)\n","        grads=tf.gradients(cost, tvars)\n","        grads, _ = tf.clip_by_global_norm(grads,config.max_grad_norm)\n","        self.grads=grads\n","        optimizer = tf.train.AdamOptimizer(config.learning_rate)        \n","        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","    #assign value to learning rate\n","    def assign_lr(self, session, lr_value):\n","        session.run(tf.assign(self.lr, lr_value))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6w_fbBM2Qp1O","colab_type":"text"},"source":["## get_minibatches_idx()"]},{"cell_type":"code","metadata":{"id":"YzblKztaO-OK","colab_type":"code","colab":{}},"source":["def get_minibatches_idx(n, batch_size, shuffle=True):\n","    idx_list = np.arange(n, dtype=\"int32\")\n","\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","\n","    minibatches = []\n","    minibatch_start = 0\n","    for i in range(n // batch_size):\n","        minibatches.append(idx_list[minibatch_start:\n","                                    minibatch_start + batch_size])\n","        minibatch_start += batch_size\n","    if (minibatch_start != n):\n","        # Make a minibatch out of what is left\n","        minibatches.append(idx_list[minibatch_start:])\n","    return minibatches\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxLb_HGZQwM_","colab_type":"text"},"source":["## run_epoch"]},{"cell_type":"code","metadata":{"id":"uhuVbz60PN2n","colab_type":"code","colab":{}},"source":["\n","def run_epoch(session, config, model, data, eval_op, keep_prob, is_training):\n","    n_samples = len(data[0])\n","    print(\"Running %d samples:\"%(n_samples))  \n","    minibatches = get_minibatches_idx(n_samples, config.batch_size, shuffle=False)\n","\n","    predictions = []\n","    correct = 0.\n","    total = 0\n","    total_cost=0\n","    prog = Progbar(target=len(minibatches))\n","    #dummynode_hidden_states_collector=np.array([[0]*config.hidden_size])\n","\n","    to_print_total=np.array([[0]*2])\n","    for i, inds in enumerate(minibatches):\n","        x = data[0][inds]\n","        if argument4=='cnn':\n","            x=pad_sequences(x, maxlen=700, dtype='int32',padding='post', truncating='post', value=0.)\n","        else:\n","            x=pad_sequences(x, maxlen=None, dtype='int32',padding='post', truncating='post', value=0.)\n","        y = data[1][inds]\n","        mask = data[2][inds]\n","\n","\n","\n","        count, _, cost, to_print,prediction= \\\n","        session.run([model.accuracy, eval_op,model.cost, model.to_print,model.prediction],\\\n","            {model.input_data: x, model.labels: y, model.mask:mask, model.keep_prob:keep_prob}) \n","        \n","\n","        if not is_training:\n","            to_print_total=np.concatenate((to_print_total, to_print),axis=0)\n","        correct += count \n","        total += len(inds)\n","        total_cost+=cost\n","        predictions.extend(prediction.tolist())\n","        prog.update(i + 1, [(\"train loss\", cost)])\n","    #if not is_training:\n","    #    print(to_print_total[:, 0].tolist())\n","    #    print(data[1].tolist())\n","    #    print(data[2].tolist())\n","\n","    actual = data[1]\n","\n","    TN, FP, FN, TP = confusion_matrix(actual, predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","    print(\"Total loss:\")\n","    print(total_cost)\n","\n","    accuracy = correct/total\n","\n","    return accuracy,precision,recall,f1, actual, predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnwxV-GHQ2EP","colab_type":"text"},"source":["## train_test_model"]},{"cell_type":"code","metadata":{"id":"kWuJ7_ZwPQlg","colab_type":"code","colab":{}},"source":["def train_test_model(config, i, session, model, train_dataset,test_dataset):\n","  \n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","    #training            \n","    print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\n","    start_time = time.time()\n","    train_acc, train_precision, train_recall, train_f1,actual,predictions = run_epoch(session, config, model, train_dataset, model.train_op, config.keep_prob, True)\n","    print(\"Training Accuracy = %.4f, time = %.3f seconds\\n\"%(train_acc, time.time()-start_time))\n","\n","    #testing\n","    start_time = time.time()\n","    test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    \n","\n","    return test_actual,test_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5GQEegkQ5DA","colab_type":"text"},"source":["## start_epoches"]},{"cell_type":"code","metadata":{"id":"pYlt-2iEPUpE","colab_type":"code","colab":{}},"source":["def start_epoches(config, session,classifier, train_dataset, test_dataset):\n","    all_actual = []\n","    all_predictions = []\n","\n","    for i in range(config.max_max_epoch):\n","      test_actual,test_predictions = train_test_model(config, i, session, classifier, train_dataset, test_dataset)\n","      \n","      all_actual.extend(test_actual)\n","      all_predictions.extend(test_predictions)\n","\n","    return all_actual, all_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3kBeQDxRgKl","colab_type":"text"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"SeOS2yohKwEy","colab_type":"text"},"source":["## assign word2vec"]},{"cell_type":"code","metadata":{"id":"dFvOyWeoPYAn","colab_type":"code","colab":{}},"source":["def word_to_vec(matrix, session,config, *args):\n","    \n","    print(\"word2vec shape: \", matrix.shape)\n","    \n","    for model in args:\n","        session.run(tf.assign(model.embedding, matrix))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZa7J54XRvwG","colab_type":"text"},"source":["## configs"]},{"cell_type":"code","metadata":{"id":"ifEZIahRRjEl","colab_type":"code","outputId":"e1a4cf82-2dc7-4fdc-e6e2-7f018ea94b4b","executionInfo":{"status":"ok","timestamp":1584545258717,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["argument1 = \"7\"\n","argument2 = \"2\"\n","argument3 = \"sinhala_news\"\n","argument4 = \"slstm\"\n","\n","config = Config()\n","config.layer=int(argument1)\n","config.step=int(argument2)\n","config.vocab_size=(18413) # number of words in fastText model\n","print(\"dataset: \"+argument3)\n","print(\"iteration: \"+str(config.layer))\n","print(\"step: \"+str(config.step))\n","print(\"model: \"+str(argument4))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["dataset: sinhala_news\n","iteration: 7\n","step: 2\n","model: slstm\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ddbrDIW8SsV4","colab_type":"text"},"source":["## Open Vectors"]},{"cell_type":"code","metadata":{"id":"K0aqSa1BSvYt","colab_type":"code","outputId":"5baaaf32-fcf9-4f90-d436-f64149df21fe","executionInfo":{"status":"ok","timestamp":1584545308670,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["f = open(vector_path, 'rb')\n","matrix= np.array(pickle.load(f))\n","config.vocab_size=matrix.shape[0]\n","print(config.vocab_size)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["18415\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x3yCllIB6Im-","colab_type":"text"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"id":"seazfYMrhgh5","colab_type":"code","colab":{}},"source":["data_x, data_y = load_data_for_crossVal(path=path,n_words=config.vocab_size)\n","config.num_label= 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7fk98v_6LUT","colab_type":"code","outputId":"bb027aa2-3dc4-4764-9b2c-0f1b2dc5535c","executionInfo":{"status":"ok","timestamp":1584545317129,"user_tz":-330,"elapsed":1378,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","print(\"number label: \"+str(config.num_label))\n","all_dataset = prepare_data(data_x,  data_y)\n","# valid_dataset = prepare_data(valid_dataset[0], valid_dataset[1])\n","# test_dataset = prepare_data(test_dataset[0], test_dataset[1])\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["number label: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYnmpOWR25Lp","colab_type":"code","outputId":"887c2e82-71ab-4bbb-ab01-c472b4a74b08","executionInfo":{"status":"ok","timestamp":1584545321416,"user_tz":-330,"elapsed":1448,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["print(all_dataset)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[array([list([133, 105, 445, 19, 207, 2, 6, 4429, 12, 4430, 1533, 375, 77, 160, 33, 7, 10, 423, 17, 2088, 155, 10, 3205]),\n","       list([204, 26]),\n","       list([318, 180, 4431, 303, 7096, 181, 2512, 7097, 7, 4432, 682, 7098, 723, 48, 7099, 7100, 2513, 4433, 1534, 887, 772, 7101, 7102, 1337, 7103, 1338, 7104, 7105, 34, 150, 127]),\n","       ..., list([298, 6382, 3161]), list([313, 6469, 52, 8, 243]),\n","       list([1222, 60, 147, 9, 14, 281, 3, 27, 10, 2, 6, 1132, 6689, 40])],\n","      dtype=object), array([1, 0, 1, ..., 1, 1, 1], dtype=int32), array([23,  2, 31, ...,  3,  5, 14], dtype=int32)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vuF3jIbg4Q4b","colab_type":"text"},"source":["# Train NN"]},{"cell_type":"code","metadata":{"id":"sZn_oDbv7sWV","colab_type":"code","colab":{}},"source":["def train_nn(train_dataset, test_dataset,i) :\n","  all_actual = []\n","  all_predictions = []\n","\n","  with tf.Graph().as_default(), tf.Session() as session:\n","      initializer = tf.random_normal_initializer(0, 0.05)\n","\n","      classifier= Classifer(config=config, session=session)\n","      saver = tf.train.Saver()\n","\n","      # total=0\n","      # #print trainable variables\n","      # for v in tf.trainable_variables():\n","      #     print(v.name)\n","      #     shape=v.get_shape()\n","      #     try:\n","      #         size=shape[0].value*shape[1].value\n","      #     except:\n","      #         size=shape[0].value\n","      #     total+=size\n","      # print(total)\n","\n","      #initialize\n","      init = tf.global_variables_initializer()\n","\n","      session.run(init)\n","      #train test model\n","\n","      # print (\"model_test\",matrix)\n","\n","      word_to_vec(matrix, session,config, classifier)\n","      all_actual, all_predictions = start_epoches(config, session,classifier, train_dataset,test_dataset)\n","\n","      # if(i%10 ==0 ):\n","      #   save_path = saver.save(session, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/cross_validated/1/\")\n","      #   print(\"Model saved in path: %s\" % save_path)\n","\n","  return all_actual, all_predictions \n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljEdTr622OUT","colab_type":"code","outputId":"b0f67130-7647-40af-afe4-049a29944f48","executionInfo":{"status":"ok","timestamp":1583757580997,"user_tz":-330,"elapsed":7152320,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["kf = KFold(n_splits=10)\n","kf.get_n_splits(all_dataset[0])\n","i = 1\n","\n","final_actual = []\n","final_predictions = []\n","\n","\n","\n","for train_index, test_index in kf.split(all_dataset[0]):\n","  print(\"Cross Validation step : \",i)\n","  \n","  train_data_comments, test_data_comments = all_dataset[0][train_index], all_dataset[0][test_index]\n","  train_data_labels, test_data_labels = all_dataset[1][train_index], all_dataset[1][test_index]\n","  train_lengths, test_lengths = all_dataset[2][train_index], all_dataset[2][test_index]\n"," \n","  train_dataset = [train_data_comments,train_data_labels,train_lengths]\n","  test_dataset = [test_data_comments,test_data_labels,test_lengths]\n","\n","  tf.reset_default_graph()\n","\n","  actual , prediction = train_nn(train_dataset, test_dataset,i)\n","\n","  final_actual.extend(actual)\n","  final_predictions.extend(prediction)\n","\n","  i += 1\n","\n","TN, FP, FN, TP = confusion_matrix(final_actual, final_predictions).ravel()\n","\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","f1 = 2 * precision * recall / (precision + recall)\n","accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","print(\"final Accuracy : \",accuracy)\n","print(\"final precision : \",precision)\n","print(\"final recall : \",recall)\n","print(\"final f1 : \",f1)\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cross Validation step :  1\n","WARNING:tensorflow:From <ipython-input-8-7fa0982e3379>:256: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use the `axis` argument instead\n","WARNING:tensorflow:From <ipython-input-8-7fa0982e3379>:150: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n","Instructions for updating:\n","dim is deprecated, use axis instead\n","WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","449.9853431424126\n","Training Accuracy = 0.7953, time = 31.014 seconds\n","\n","Running 501 samples:\n","Total loss:\n","46.807815324515104\n","Test Accuracy = 0.7645, Test Precision = 0.6812, Test Recall = 0.9671, Test F1 = 0.7993\n","\n","Time = 1.464 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","244.21232940065238\n","Training Accuracy = 0.9044, time = 27.144 seconds\n","\n","Running 501 samples:\n","Total loss:\n","39.02610182855278\n","Test Accuracy = 0.8423, Test Precision = 0.7887, Test Recall = 0.9218, Test F1 = 0.8501\n","\n","Time = 1.498 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","129.23266246110748\n","Training Accuracy = 0.9530, time = 26.830 seconds\n","\n","Running 501 samples:\n","Total loss:\n","66.62233360734353\n","Test Accuracy = 0.8583, Test Precision = 0.8185, Test Recall = 0.9095, Test F1 = 0.8616\n","\n","Time = 1.503 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","84.2122204417152\n","Training Accuracy = 0.9749, time = 26.418 seconds\n","\n","Running 501 samples:\n","Total loss:\n","80.21783550709188\n","Test Accuracy = 0.8822, Test Precision = 0.8898, Test Recall = 0.8642, Test F1 = 0.8768\n","\n","Time = 1.489 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","53.98343003877584\n","Training Accuracy = 0.9829, time = 26.412 seconds\n","\n","Running 501 samples:\n","Total loss:\n","104.25243184956112\n","Test Accuracy = 0.8882, Test Precision = 0.8848, Test Recall = 0.8848, Test F1 = 0.8848\n","\n","Time = 1.505 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","41.58470892544431\n","Training Accuracy = 0.9905, time = 26.666 seconds\n","\n","Running 501 samples:\n","Total loss:\n","129.80907747766355\n","Test Accuracy = 0.8862, Test Precision = 0.8605, Test Recall = 0.9136, Test F1 = 0.8862\n","\n","Time = 1.515 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","32.293699406516325\n","Training Accuracy = 0.9925, time = 26.220 seconds\n","\n","Running 501 samples:\n","Total loss:\n","143.04435284194273\n","Test Accuracy = 0.8723, Test Precision = 0.8303, Test Recall = 0.9259, Test F1 = 0.8755\n","\n","Time = 1.491 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","39.84376503795594\n","Training Accuracy = 0.9922, time = 26.845 seconds\n","\n","Running 501 samples:\n","Total loss:\n","122.21508503085043\n","Test Accuracy = 0.8882, Test Precision = 0.8979, Test Recall = 0.8683, Test F1 = 0.8828\n","\n","Time = 1.497 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","35.298216338528604\n","Training Accuracy = 0.9931, time = 26.638 seconds\n","\n","Running 501 samples:\n","Total loss:\n","151.35904386269584\n","Test Accuracy = 0.8762, Test Precision = 0.8755, Test Recall = 0.8683, Test F1 = 0.8719\n","\n","Time = 1.517 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","14.019712856396922\n","Training Accuracy = 0.9958, time = 26.209 seconds\n","\n","Running 501 samples:\n","Total loss:\n","172.07227913807117\n","Test Accuracy = 0.8643, Test Precision = 0.8431, Test Recall = 0.8848, Test F1 = 0.8635\n","\n","Time = 1.518 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","43.02327965533125\n","Training Accuracy = 0.9920, time = 26.395 seconds\n","\n","Running 501 samples:\n","Total loss:\n","95.26991255827707\n","Test Accuracy = 0.8782, Test Precision = 0.8527, Test Recall = 0.9053, Test F1 = 0.8782\n","\n","Time = 1.507 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","13.1347311732093\n","Training Accuracy = 0.9965, time = 26.469 seconds\n","\n","Running 501 samples:\n","Total loss:\n","155.50694520281908\n","Test Accuracy = 0.8842, Test Precision = 0.8599, Test Recall = 0.9095, Test F1 = 0.8840\n","\n","Time = 1.516 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","25.176831031408966\n","Training Accuracy = 0.9962, time = 26.443 seconds\n","\n","Running 501 samples:\n","Total loss:\n","122.09489266637024\n","Test Accuracy = 0.8862, Test Precision = 0.8605, Test Recall = 0.9136, Test F1 = 0.8862\n","\n","Time = 1.528 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","21.55799548895826\n","Training Accuracy = 0.9953, time = 26.376 seconds\n","\n","Running 501 samples:\n","Total loss:\n","118.72379476531063\n","Test Accuracy = 0.8882, Test Precision = 0.8667, Test Recall = 0.9095, Test F1 = 0.8876\n","\n","Time = 1.584 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","10.591365493009036\n","Training Accuracy = 0.9971, time = 26.590 seconds\n","\n","Running 501 samples:\n","Total loss:\n","163.3260172995452\n","Test Accuracy = 0.8862, Test Precision = 0.8633, Test Recall = 0.9095, Test F1 = 0.8858\n","\n","Time = 1.523 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","5.391963067749128\n","Training Accuracy = 0.9987, time = 26.362 seconds\n","\n","Running 501 samples:\n","Total loss:\n","196.78083250342027\n","Test Accuracy = 0.8962, Test Precision = 0.8930, Test Recall = 0.8930, Test F1 = 0.8930\n","\n","Time = 1.506 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","7.824967556603536\n","Training Accuracy = 0.9987, time = 26.209 seconds\n","\n","Running 501 samples:\n","Total loss:\n","237.1831144420565\n","Test Accuracy = 0.8842, Test Precision = 0.8571, Test Recall = 0.9136, Test F1 = 0.8845\n","\n","Time = 1.515 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","4.996145527635726\n","Training Accuracy = 0.9989, time = 26.224 seconds\n","\n","Running 501 samples:\n","Total loss:\n","237.19469522052447\n","Test Accuracy = 0.8902, Test Precision = 0.8790, Test Recall = 0.8971, Test F1 = 0.8880\n","\n","Time = 1.539 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","6.572645062080152\n","Training Accuracy = 0.9980, time = 26.624 seconds\n","\n","Running 501 samples:\n","Total loss:\n","276.77032381232937\n","Test Accuracy = 0.8643, Test Precision = 0.8277, Test Recall = 0.9095, Test F1 = 0.8667\n","\n","Time = 1.557 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","13.425915541285097\n","Training Accuracy = 0.9978, time = 26.289 seconds\n","\n","Running 501 samples:\n","Total loss:\n","237.8774337492701\n","Test Accuracy = 0.8882, Test Precision = 0.8848, Test Recall = 0.8848, Test F1 = 0.8848\n","\n","Time = 1.528 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","16.2359120620877\n","Training Accuracy = 0.9982, time = 26.189 seconds\n","\n","Running 501 samples:\n","Total loss:\n","177.16439972644778\n","Test Accuracy = 0.8822, Test Precision = 0.8566, Test Recall = 0.9095, Test F1 = 0.8822\n","\n","Time = 1.514 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","4.392081247313001\n","Training Accuracy = 0.9989, time = 26.219 seconds\n","\n","Running 501 samples:\n","Total loss:\n","251.68764892597125\n","Test Accuracy = 0.8723, Test Precision = 0.8482, Test Recall = 0.8971, Test F1 = 0.8720\n","\n","Time = 1.547 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","9.311143805899569\n","Training Accuracy = 0.9982, time = 26.124 seconds\n","\n","Running 501 samples:\n","Total loss:\n","245.98956229438556\n","Test Accuracy = 0.8922, Test Precision = 0.9021, Test Recall = 0.8724, Test F1 = 0.8870\n","\n","Time = 1.552 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","6.462769830439626\n","Training Accuracy = 0.9989, time = 26.491 seconds\n","\n","Running 501 samples:\n","Total loss:\n","263.1856468016876\n","Test Accuracy = 0.8663, Test Precision = 0.8143, Test Recall = 0.9383, Test F1 = 0.8719\n","\n","Time = 1.546 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","13.753694766057967\n","Training Accuracy = 0.9984, time = 26.289 seconds\n","\n","Running 501 samples:\n","Total loss:\n","188.40485837126414\n","Test Accuracy = 0.8882, Test Precision = 0.8638, Test Recall = 0.9136, Test F1 = 0.8880\n","\n","Time = 1.539 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","8.971615382414392\n","Training Accuracy = 0.9989, time = 26.191 seconds\n","\n","Running 501 samples:\n","Total loss:\n","185.14656833222855\n","Test Accuracy = 0.8902, Test Precision = 0.8615, Test Recall = 0.9218, Test F1 = 0.8907\n","\n","Time = 1.557 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","4.073319303176852\n","Training Accuracy = 0.9987, time = 26.658 seconds\n","\n","Running 501 samples:\n","Total loss:\n","241.01533866576926\n","Test Accuracy = 0.8802, Test Precision = 0.8453, Test Recall = 0.9218, Test F1 = 0.8819\n","\n","Time = 1.555 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","7.24287726858833\n","Training Accuracy = 0.9991, time = 26.107 seconds\n","\n","Running 501 samples:\n","Total loss:\n","186.94594879246122\n","Test Accuracy = 0.8862, Test Precision = 0.8690, Test Recall = 0.9012, Test F1 = 0.8848\n","\n","Time = 1.548 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","1.6587555329747197\n","Training Accuracy = 0.9993, time = 26.155 seconds\n","\n","Running 501 samples:\n","Total loss:\n","248.7449631286693\n","Test Accuracy = 0.8762, Test Precision = 0.8494, Test Recall = 0.9053, Test F1 = 0.8765\n","\n","Time = 1.546 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","10.247353136060418\n","Training Accuracy = 0.9989, time = 26.176 seconds\n","\n","Running 501 samples:\n","Total loss:\n","219.59474035569968\n","Test Accuracy = 0.8962, Test Precision = 0.8775, Test Recall = 0.9136, Test F1 = 0.8952\n","\n","Time = 1.519 seconds\n","\n","Cross Validation step :  2\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","459.2330985162407\n","Training Accuracy = 0.7909, time = 28.925 seconds\n","\n","Running 501 samples:\n","Total loss:\n","41.9323930463288\n","Test Accuracy = 0.7984, Test Precision = 0.7337, Test Recall = 0.9575, Test F1 = 0.8308\n","\n","Time = 1.506 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","241.39203937527475\n","Training Accuracy = 0.9057, time = 26.033 seconds\n","\n","Running 501 samples:\n","Total loss:\n","52.884558198762534\n","Test Accuracy = 0.8603, Test Precision = 0.8119, Test Recall = 0.9498, Test F1 = 0.8754\n","\n","Time = 1.519 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","130.15833835150733\n","Training Accuracy = 0.9530, time = 25.709 seconds\n","\n","Running 501 samples:\n","Total loss:\n","50.38563983829118\n","Test Accuracy = 0.8743, Test Precision = 0.8712, Test Recall = 0.8880, Test F1 = 0.8795\n","\n","Time = 1.519 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","83.11942232131031\n","Training Accuracy = 0.9749, time = 25.907 seconds\n","\n","Running 501 samples:\n","Total loss:\n","76.60040973546148\n","Test Accuracy = 0.8723, Test Precision = 0.9079, Test Recall = 0.8378, Test F1 = 0.8715\n","\n","Time = 1.544 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","59.45401915867231\n","Training Accuracy = 0.9829, time = 25.900 seconds\n","\n","Running 501 samples:\n","Total loss:\n","87.04214369309923\n","Test Accuracy = 0.8663, Test Precision = 0.8556, Test Recall = 0.8919, Test F1 = 0.8733\n","\n","Time = 1.511 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","45.78038384772044\n","Training Accuracy = 0.9885, time = 25.557 seconds\n","\n","Running 501 samples:\n","Total loss:\n","125.37634100003677\n","Test Accuracy = 0.8663, Test Precision = 0.9174, Test Recall = 0.8147, Test F1 = 0.8630\n","\n","Time = 1.537 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","37.29004899668248\n","Training Accuracy = 0.9918, time = 25.778 seconds\n","\n","Running 501 samples:\n","Total loss:\n","145.84560614738547\n","Test Accuracy = 0.8683, Test Precision = 0.8939, Test Recall = 0.8456, Test F1 = 0.8690\n","\n","Time = 1.524 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","31.479405613361426\n","Training Accuracy = 0.9953, time = 25.890 seconds\n","\n","Running 501 samples:\n","Total loss:\n","127.74046391258219\n","Test Accuracy = 0.8603, Test Precision = 0.8487, Test Recall = 0.8880, Test F1 = 0.8679\n","\n","Time = 1.545 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","20.78407686772299\n","Training Accuracy = 0.9951, time = 25.663 seconds\n","\n","Running 501 samples:\n","Total loss:\n","153.76230168396685\n","Test Accuracy = 0.8623, Test Precision = 0.8493, Test Recall = 0.8919, Test F1 = 0.8701\n","\n","Time = 1.531 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","21.10061730934788\n","Training Accuracy = 0.9960, time = 25.828 seconds\n","\n","Running 501 samples:\n","Total loss:\n","184.7265226493795\n","Test Accuracy = 0.8743, Test Precision = 0.8984, Test Recall = 0.8533, Test F1 = 0.8752\n","\n","Time = 1.556 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","34.6568068666938\n","Training Accuracy = 0.9949, time = 26.008 seconds\n","\n","Running 501 samples:\n","Total loss:\n","100.81344176459562\n","Test Accuracy = 0.8583, Test Precision = 0.8701, Test Recall = 0.8533, Test F1 = 0.8616\n","\n","Time = 1.552 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","22.00214685199094\n","Training Accuracy = 0.9953, time = 25.660 seconds\n","\n","Running 501 samples:\n","Total loss:\n","137.72188834885156\n","Test Accuracy = 0.8762, Test Precision = 0.8803, Test Recall = 0.8803, Test F1 = 0.8803\n","\n","Time = 1.551 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","18.224514941164955\n","Training Accuracy = 0.9960, time = 25.720 seconds\n","\n","Running 501 samples:\n","Total loss:\n","163.38930987281233\n","Test Accuracy = 0.8623, Test Precision = 0.8711, Test Recall = 0.8610, Test F1 = 0.8660\n","\n","Time = 1.555 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","9.732775855665428\n","Training Accuracy = 0.9984, time = 25.865 seconds\n","\n","Running 501 samples:\n","Total loss:\n","182.33363033754688\n","Test Accuracy = 0.8683, Test Precision = 0.8642, Test Recall = 0.8842, Test F1 = 0.8740\n","\n","Time = 1.543 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","24.072575335552788\n","Training Accuracy = 0.9958, time = 25.564 seconds\n","\n","Running 501 samples:\n","Total loss:\n","98.83377365901161\n","Test Accuracy = 0.8782, Test Precision = 0.8779, Test Recall = 0.8880, Test F1 = 0.8829\n","\n","Time = 1.598 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","7.636468976239211\n","Training Accuracy = 0.9978, time = 25.862 seconds\n","\n","Running 501 samples:\n","Total loss:\n","169.01186233813795\n","Test Accuracy = 0.8822, Test Precision = 0.8846, Test Recall = 0.8880, Test F1 = 0.8863\n","\n","Time = 1.550 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","7.373312506108444\n","Training Accuracy = 0.9976, time = 25.575 seconds\n","\n","Running 501 samples:\n","Total loss:\n","176.45756411593862\n","Test Accuracy = 0.8822, Test Precision = 0.9202, Test Recall = 0.8456, Test F1 = 0.8813\n","\n","Time = 1.556 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","12.76825603901397\n","Training Accuracy = 0.9989, time = 25.604 seconds\n","\n","Running 501 samples:\n","Total loss:\n","148.11204523353163\n","Test Accuracy = 0.8762, Test Precision = 0.9121, Test Recall = 0.8417, Test F1 = 0.8755\n","\n","Time = 1.545 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","8.801562933060328\n","Training Accuracy = 0.9980, time = 25.841 seconds\n","\n","Running 501 samples:\n","Total loss:\n","228.08536350214675\n","Test Accuracy = 0.8703, Test Precision = 0.9042, Test Recall = 0.8378, Test F1 = 0.8697\n","\n","Time = 1.553 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","8.56590266881132\n","Training Accuracy = 0.9980, time = 25.495 seconds\n","\n","Running 501 samples:\n","Total loss:\n","187.61353952008582\n","Test Accuracy = 0.8743, Test Precision = 0.8657, Test Recall = 0.8958, Test F1 = 0.8805\n","\n","Time = 1.562 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","3.956070070250341\n","Training Accuracy = 0.9984, time = 25.814 seconds\n","\n","Running 501 samples:\n","Total loss:\n","229.38284142335846\n","Test Accuracy = 0.8762, Test Precision = 0.8774, Test Recall = 0.8842, Test F1 = 0.8808\n","\n","Time = 1.550 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","16.87213198916739\n","Training Accuracy = 0.9987, time = 26.255 seconds\n","\n","Running 501 samples:\n","Total loss:\n","152.47204661276947\n","Test Accuracy = 0.8802, Test Precision = 0.8872, Test Recall = 0.8803, Test F1 = 0.8837\n","\n","Time = 1.556 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","6.305832532461267\n","Training Accuracy = 0.9987, time = 26.762 seconds\n","\n","Running 501 samples:\n","Total loss:\n","196.37196311314185\n","Test Accuracy = 0.8723, Test Precision = 0.8980, Test Recall = 0.8494, Test F1 = 0.8730\n","\n","Time = 1.596 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","13.768473105857737\n","Training Accuracy = 0.9980, time = 25.962 seconds\n","\n","Running 501 samples:\n","Total loss:\n","158.15285722116545\n","Test Accuracy = 0.8683, Test Precision = 0.8434, Test Recall = 0.9151, Test F1 = 0.8778\n","\n","Time = 1.554 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","8.847136428020281\n","Training Accuracy = 0.9980, time = 26.382 seconds\n","\n","Running 501 samples:\n","Total loss:\n","216.38421854733156\n","Test Accuracy = 0.8703, Test Precision = 0.8731, Test Recall = 0.8764, Test F1 = 0.8748\n","\n","Time = 1.592 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","20.368719154871812\n","Training Accuracy = 0.9971, time = 26.059 seconds\n","\n","Running 501 samples:\n","Total loss:\n","137.55063597326446\n","Test Accuracy = 0.8683, Test Precision = 0.8971, Test Recall = 0.8417, Test F1 = 0.8685\n","\n","Time = 1.565 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","0.428069444600391\n","Training Accuracy = 1.0000, time = 26.329 seconds\n","\n","Running 501 samples:\n","Total loss:\n","178.26841768100346\n","Test Accuracy = 0.8683, Test Precision = 0.8876, Test Recall = 0.8533, Test F1 = 0.8701\n","\n","Time = 1.590 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","9.81399599478916\n","Training Accuracy = 0.9984, time = 26.283 seconds\n","\n","Running 501 samples:\n","Total loss:\n","153.1695856754984\n","Test Accuracy = 0.8663, Test Precision = 0.9103, Test Recall = 0.8224, Test F1 = 0.8641\n","\n","Time = 1.631 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","5.2114863891786705\n","Training Accuracy = 0.9991, time = 26.584 seconds\n","\n","Running 501 samples:\n","Total loss:\n","195.24482889260796\n","Test Accuracy = 0.8663, Test Precision = 0.8750, Test Recall = 0.8649, Test F1 = 0.8699\n","\n","Time = 1.577 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","5.592497447394457\n","Training Accuracy = 0.9991, time = 25.927 seconds\n","\n","Running 501 samples:\n","Total loss:\n","185.72026926180692\n","Test Accuracy = 0.8603, Test Precision = 0.8593, Test Recall = 0.8726, Test F1 = 0.8659\n","\n","Time = 1.622 seconds\n","\n","Cross Validation step :  3\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","449.7335510067642\n","Training Accuracy = 0.7991, time = 28.996 seconds\n","\n","Running 501 samples:\n","Total loss:\n","43.57457278855145\n","Test Accuracy = 0.7984, Test Precision = 0.7231, Test Recall = 0.9553, Test F1 = 0.8231\n","\n","Time = 1.516 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","236.27154475994757\n","Training Accuracy = 0.9004, time = 25.884 seconds\n","\n","Running 501 samples:\n","Total loss:\n","52.84531713742763\n","Test Accuracy = 0.8164, Test Precision = 0.7500, Test Recall = 0.9390, Test F1 = 0.8339\n","\n","Time = 1.538 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","129.63494593614203\n","Training Accuracy = 0.9572, time = 26.462 seconds\n","\n","Running 501 samples:\n","Total loss:\n","61.926515150115506\n","Test Accuracy = 0.8723, Test Precision = 0.8555, Test Recall = 0.8902, Test F1 = 0.8725\n","\n","Time = 1.557 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","78.25412145872136\n","Training Accuracy = 0.9741, time = 25.712 seconds\n","\n","Running 501 samples:\n","Total loss:\n","99.40222213766651\n","Test Accuracy = 0.8882, Test Precision = 0.9167, Test Recall = 0.8496, Test F1 = 0.8819\n","\n","Time = 1.548 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","57.571211826526394\n","Training Accuracy = 0.9836, time = 26.007 seconds\n","\n","Running 501 samples:\n","Total loss:\n","117.83690761165174\n","Test Accuracy = 0.8842, Test Precision = 0.8917, Test Recall = 0.8699, Test F1 = 0.8807\n","\n","Time = 1.543 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","39.841635242886944\n","Training Accuracy = 0.9914, time = 26.051 seconds\n","\n","Running 501 samples:\n","Total loss:\n","161.55743088192733\n","Test Accuracy = 0.8723, Test Precision = 0.8527, Test Recall = 0.8943, Test F1 = 0.8730\n","\n","Time = 1.547 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","44.58298719284686\n","Training Accuracy = 0.9902, time = 25.927 seconds\n","\n","Running 501 samples:\n","Total loss:\n","128.59356945318913\n","Test Accuracy = 0.8802, Test Precision = 0.8875, Test Recall = 0.8659, Test F1 = 0.8765\n","\n","Time = 1.552 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","31.298161322671113\n","Training Accuracy = 0.9936, time = 25.931 seconds\n","\n","Running 501 samples:\n","Total loss:\n","136.18024291100733\n","Test Accuracy = 0.8683, Test Precision = 0.8543, Test Recall = 0.8821, Test F1 = 0.8680\n","\n","Time = 1.545 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","23.84833565999604\n","Training Accuracy = 0.9967, time = 26.121 seconds\n","\n","Running 501 samples:\n","Total loss:\n","184.47565988368217\n","Test Accuracy = 0.8703, Test Precision = 0.8549, Test Recall = 0.8862, Test F1 = 0.8703\n","\n","Time = 1.549 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","37.70725144255415\n","Training Accuracy = 0.9927, time = 26.509 seconds\n","\n","Running 501 samples:\n","Total loss:\n","165.15763172220036\n","Test Accuracy = 0.8643, Test Precision = 0.8346, Test Recall = 0.9024, Test F1 = 0.8672\n","\n","Time = 1.561 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","23.244352189418787\n","Training Accuracy = 0.9947, time = 25.939 seconds\n","\n","Running 501 samples:\n","Total loss:\n","164.9910272322257\n","Test Accuracy = 0.8743, Test Precision = 0.8765, Test Recall = 0.8659, Test F1 = 0.8712\n","\n","Time = 1.578 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","26.30054749348651\n","Training Accuracy = 0.9953, time = 26.049 seconds\n","\n","Running 501 samples:\n","Total loss:\n","147.23441531710466\n","Test Accuracy = 0.8683, Test Precision = 0.8462, Test Recall = 0.8943, Test F1 = 0.8696\n","\n","Time = 1.556 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","22.89017560252702\n","Training Accuracy = 0.9956, time = 25.939 seconds\n","\n","Running 501 samples:\n","Total loss:\n","126.3271664303502\n","Test Accuracy = 0.8583, Test Precision = 0.8277, Test Recall = 0.8984, Test F1 = 0.8616\n","\n","Time = 1.575 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","18.401965977756127\n","Training Accuracy = 0.9967, time = 26.578 seconds\n","\n","Running 501 samples:\n","Total loss:\n","143.79243452113485\n","Test Accuracy = 0.8822, Test Precision = 0.8667, Test Recall = 0.8984, Test F1 = 0.8822\n","\n","Time = 1.574 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","6.050627981692013\n","Training Accuracy = 0.9984, time = 26.359 seconds\n","\n","Running 501 samples:\n","Total loss:\n","234.96159780865918\n","Test Accuracy = 0.8762, Test Precision = 0.8459, Test Recall = 0.9146, Test F1 = 0.8789\n","\n","Time = 1.566 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","12.596719026854654\n","Training Accuracy = 0.9973, time = 25.819 seconds\n","\n","Running 501 samples:\n","Total loss:\n","166.06608445904885\n","Test Accuracy = 0.8942, Test Precision = 0.8971, Test Recall = 0.8862, Test F1 = 0.8916\n","\n","Time = 1.611 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","9.913197582057466\n","Training Accuracy = 0.9987, time = 26.127 seconds\n","\n","Running 501 samples:\n","Total loss:\n","157.51630062757488\n","Test Accuracy = 0.8643, Test Precision = 0.8346, Test Recall = 0.9024, Test F1 = 0.8672\n","\n","Time = 1.571 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","19.228380442850153\n","Training Accuracy = 0.9969, time = 26.031 seconds\n","\n","Running 501 samples:\n","Total loss:\n","145.48797473487429\n","Test Accuracy = 0.8862, Test Precision = 0.8795, Test Recall = 0.8902, Test F1 = 0.8848\n","\n","Time = 1.606 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","10.537693605248693\n","Training Accuracy = 0.9984, time = 25.837 seconds\n","\n","Running 501 samples:\n","Total loss:\n","118.16996286900792\n","Test Accuracy = 0.8743, Test Precision = 0.8427, Test Recall = 0.9146, Test F1 = 0.8772\n","\n","Time = 1.576 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","17.162871407829453\n","Training Accuracy = 0.9973, time = 26.015 seconds\n","\n","Running 501 samples:\n","Total loss:\n","148.02007111485028\n","Test Accuracy = 0.8703, Test Precision = 0.8467, Test Recall = 0.8984, Test F1 = 0.8718\n","\n","Time = 1.619 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","6.820172293439384\n","Training Accuracy = 0.9980, time = 26.194 seconds\n","\n","Running 501 samples:\n","Total loss:\n","147.0061541083964\n","Test Accuracy = 0.8842, Test Precision = 0.8852, Test Recall = 0.8780, Test F1 = 0.8816\n","\n","Time = 1.572 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","11.00689840464678\n","Training Accuracy = 0.9978, time = 25.727 seconds\n","\n","Running 501 samples:\n","Total loss:\n","144.63823072713555\n","Test Accuracy = 0.8782, Test Precision = 0.8745, Test Recall = 0.8780, Test F1 = 0.8763\n","\n","Time = 1.585 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","5.545407756861145\n","Training Accuracy = 0.9991, time = 25.986 seconds\n","\n","Running 501 samples:\n","Total loss:\n","106.53857502860468\n","Test Accuracy = 0.8862, Test Precision = 0.8795, Test Recall = 0.8902, Test F1 = 0.8848\n","\n","Time = 1.578 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","2.3444624457942815\n","Training Accuracy = 0.9991, time = 25.918 seconds\n","\n","Running 501 samples:\n","Total loss:\n","170.47621600923102\n","Test Accuracy = 0.8862, Test Precision = 0.9021, Test Recall = 0.8618, Test F1 = 0.8815\n","\n","Time = 1.571 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","8.108827551081028\n","Training Accuracy = 0.9982, time = 26.179 seconds\n","\n","Running 501 samples:\n","Total loss:\n","175.68712862702125\n","Test Accuracy = 0.8802, Test Precision = 0.8750, Test Recall = 0.8821, Test F1 = 0.8785\n","\n","Time = 1.574 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","10.59737440824625\n","Training Accuracy = 0.9987, time = 26.132 seconds\n","\n","Running 501 samples:\n","Total loss:\n","136.14427627336244\n","Test Accuracy = 0.8802, Test Precision = 0.8750, Test Recall = 0.8821, Test F1 = 0.8785\n","\n","Time = 1.581 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","7.858102769898924\n","Training Accuracy = 0.9989, time = 25.760 seconds\n","\n","Running 501 samples:\n","Total loss:\n","140.6595261502731\n","Test Accuracy = 0.8782, Test Precision = 0.8627, Test Recall = 0.8943, Test F1 = 0.8782\n","\n","Time = 1.576 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H7toABPhbeV5","colab_type":"text"},"source":["### Maximum accuracy fasttext without removing puncuations **85.66**\n","\n","Results from fast text after removing puncuations\n","final Accuracy :  0.8738522954091816\n","final precision :  0.9021142508639968\n","final recall :  0.850095785440613\n","final f1 :  0.8753328730644049"]},{"cell_type":"code","metadata":{"id":"NGpAcB9E7GN1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoYNLo9pOZBk","colab_type":"text"},"source":["# Cross Validation"]},{"cell_type":"code","metadata":{"id":"ccCXP2m9NJwf","colab_type":"code","colab":{}},"source":["def start_test_epoches(config, session,classifier, train_dataset, test_dataset):\n","    #record max\n","    #max_val_acc=-1\n","    #max_test_acc=-1\n"," \n","    all_actual = []\n","    all_predictions = []\n","    for i in range(config.max_max_epoch):\n","        actual, prediction = test_model(config, i, session, classifier, train_dataset, test_dataset)\n","        all_actual.extend(actual)\n","        all_predictions.extend(prediction)\n","\n","\n","    TN, FP, FN, TP = confusion_matrix(all_actual, all_predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","    accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","    print(\"final Accuracy : \",accuracy)\n","    print(\"final precision : \",precision)\n","    print(\"final recall : \",recall)\n","    print(\"final f1 : \",f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqeU8wHGNbeR","colab_type":"code","colab":{}},"source":["def test_model(config, i, session, model, train_dataset,test_dataset):\n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","\n","    #testing\n","    start_time = time.time()\n","    test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    #return valid_acc, test_acc\n","\n","    return test_actual,test_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bvR6QttXOXAN","colab":{}},"source":["tf.reset_default_graph()\n","\n","with tf.Graph().as_default(), tf.Session() as session:\n","  #initializer = tf.random_normal_initializer(0, 0.05)\n","\n","  classifier= Classifer(config=config, session=session)\n","  saver = tf.train.Saver()\n","  \n","  init = tf.global_variables_initializer()\n","  session.run(init)\n","\n","  saver = tf.train.Saver()\n","\n","  tf.train.Saver().restore(session,tf.train.latest_checkpoint(\"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/100epochs/\") )\n","  print(\"Model restored.\")\n","\n","  word_to_vec(matrix, session,config, classifier)\n","  start_test_epoches(config, session,classifier, train_dataset, test_dataset)\n"],"execution_count":0,"outputs":[]}]}