{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment_Tagger_slstm_with_fasttext_with_save_crossVal.ipynb","provenance":[],"collapsed_sections":["W1kYrhsN6xEs","B4wf4YZ9RUW-","0n2fJGeq63I7","6w_fbBM2Qp1O","XeFsOVyYQ7y6","BZa7J54XRvwG","ddbrDIW8SsV4"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"t48aRicQOinf","colab_type":"code","outputId":"785a17d7-50a5-4d90-d580-09967c8906a6","executionInfo":{"status":"ok","timestamp":1584637968940,"user_tz":-330,"elapsed":1431,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bG9mz-zbS8Rt","colab_type":"code","colab":{}},"source":["path='/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/SLSTM/parsed_data/from_fasttext/data_set_common_docid_removed'\n","vector_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/SLSTM/parsed_data/from_fasttext/fasttext_vectors_common_docid_removed'\n","# run from lahiru1st@gmail.com\n","# path='/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/data_set'\n","# vector_path = '/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/fasttext_vectors'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwA4weGRP9OQ","colab_type":"text"},"source":["# Imports"]},{"cell_type":"markdown","metadata":{"id":"W1kYrhsN6xEs","colab_type":"text"},"source":["## Standard Imports"]},{"cell_type":"code","metadata":{"id":"OXwBif125t0Y","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","# import six.moves.cPickle as pickle\n","# from collections import OrderedDict\n","import sys\n","import time\n","import numpy as np\n","import tensorflow as tf\n","# import read_data\n","# from random import shuffle\n","# import random\n","import pickle\n","# import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import confusion_matrix\n","# from general_utils import Progbar\n","# import tensorflow.contrib.slim as slim\n","# from sst_config import Config\n","from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0T3y_TelRPlY","colab_type":"text"},"source":["## config"]},{"cell_type":"code","metadata":{"id":"kiZjosbeRMm-","colab_type":"code","colab":{}},"source":["class Config(object):\n","    vocab_size=15000\n","    max_grad_norm = 5\n","    init_scale = 0.05\n","    hidden_size = 300\n","    lr_decay = 0.95\n","    valid_portion=0.0\n","    batch_size=5\n","    keep_prob = 0.5\n","    #0.05\n","    learning_rate = 0.001\n","    max_epoch =2\n","    # max_max_epoch =40\n","    max_max_epoch = 30\n","    num_label=5\n","    attention_iteration=3\n","    random_initialize=False\n","    embedding_trainable=True\n","    l2_beta=0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZa7J54XRvwG","colab_type":"text"},"source":["## configs"]},{"cell_type":"code","metadata":{"id":"ifEZIahRRjEl","colab_type":"code","outputId":"ce5a320c-380c-4d51-bbc5-a1f8deb48bcc","executionInfo":{"status":"ok","timestamp":1584637970433,"user_tz":-330,"elapsed":2781,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["argument1 = \"7\"\n","argument2 = \"2\"\n","argument3 = \"sinhala_news\"\n","argument4 = \"slstm\"\n","\n","config = Config()\n","config.layer=int(argument1)\n","config.step=int(argument2)\n","config.vocab_size=(15000) # number of words in fastText model\n","config.max_max_epoch=int(50)\n","config.batch_size=int(64)\n","print(\"dataset: \"+argument3)\n","print(\"iteration: \"+str(config.layer))\n","print(\"step: \"+str(config.step))\n","print(\"model: \"+str(argument4))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["dataset: sinhala_news\n","iteration: 7\n","step: 2\n","model: slstm\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B4wf4YZ9RUW-","colab_type":"text"},"source":["## Progbar"]},{"cell_type":"code","metadata":{"id":"HQxItjCIPrgH","colab_type":"code","colab":{}},"source":["import time\n","import sys\n","import logging\n","import numpy as np\n","\n","\n","class Progbar(object):\n","    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n","\n","    Displays a progress bar.\n","    Small edit : added strict arg to update\n","    # Arguments\n","        target: Total number of steps expected.\n","        interval: Minimum visual progress update interval (in seconds).\n","    \"\"\"\n","\n","    def __init__(self, target, width=30, verbose=0):\n","        self.width = width\n","        self.target = target\n","        self.sum_values = {}\n","        self.unique_values = []\n","        self.start = time.time()\n","        self.total_width = 0\n","        self.seen_so_far = 0\n","        self.verbose = verbose\n","\n","    def update(self, current, values=[], exact=[], strict=[]):\n","        \"\"\"\n","        Updates the progress bar.\n","        # Arguments\n","            current: Index of current step.\n","            values: List of tuples (name, value_for_last_step).\n","                The progress bar will display averages for these values.\n","            exact: List of tuples (name, value_for_last_step).\n","                The progress bar will display these values directly.\n","        \"\"\"\n","\n","        for k, v in values:\n","            if k not in self.sum_values:\n","                self.sum_values[k] = [v * (current - self.seen_so_far),\n","                                      current - self.seen_so_far]\n","                self.unique_values.append(k)\n","            else:\n","                self.sum_values[k][0] += v * (current - self.seen_so_far)\n","                self.sum_values[k][1] += (current - self.seen_so_far)\n","        for k, v in exact:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = [v, 1]\n","\n","        for k, v in strict:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = v\n","\n","        self.seen_so_far = current\n","\n","        now = time.time()\n","        if self.verbose == 1:\n","            prev_total_width = self.total_width\n","            sys.stdout.write(\"\\b\" * prev_total_width)\n","            sys.stdout.write(\"\\r\")\n","\n","            numdigits = int(np.floor(np.log10(self.target))) + 1\n","            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n","            bar = barstr % (current, self.target)\n","            prog = float(current)/self.target\n","            prog_width = int(self.width*prog)\n","            if prog_width > 0:\n","                bar += ('='*(prog_width-1))\n","                if current < self.target:\n","                    bar += '>'\n","                else:\n","                    bar += '='\n","            bar += ('.'*(self.width-prog_width))\n","            bar += ']'\n","            sys.stdout.write(bar)\n","            self.total_width = len(bar)\n","\n","            if current:\n","                time_per_unit = (now - self.start) / current\n","            else:\n","                time_per_unit = 0\n","            eta = time_per_unit*(self.target - current)\n","            info = ''\n","            if current < self.target:\n","                info += ' - ETA: %ds' % eta\n","            else:\n","                info += ' - %ds' % (now - self.start)\n","            for k in self.unique_values:\n","                if type(self.sum_values[k]) is list:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                else:\n","                    info += ' - %s: %s' % (k, self.sum_values[k])\n","\n","            self.total_width += len(info)\n","            if prev_total_width > self.total_width:\n","                info += ((prev_total_width-self.total_width) * \" \")\n","\n","            sys.stdout.write(info)\n","            sys.stdout.flush()\n","\n","            if current >= self.target:\n","                sys.stdout.write(\"\\n\")\n","\n","        if self.verbose == 2:\n","            if current >= self.target:\n","                info = '%ds' % (now - self.start)\n","                for k in self.unique_values:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                sys.stdout.write(info + \"\\n\")\n","\n","    def add(self, n, values=[]):\n","        self.update(self.seen_so_far+n, values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0n2fJGeq63I7","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"code","metadata":{"id":"_pSHY_uE66j1","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","from six.moves import xrange\n","import six.moves.cPickle as pickle\n","import gzip\n","import os\n","import numpy\n","\n","def generate_matrix(seqs, maxlen, lengths):\n","    n_samples = len(seqs)\n","    x= numpy.zeros((n_samples, maxlen)).astype('int64')\n","\n","    for idx, s in enumerate(seqs):\n","        if lengths[idx]>= maxlen:\n","            s=s[:maxlen]\n","        x[idx, :lengths[idx]] = s\n","    return x\n","\n","def prepare_data(seqs, labels):\n","    lengths = [len(s) for s in seqs]\n","    labels = numpy.array(labels).astype('int32')\n","    return [numpy.array(seqs), labels, numpy.array(lengths).astype('int32')]\n","\n","def remove_unk(x, n_words):\n","    return [[1 if w >= n_words else w for w in sen] for sen in x] \n","\n","def load_data(path, n_words):\n","    with open(path, 'rb') as f:\n","        dataset_x, dataset_label= pickle.load(f)\n","        train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","        # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","        test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","    train_set_x = remove_unk(train_set_x, n_words)\n","    # valid_set_x = remove_unk(valid_set_x, n_words)\n","    test_set_x = remove_unk(test_set_x, n_words)\n","\n","    return [train_set_x, train_set_y],[test_set_x, test_set_y]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_1MavSboWVk","colab_type":"code","colab":{}},"source":["def load_data_for_crossVal(path, n_words):\n","  with open(path, 'rb') as f:\n","    dataset_x, dataset_label= pickle.load(f)\n","    train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","    # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","    test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","  train_set_x = remove_unk(train_set_x, n_words)\n","  # valid_set_x = remove_unk(valid_set_x, n_words)\n","  test_set_x = remove_unk(test_set_x, n_words)\n","\n","  \n","\n","  train_set_x.extend(test_set_x)\n","  dataset_label[0].extend( dataset_label[1])\n","\n","  return train_set_x,dataset_label[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEgTqwtlQanA","colab_type":"text"},"source":["# Implementation"]},{"cell_type":"markdown","metadata":{"id":"lQArWlWmQgCQ","colab_type":"text"},"source":["## LSTM Layer"]},{"cell_type":"code","metadata":{"id":"9chxIjuYONVw","colab_type":"code","colab":{}},"source":["def lstm_layer(initial_hidden_states, config, keep_prob, mask):\n","    with tf.variable_scope('forward'):\n","        fw_lstm = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        fw_lstm = tf.contrib.rnn.DropoutWrapper(fw_lstm, output_keep_prob=keep_prob)\n","\n","    with tf.variable_scope('backward'):\n","        bw_lstm = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        bw_lstm = tf.contrib.rnn.DropoutWrapper(bw_lstm, output_keep_prob=keep_prob)\n","\n","    #bidirectional rnn\n","    with tf.variable_scope('bilstm'):\n","        lstm_output=tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, inputs=initial_hidden_states, sequence_length=mask, time_major=False, dtype=tf.float32)\n","        lstm_output=tf.concat(lstm_output[0], 2)\n","\n","    return lstm_output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTmP1ZQbQlTv","colab_type":"text"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"Z3LlPobkOq1_","colab_type":"code","colab":{}},"source":["class Classifer(object):\n","\n","    def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,:-step,:]\n","        #concat padding\n","        return tf.concat([padding, displaced_hidden_states], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,step:,:]\n","        #concat padding\n","        return tf.concat([displaced_hidden_states, padding], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def sum_together(self, l):\n","        combined_state=None\n","        for tensor in l:\n","            if combined_state==None:\n","                combined_state=tensor\n","            else:\n","                combined_state=combined_state+tensor\n","        return combined_state\n","    \n","    def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n","        with tf.name_scope(name_scope_name):\n","            #Word parameters \n","            #forget gate for left \n","            with tf.name_scope(\"f1_gate\"):\n","                #current\n","                Wxf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                #left right\n","                Whf1 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                #initial state\n","                Wif1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                #dummy node\n","                Wdf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for right \n","            with tf.name_scope(\"f2_gate\"):\n","                Wxf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf2 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for inital states     \n","            with tf.name_scope(\"f3_gate\"):\n","                Wxf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf3 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for dummy states     \n","            with tf.name_scope(\"f4_gate\"):\n","                Wxf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf4 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #input gate for current state     \n","            with tf.name_scope(\"i_gate\"):\n","                Wxi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n","                Whi = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n","                Wii = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n","                Wdi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n","            #input gate for output gate\n","            with tf.name_scope(\"o_gate\"):\n","                Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                Who = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","                Wio = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n","                Wdo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n","            #bias for the gates    \n","            with tf.name_scope(\"biases\"):\n","                bi = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                bf1 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n","                bf2 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n","                bf3 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n","                bf4 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n","\n","            #dummy node gated attention parameters\n","            #input gate for dummy state\n","            with tf.name_scope(\"gated_d_gate\"):\n","                gated_Wxd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                gated_Whd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","            #output gate\n","            with tf.name_scope(\"gated_o_gate\"):\n","                gated_Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Who = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #forget gate for states of word\n","            with tf.name_scope(\"gated_f_gate\"):\n","                gated_Wxf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Whf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #biases\n","            with tf.name_scope(\"gated_biases\"):\n","                gated_bd = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                gated_bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                gated_bf = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","\n","        #filters for attention        \n","        mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n","        mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, dim=2)               \n","        #filter invalid steps\n","        sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n","        #filter embedding states\n","        initial_hidden_states=initial_hidden_states*sequence_mask\n","        initial_cell_states=initial_cell_states*sequence_mask\n","        #record shape of the batch\n","        shape=tf.shape(initial_hidden_states)\n","        \n","        #initial embedding states\n","        embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n","        embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","        #randomly initialize the states\n","        if config.random_initialize:\n","            initial_hidden_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            initial_cell_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            #filter it\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","        #inital dummy node states\n","        dummynode_hidden_states=tf.reduce_mean(initial_hidden_states, axis=1)\n","        dummynode_cell_states=tf.reduce_mean(initial_cell_states, axis=1)\n","\n","        for i in range(num_layers):\n","            #update dummy node states\n","            #average states\n","            combined_word_hidden_state=tf.reduce_mean(initial_hidden_states, axis=1)\n","            reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            #copy dummy states for computing forget gate\n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            #input gate\n","            gated_d_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n","            )\n","            #output gate\n","            gated_o_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n","            )\n","            #forget gate for hidden states\n","            gated_f_t = tf.nn.sigmoid(\n","                tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n","            )\n","\n","            #softmax on each hidden dimension \n","            reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n","            gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, dim=1)], axis=1), dim=1)\n","            #split the softmax scores\n","            new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n","            new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n","            #new dummy states\n","            dummy_c_t=tf.reduce_sum(new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n","            dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n","\n","            #update word node states\n","            #get states before\n","            initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n","            initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n","            #get states after\n","            initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_before=self.sum_together(initial_cell_states_before)\n","            initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_after=self.sum_together(initial_cell_states_after)\n","            \n","            #reshape for matmul\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","            #concat before and after hidden states\n","            concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n","\n","            #copy dummy node states \n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","\n","            f1_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n","                tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n","            )\n","\n","            f2_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n","                tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n","            )\n","\n","            f3_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n","                tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n","            )\n","\n","            f4_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n","                tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n","            )\n","            \n","            i_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n","                tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n","            )\n","            \n","            o_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n","                tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n","            )\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n","\n","\n","            five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n","            five_gates=tf.nn.softmax(five_gates, dim=1)\n","            f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n","\n","            c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n","            \n","            h_t = o_t * tf.nn.tanh(c_t)\n","\n","            #update states\n","            initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n","            initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","            dummynode_hidden_states=dummy_h_t\n","            dummynode_cell_states=dummy_c_t\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,self.dropout)\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, self.dropout)\n","\n","        return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n","\n","\n","\n","    def __init__(self, config, session):\n","        #inputs: features, mask, keep_prob, labels\n","        self.input_data = tf.placeholder(tf.int32, [None, None], name=\"inputs\")\n","        self.labels=tf.placeholder(tf.int64, [None,], name=\"labels\")\n","        self.mask=tf.placeholder(tf.int32, [None,], name=\"mask\")\n","        self.dropout=self.keep_prob=keep_prob=tf.placeholder(tf.float32, name=\"keep_prob\")\n","        self.config=config\n","        shape=tf.shape(self.input_data)\n","        #if sys.argv[4]=='lstm':\n","        #    self.dummy_input = tf.placeholder(tf.float32, [None, None], name=\"dummy\")\n","        #embedding\n","        self.embedding=embedding = tf.Variable(tf.random_normal([config.vocab_size, config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"embedding\", trainable=config.embedding_trainable)\n","        #apply embedding\n","        initial_hidden_states=tf.nn.embedding_lookup(embedding, self.input_data)\n","        initial_cell_states=tf.identity(initial_hidden_states)\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,keep_prob)\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, keep_prob)\n","\n","        #create layers \n","        if argument4=='slstm':\n","            new_hidden_states,new_cell_state, dummynode_hidden_states=self.slstm_cell(\"word_slstm\", config.hidden_size,self.mask, initial_hidden_states, initial_cell_states, config.layer)\n","            \n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            #representation=dummynode_hidden_states\n","            representation=tf.reduce_mean(tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n","            \n","            softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","\n","        elif argument4=='lstm':\n","            initial_hidden_states=lstm_layer(initial_hidden_states,config, self.keep_prob, self.mask)\n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            representation=tf.reduce_sum(initial_hidden_states,axis=1)\n","            config.hidden_size_sum=2*config.hidden_size\n","        elif argument4=='cnn':\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, 700, config.hidden_size])            \n","            initial_hidden_states = tf.expand_dims(initial_hidden_states, -1)\n","            pooled_outputs = []\n","            for i, filter_size in enumerate([3]):\n","                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                    # Convolution Layer\n","                    filter_shape = [filter_size, config.hidden_size, 1, config.hidden_size]\n","                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","                    b = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b\")\n","\n","                    W2 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W2\")\n","                    b2 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b2\")\n","\n","                    W3 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W3\")\n","                    b3 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b3\")\n","\n","                    W4 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W4\")\n","                    b4 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b4\")\n","\n","                    conv = tf.nn.conv2d(\n","                        initial_hidden_states,\n","                        W,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv\")\n","                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                    print(h.get_shape())\n","                    h=tf.transpose(h, [0,1,3,2])\n","                    # Apply nonlinearity\n","\n","\n","                    conv2 = tf.nn.conv2d(\n","                        h,\n","                        W2,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv2\")\n","                    h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n","                    print(h2.get_shape())\n","                    h2=tf.transpose(h2, [0,1,3,2])\n","\n","                    conv3 = tf.nn.conv2d(\n","                        h2,\n","                        W3,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv3\")  \n","                    h3 = tf.nn.relu(tf.nn.bias_add(conv3, b3), name=\"relu3\")\n","                    print(h3.get_shape())\n","\n","                    # Max-pooling over the outputs\n","                    pooled = tf.nn.max_pool(\n","                        h3,\n","                        ksize=[1, 700 - 3*filter_size + 3, 1, 1],\n","                        strides=[1, 1, 1, 1],\n","                        padding='VALID',\n","                        name=\"pool\")\n","                    pooled_outputs.append(pooled)\n","            # Combine all the pooled features\n","            num_filters_total = 1 * config.hidden_size\n","            self.h_pool = tf.concat(pooled_outputs, axis=3)\n","            representation = tf.reshape(self.h_pool, [-1, num_filters_total])\n","\n","            softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","\n","            softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","        else:\n","            print(\"Invalid model\")\n","            exit(1)\n","        \n","        self.logits=logits = tf.matmul(representation, softmax_w) + softmax_b\n","        self.to_print=tf.nn.softmax(logits)\n","        #operators for prediction\n","        self.prediction=prediction=tf.argmax(logits,1)\n","        correct_prediction = tf.equal(prediction, self.labels)\n","        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n","        \n","        #cross entropy loss\n","        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits)\n","        self.cost=cost=tf.reduce_mean(loss)+ config.l2_beta*tf.nn.l2_loss(embedding)\n","\n","        #designate training variables\n","        tvars=tf.trainable_variables()\n","        self.lr = tf.Variable(0.0, trainable=False)\n","        grads=tf.gradients(cost, tvars)\n","        grads, _ = tf.clip_by_global_norm(grads,config.max_grad_norm)\n","        self.grads=grads\n","        optimizer = tf.train.AdamOptimizer(config.learning_rate)        \n","        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","    #assign value to learning rate\n","    def assign_lr(self, session, lr_value):\n","        session.run(tf.assign(self.lr, lr_value))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6w_fbBM2Qp1O","colab_type":"text"},"source":["## get_minibatches_idx()"]},{"cell_type":"code","metadata":{"id":"YzblKztaO-OK","colab_type":"code","colab":{}},"source":["def get_minibatches_idx(n, batch_size, shuffle=True):\n","    idx_list = np.arange(n, dtype=\"int32\")\n","\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","\n","    minibatches = []\n","    minibatch_start = 0\n","    for i in range(n // batch_size):\n","        minibatches.append(idx_list[minibatch_start:\n","                                    minibatch_start + batch_size])\n","        minibatch_start += batch_size\n","    if (minibatch_start != n):\n","        # Make a minibatch out of what is left\n","        minibatches.append(idx_list[minibatch_start:])\n","    return minibatches\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxLb_HGZQwM_","colab_type":"text"},"source":["## run_epoch"]},{"cell_type":"code","metadata":{"id":"uhuVbz60PN2n","colab_type":"code","colab":{}},"source":["\n","def run_epoch(session, config, model, data, eval_op, keep_prob, is_training):\n","    n_samples = len(data[0])\n","    print(\"Running %d samples:\"%(n_samples))  \n","    minibatches = get_minibatches_idx(n_samples, config.batch_size, shuffle=False)\n","\n","    predictions = []\n","    correct = 0.\n","    total = 0\n","    total_cost=0\n","    prog = Progbar(target=len(minibatches))\n","    #dummynode_hidden_states_collector=np.array([[0]*config.hidden_size])\n","\n","    to_print_total=np.array([[0]*2])\n","    for i, inds in enumerate(minibatches):\n","        x = data[0][inds]\n","        if argument4=='cnn':\n","            x=pad_sequences(x, maxlen=700, dtype='int32',padding='post', truncating='post', value=0.)\n","        else:\n","            x=pad_sequences(x, maxlen=None, dtype='int32',padding='post', truncating='post', value=0.)\n","        y = data[1][inds]\n","        mask = data[2][inds]\n","\n","\n","\n","        count, _, cost, to_print,prediction= \\\n","        session.run([model.accuracy, eval_op,model.cost, model.to_print,model.prediction],\\\n","            {model.input_data: x, model.labels: y, model.mask:mask, model.keep_prob:keep_prob}) \n","        \n","\n","        if not is_training:\n","            to_print_total=np.concatenate((to_print_total, to_print),axis=0)\n","        correct += count \n","        total += len(inds)\n","        total_cost+=cost\n","        predictions.extend(prediction.tolist())\n","        prog.update(i + 1, [(\"train loss\", cost)])\n","    #if not is_training:\n","    #    print(to_print_total[:, 0].tolist())\n","    #    print(data[1].tolist())\n","    #    print(data[2].tolist())\n","\n","    actual = data[1]\n","\n","    TN, FP, FN, TP = confusion_matrix(actual, predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","    print(\"Total loss:\")\n","    print(total_cost)\n","\n","    accuracy = correct/total\n","\n","    return accuracy,precision,recall,f1, actual, predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnwxV-GHQ2EP","colab_type":"text"},"source":["## train_test_model"]},{"cell_type":"code","metadata":{"id":"kWuJ7_ZwPQlg","colab_type":"code","colab":{}},"source":["def train_test_model(config, i, session, model, train_dataset,test_dataset):\n","  \n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","    #training            \n","    print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\n","    start_time = time.time()\n","    train_acc, train_precision, train_recall, train_f1,actual,predictions = run_epoch(session, config, model, train_dataset, model.train_op, config.keep_prob, True)\n","    print(\"Training Accuracy = %.4f, time = %.3f seconds\\n\"%(train_acc, time.time()-start_time))\n","\n","    #testing\n","    start_time = time.time()\n","    test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    \n","\n","    return test_actual,test_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5GQEegkQ5DA","colab_type":"text"},"source":["## start_epoches"]},{"cell_type":"code","metadata":{"id":"pYlt-2iEPUpE","colab_type":"code","colab":{}},"source":["def start_epoches(config, session,classifier, train_dataset, test_dataset):\n","    all_actual = []\n","    all_predictions = []\n","\n","    for i in range(config.max_max_epoch):\n","      test_actual,test_predictions = train_test_model(config, i, session, classifier, train_dataset, test_dataset)\n","      \n","      all_actual.extend(test_actual)\n","      all_predictions.extend(test_predictions)\n","\n","    return all_actual, all_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3kBeQDxRgKl","colab_type":"text"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"SeOS2yohKwEy","colab_type":"text"},"source":["## assign word2vec"]},{"cell_type":"code","metadata":{"id":"dFvOyWeoPYAn","colab_type":"code","colab":{}},"source":["def word_to_vec(matrix, session,config, *args):\n","    \n","    print(\"word2vec shape: \", matrix.shape)\n","    \n","    for model in args:\n","        session.run(tf.assign(model.embedding, matrix))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddbrDIW8SsV4","colab_type":"text"},"source":["## Open Vectors"]},{"cell_type":"code","metadata":{"id":"K0aqSa1BSvYt","colab_type":"code","outputId":"fe13e478-ee3f-4722-fc7d-98c9aabe7f4f","executionInfo":{"status":"ok","timestamp":1584637972741,"user_tz":-330,"elapsed":4863,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["f = open(vector_path, 'rb')\n","matrix= np.array(pickle.load(f))\n","config.vocab_size=matrix.shape[0]\n","print(config.vocab_size)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["18415\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x3yCllIB6Im-","colab_type":"text"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"id":"seazfYMrhgh5","colab_type":"code","colab":{}},"source":["data_x, data_y = load_data_for_crossVal(path=path,n_words=config.vocab_size)\n","config.num_label= 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7fk98v_6LUT","colab_type":"code","outputId":"25c040e4-0261-4e20-e8c9-6b8807842839","executionInfo":{"status":"ok","timestamp":1584637973346,"user_tz":-330,"elapsed":5433,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","print(\"number label: \"+str(config.num_label))\n","all_dataset = prepare_data(data_x,  data_y)\n","# valid_dataset = prepare_data(valid_dataset[0], valid_dataset[1])\n","# test_dataset = prepare_data(test_dataset[0], test_dataset[1])\n","\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["number label: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYnmpOWR25Lp","colab_type":"code","outputId":"bcc369c3-d0b0-4ca9-d5fb-3c4a1feafc3f","executionInfo":{"status":"ok","timestamp":1584637973347,"user_tz":-330,"elapsed":5416,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["print(all_dataset)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["[array([list([133, 105, 445, 19, 207, 2, 6, 4429, 12, 4430, 1533, 375, 77, 160, 33, 7, 10, 423, 17, 2088, 155, 10, 3205]),\n","       list([204, 26]),\n","       list([318, 180, 4431, 303, 7096, 181, 2512, 7097, 7, 4432, 682, 7098, 723, 48, 7099, 7100, 2513, 4433, 1534, 887, 772, 7101, 7102, 1337, 7103, 1338, 7104, 7105, 34, 150, 127]),\n","       ..., list([298, 6382, 3161]), list([313, 6469, 52, 8, 243]),\n","       list([1222, 60, 147, 9, 14, 281, 3, 27, 10, 2, 6, 1132, 6689, 40])],\n","      dtype=object), array([1, 0, 1, ..., 1, 1, 1], dtype=int32), array([23,  2, 31, ...,  3,  5, 14], dtype=int32)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vuF3jIbg4Q4b","colab_type":"text"},"source":["# Train NN"]},{"cell_type":"code","metadata":{"id":"sZn_oDbv7sWV","colab_type":"code","colab":{}},"source":["def train_nn(train_dataset, test_dataset,i) :\n","  all_actual = []\n","  all_predictions = []\n","\n","  with tf.Graph().as_default(), tf.Session() as session:\n","      initializer = tf.random_normal_initializer(0, 0.05)\n","\n","      classifier= Classifer(config=config, session=session)\n","      saver = tf.train.Saver()\n","\n","      # total=0\n","      # #print trainable variables\n","      # for v in tf.trainable_variables():\n","      #     print(v.name)\n","      #     shape=v.get_shape()\n","      #     try:\n","      #         size=shape[0].value*shape[1].value\n","      #     except:\n","      #         size=shape[0].value\n","      #     total+=size\n","      # print(total)\n","\n","      #initialize\n","      init = tf.global_variables_initializer()\n","\n","      session.run(init)\n","      #train test model\n","\n","      # print (\"model_test\",matrix)\n","\n","      word_to_vec(matrix, session,config, classifier)\n","      all_actual, all_predictions = start_epoches(config, session,classifier, train_dataset,test_dataset)\n","\n","      # if(i%10 ==0 ):\n","      #   save_path = saver.save(session, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/cross_validated/1/\")\n","      #   print(\"Model saved in path: %s\" % save_path)\n","\n","  return all_actual, all_predictions \n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljEdTr622OUT","colab_type":"code","outputId":"06699a44-2348-4b84-a9fd-81e0fa1ba5b0","executionInfo":{"status":"ok","timestamp":1584650091701,"user_tz":-330,"elapsed":553799,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["kf = KFold(n_splits=10)\n","kf.get_n_splits(all_dataset[0])\n","i = 1\n","\n","final_actual = []\n","final_predictions = []\n","\n","\n","\n","for train_index, test_index in kf.split(all_dataset[0]):\n","  print(\"Cross Validation step : \",i)\n","  \n","  train_data_comments, test_data_comments = all_dataset[0][train_index], all_dataset[0][test_index]\n","  train_data_labels, test_data_labels = all_dataset[1][train_index], all_dataset[1][test_index]\n","  train_lengths, test_lengths = all_dataset[2][train_index], all_dataset[2][test_index]\n"," \n","  train_dataset = [train_data_comments,train_data_labels,train_lengths]\n","  test_dataset = [test_data_comments,test_data_labels,test_lengths]\n","\n","  tf.reset_default_graph()\n","\n","  actual , prediction = train_nn(train_dataset, test_dataset,i)\n","\n","  final_actual.extend(actual)\n","  final_predictions.extend(prediction)\n","\n","  i += 1\n","\n","TN, FP, FN, TP = confusion_matrix(final_actual, final_predictions).ravel()\n","\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","f1 = 2 * precision * recall / (precision + recall)\n","accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","print(\"final Accuracy : \",accuracy)\n","print(\"final precision : \",precision)\n","print(\"final recall : \",recall)\n","print(\"final f1 : \",f1)\n","\n","\n"],"execution_count":40,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Time = 1.918 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.251826226915\n","Training Accuracy = 0.9951, time = 21.328 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.223903000354767\n","Test Accuracy = 0.8683, Test Precision = 0.8750, Test Recall = 0.8537, Test F1 = 0.8642\n","\n","Time = 1.931 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.2915726862775045\n","Training Accuracy = 0.9958, time = 21.320 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.770732283592224\n","Test Accuracy = 0.8723, Test Precision = 0.8669, Test Recall = 0.8740, Test F1 = 0.8704\n","\n","Time = 1.940 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","1.5939730225218227\n","Training Accuracy = 0.9938, time = 21.337 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.66279935836792\n","Test Accuracy = 0.8743, Test Precision = 0.8961, Test Recall = 0.8415, Test F1 = 0.8679\n","\n","Time = 1.909 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","1.387766561274475\n","Training Accuracy = 0.9949, time = 21.306 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.815121471881866\n","Test Accuracy = 0.8703, Test Precision = 0.8755, Test Recall = 0.8577, Test F1 = 0.8665\n","\n","Time = 1.908 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","2.2743074360187165\n","Training Accuracy = 0.9933, time = 21.327 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.87898075580597\n","Test Accuracy = 0.8723, Test Precision = 0.8611, Test Recall = 0.8821, Test F1 = 0.8715\n","\n","Time = 1.934 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","2.2794990198890446\n","Training Accuracy = 0.9936, time = 21.354 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.733803868293762\n","Test Accuracy = 0.8822, Test Precision = 0.9048, Test Recall = 0.8496, Test F1 = 0.8763\n","\n","Time = 1.905 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","1.7493972725933418\n","Training Accuracy = 0.9958, time = 21.325 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.86676687002182\n","Test Accuracy = 0.8762, Test Precision = 0.8898, Test Recall = 0.8537, Test F1 = 0.8714\n","\n","Time = 1.906 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","1.192137848085622\n","Training Accuracy = 0.9951, time = 21.327 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.885725736618042\n","Test Accuracy = 0.8703, Test Precision = 0.8694, Test Recall = 0.8659, Test F1 = 0.8676\n","\n","Time = 1.933 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","1.0028416314453352\n","Training Accuracy = 0.9965, time = 21.337 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.198299527168274\n","Test Accuracy = 0.8723, Test Precision = 0.8760, Test Recall = 0.8618, Test F1 = 0.8689\n","\n","Time = 1.915 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","1.5830946353526087\n","Training Accuracy = 0.9956, time = 21.339 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.864394009113312\n","Test Accuracy = 0.8743, Test Precision = 0.8894, Test Recall = 0.8496, Test F1 = 0.8690\n","\n","Time = 1.920 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","1.2477841479121707\n","Training Accuracy = 0.9960, time = 21.328 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.95667314529419\n","Test Accuracy = 0.8723, Test Precision = 0.8730, Test Recall = 0.8659, Test F1 = 0.8694\n","\n","Time = 1.961 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.9131783238990465\n","Training Accuracy = 0.9980, time = 21.272 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.232283115386963\n","Test Accuracy = 0.8683, Test Precision = 0.8543, Test Recall = 0.8821, Test F1 = 0.8680\n","\n","Time = 1.918 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.8946692101744702\n","Training Accuracy = 0.9984, time = 21.267 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.322661995887756\n","Test Accuracy = 0.8782, Test Precision = 0.8903, Test Recall = 0.8577, Test F1 = 0.8737\n","\n","Time = 1.942 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","1.3378807050139585\n","Training Accuracy = 0.9967, time = 21.270 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.704518616199493\n","Test Accuracy = 0.8822, Test Precision = 0.9013, Test Recall = 0.8537, Test F1 = 0.8768\n","\n","Time = 1.922 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.4636538499225935\n","Training Accuracy = 0.9987, time = 21.254 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.42279702425003\n","Test Accuracy = 0.8762, Test Precision = 0.8770, Test Recall = 0.8699, Test F1 = 0.8735\n","\n","Time = 1.933 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.7032828141909704\n","Training Accuracy = 0.9984, time = 21.256 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.687956929206848\n","Test Accuracy = 0.8762, Test Precision = 0.8710, Test Recall = 0.8780, Test F1 = 0.8745\n","\n","Time = 1.937 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.6078988607987412\n","Training Accuracy = 0.9984, time = 21.289 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.555617094039917\n","Test Accuracy = 0.8583, Test Precision = 0.8543, Test Recall = 0.8577, Test F1 = 0.8560\n","\n","Time = 1.931 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.5256367758665874\n","Training Accuracy = 0.9980, time = 21.288 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.367182493209839\n","Test Accuracy = 0.8663, Test Precision = 0.8653, Test Recall = 0.8618, Test F1 = 0.8635\n","\n","Time = 1.931 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.8547491498156887\n","Training Accuracy = 0.9973, time = 21.283 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.854930639266968\n","Test Accuracy = 0.8782, Test Precision = 0.9039, Test Recall = 0.8415, Test F1 = 0.8716\n","\n","Time = 1.939 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.6190965407613476\n","Training Accuracy = 0.9980, time = 21.262 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.825931012630463\n","Test Accuracy = 0.8723, Test Precision = 0.8669, Test Recall = 0.8740, Test F1 = 0.8704\n","\n","Time = 1.932 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.6493964218170731\n","Training Accuracy = 0.9980, time = 21.263 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.883442521095276\n","Test Accuracy = 0.8782, Test Precision = 0.8776, Test Recall = 0.8740, Test F1 = 0.8758\n","\n","Time = 1.932 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","1.1014053781536859\n","Training Accuracy = 0.9978, time = 21.262 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.018302083015442\n","Test Accuracy = 0.8723, Test Precision = 0.8792, Test Recall = 0.8577, Test F1 = 0.8683\n","\n","Time = 1.956 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.5672489729777226\n","Training Accuracy = 0.9980, time = 21.255 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.544197916984558\n","Test Accuracy = 0.8603, Test Precision = 0.8385, Test Recall = 0.8862, Test F1 = 0.8617\n","\n","Time = 1.929 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.6686877352603915\n","Training Accuracy = 0.9976, time = 21.272 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.159586668014526\n","Test Accuracy = 0.8762, Test Precision = 0.8770, Test Recall = 0.8699, Test F1 = 0.8735\n","\n","Time = 1.929 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.5646454040927438\n","Training Accuracy = 0.9984, time = 21.311 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.555251717567444\n","Test Accuracy = 0.8643, Test Precision = 0.8423, Test Recall = 0.8902, Test F1 = 0.8656\n","\n","Time = 1.971 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.46299930925670196\n","Training Accuracy = 0.9980, time = 21.323 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.706900596618652\n","Test Accuracy = 0.8723, Test Precision = 0.8760, Test Recall = 0.8618, Test F1 = 0.8689\n","\n","Time = 1.961 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.6638165735257644\n","Training Accuracy = 0.9984, time = 21.307 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.1887286901474\n","Test Accuracy = 0.8723, Test Precision = 0.8669, Test Recall = 0.8740, Test F1 = 0.8704\n","\n","Time = 1.939 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.3016408749772381\n","Training Accuracy = 0.9989, time = 21.325 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.010846376419067\n","Test Accuracy = 0.8723, Test Precision = 0.8555, Test Recall = 0.8902, Test F1 = 0.8725\n","\n","Time = 1.945 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","0.21272349206310537\n","Training Accuracy = 0.9991, time = 21.315 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.762849807739258\n","Test Accuracy = 0.8762, Test Precision = 0.8740, Test Recall = 0.8740, Test F1 = 0.8740\n","\n","Time = 1.947 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.3543137606002347\n","Training Accuracy = 0.9989, time = 21.304 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.121605157852173\n","Test Accuracy = 0.8762, Test Precision = 0.8770, Test Recall = 0.8699, Test F1 = 0.8735\n","\n","Time = 1.952 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.4374722051106801\n","Training Accuracy = 0.9989, time = 21.306 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.59028446674347\n","Test Accuracy = 0.8643, Test Precision = 0.8397, Test Recall = 0.8943, Test F1 = 0.8661\n","\n","Time = 1.959 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.9471567648397468\n","Training Accuracy = 0.9976, time = 21.337 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.10735261440277\n","Test Accuracy = 0.8762, Test Precision = 0.8802, Test Recall = 0.8659, Test F1 = 0.8730\n","\n","Time = 1.967 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.31299624117309577\n","Training Accuracy = 0.9991, time = 21.305 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.09988021850586\n","Test Accuracy = 0.8743, Test Precision = 0.8617, Test Recall = 0.8862, Test F1 = 0.8737\n","\n","Time = 1.949 seconds\n","\n","Cross Validation step :  4\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","37.23932331800461\n","Training Accuracy = 0.7700, time = 24.978 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.7972540855407715\n","Test Accuracy = 0.7485, Test Precision = 0.6618, Test Recall = 0.9534, Test F1 = 0.7812\n","\n","Time = 1.549 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","27.620402231812477\n","Training Accuracy = 0.8410, time = 22.411 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.126053273677826\n","Test Accuracy = 0.8323, Test Precision = 0.7714, Test Recall = 0.9153, Test F1 = 0.8372\n","\n","Time = 1.555 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","21.14875876903534\n","Training Accuracy = 0.8756, time = 22.071 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.328346312046051\n","Test Accuracy = 0.8623, Test Precision = 0.8615, Test Recall = 0.8432, Test F1 = 0.8522\n","\n","Time = 1.535 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","14.99263846129179\n","Training Accuracy = 0.9195, time = 22.194 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.8476309180259705\n","Test Accuracy = 0.8443, Test Precision = 0.8347, Test Recall = 0.8347, Test F1 = 0.8347\n","\n","Time = 1.538 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","11.840996790677309\n","Training Accuracy = 0.9432, time = 22.209 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.539156079292297\n","Test Accuracy = 0.8383, Test Precision = 0.7778, Test Recall = 0.9195, Test F1 = 0.8427\n","\n","Time = 1.540 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","8.23858383204788\n","Training Accuracy = 0.9610, time = 22.193 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.755680084228516\n","Test Accuracy = 0.8543, Test Precision = 0.8099, Test Recall = 0.9025, Test F1 = 0.8537\n","\n","Time = 1.525 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","6.542627272196114\n","Training Accuracy = 0.9709, time = 22.160 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.864953726530075\n","Test Accuracy = 0.8523, Test Precision = 0.8115, Test Recall = 0.8941, Test F1 = 0.8508\n","\n","Time = 1.540 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","5.896782476454973\n","Training Accuracy = 0.9754, time = 22.180 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.956008970737457\n","Test Accuracy = 0.8543, Test Precision = 0.8075, Test Recall = 0.9068, Test F1 = 0.8543\n","\n","Time = 1.534 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","5.597712435293943\n","Training Accuracy = 0.9736, time = 22.178 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.713002115488052\n","Test Accuracy = 0.8703, Test Precision = 0.8638, Test Recall = 0.8602, Test F1 = 0.8620\n","\n","Time = 1.619 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","3.8782349545508623\n","Training Accuracy = 0.9829, time = 22.225 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.564865291118622\n","Test Accuracy = 0.8743, Test Precision = 0.8560, Test Recall = 0.8814, Test F1 = 0.8685\n","\n","Time = 1.541 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","3.114890013821423\n","Training Accuracy = 0.9891, time = 22.209 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.007514774799347\n","Test Accuracy = 0.8822, Test Precision = 0.8865, Test Recall = 0.8602, Test F1 = 0.8731\n","\n","Time = 1.568 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","2.7707411006558686\n","Training Accuracy = 0.9900, time = 22.192 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.2739357352256775\n","Test Accuracy = 0.8782, Test Precision = 0.8855, Test Recall = 0.8517, Test F1 = 0.8683\n","\n","Time = 1.573 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","2.0422115547116846\n","Training Accuracy = 0.9916, time = 22.182 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.686619222164154\n","Test Accuracy = 0.8762, Test Precision = 0.8566, Test Recall = 0.8856, Test F1 = 0.8708\n","\n","Time = 1.584 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","1.5588209280977026\n","Training Accuracy = 0.9949, time = 22.179 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.10528314113617\n","Test Accuracy = 0.8743, Test Precision = 0.8681, Test Recall = 0.8644, Test F1 = 0.8662\n","\n","Time = 1.563 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","2.403661383810686\n","Training Accuracy = 0.9933, time = 22.169 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.791634023189545\n","Test Accuracy = 0.8802, Test Precision = 0.8793, Test Recall = 0.8644, Test F1 = 0.8718\n","\n","Time = 1.611 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","1.5623952195746824\n","Training Accuracy = 0.9949, time = 22.186 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.157694876194\n","Test Accuracy = 0.8743, Test Precision = 0.8650, Test Recall = 0.8686, Test F1 = 0.8668\n","\n","Time = 1.562 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","2.1077361001516692\n","Training Accuracy = 0.9942, time = 22.207 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.771524846553802\n","Test Accuracy = 0.8902, Test Precision = 0.9095, Test Recall = 0.8517, Test F1 = 0.8796\n","\n","Time = 1.593 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","2.587545344780665\n","Training Accuracy = 0.9925, time = 22.209 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.055332481861115\n","Test Accuracy = 0.8762, Test Precision = 0.8991, Test Recall = 0.8305, Test F1 = 0.8634\n","\n","Time = 1.621 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.1777244241675362\n","Training Accuracy = 0.9962, time = 22.180 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.87348222732544\n","Test Accuracy = 0.8723, Test Precision = 0.8496, Test Recall = 0.8856, Test F1 = 0.8672\n","\n","Time = 1.642 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","0.8779081363172736\n","Training Accuracy = 0.9971, time = 22.170 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.291539251804352\n","Test Accuracy = 0.8743, Test Precision = 0.8681, Test Recall = 0.8644, Test F1 = 0.8662\n","\n","Time = 1.565 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","1.117421708368056\n","Training Accuracy = 0.9949, time = 22.166 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.344409465789795\n","Test Accuracy = 0.8802, Test Precision = 0.8729, Test Recall = 0.8729, Test F1 = 0.8729\n","\n","Time = 1.579 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","2.25164001264784\n","Training Accuracy = 0.9938, time = 22.142 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.679753839969635\n","Test Accuracy = 0.8802, Test Precision = 0.8964, Test Recall = 0.8432, Test F1 = 0.8690\n","\n","Time = 1.564 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","1.575582055884297\n","Training Accuracy = 0.9947, time = 22.142 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.520861029624939\n","Test Accuracy = 0.8942, Test Precision = 0.8894, Test Recall = 0.8856, Test F1 = 0.8875\n","\n","Time = 1.616 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","0.910477186938806\n","Training Accuracy = 0.9980, time = 22.153 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.897079706192017\n","Test Accuracy = 0.8902, Test Precision = 0.8819, Test Recall = 0.8856, Test F1 = 0.8837\n","\n","Time = 1.647 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","0.7341041418767418\n","Training Accuracy = 0.9978, time = 22.114 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.040450632572174\n","Test Accuracy = 0.8842, Test Precision = 0.8771, Test Recall = 0.8771, Test F1 = 0.8771\n","\n","Time = 1.567 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.7519911856252293\n","Training Accuracy = 0.9980, time = 22.118 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.83870267868042\n","Test Accuracy = 0.8902, Test Precision = 0.8987, Test Recall = 0.8644, Test F1 = 0.8812\n","\n","Time = 1.635 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","0.6125920112745007\n","Training Accuracy = 0.9980, time = 22.161 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.353432655334473\n","Test Accuracy = 0.8842, Test Precision = 0.8803, Test Recall = 0.8729, Test F1 = 0.8766\n","\n","Time = 1.638 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","0.4348632538029733\n","Training Accuracy = 0.9984, time = 22.148 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.461486577987671\n","Test Accuracy = 0.8802, Test Precision = 0.8729, Test Recall = 0.8729, Test F1 = 0.8729\n","\n","Time = 1.567 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.6028194467062349\n","Training Accuracy = 0.9978, time = 22.110 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.076598525047302\n","Test Accuracy = 0.8762, Test Precision = 0.8919, Test Recall = 0.8390, Test F1 = 0.8646\n","\n","Time = 1.599 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.8357738502254506\n","Training Accuracy = 0.9973, time = 22.099 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.057725608348846\n","Test Accuracy = 0.8782, Test Precision = 0.8788, Test Recall = 0.8602, Test F1 = 0.8694\n","\n","Time = 1.633 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","1.307046980567975\n","Training Accuracy = 0.9967, time = 22.124 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.766141414642334\n","Test Accuracy = 0.8802, Test Precision = 0.8667, Test Recall = 0.8814, Test F1 = 0.8739\n","\n","Time = 1.659 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.6055274923382967\n","Training Accuracy = 0.9976, time = 22.115 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.029218792915344\n","Test Accuracy = 0.8842, Test Precision = 0.9045, Test Recall = 0.8432, Test F1 = 0.8728\n","\n","Time = 1.588 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.6797853999232757\n","Training Accuracy = 0.9978, time = 22.121 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.59279876947403\n","Test Accuracy = 0.8802, Test Precision = 0.8894, Test Recall = 0.8517, Test F1 = 0.8701\n","\n","Time = 1.599 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","1.0168030621505295\n","Training Accuracy = 0.9973, time = 22.142 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.286221027374268\n","Test Accuracy = 0.8762, Test Precision = 0.8452, Test Recall = 0.9025, Test F1 = 0.8730\n","\n","Time = 1.580 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","1.2310161126915773\n","Training Accuracy = 0.9976, time = 22.138 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.498402416706085\n","Test Accuracy = 0.8842, Test Precision = 0.8836, Test Recall = 0.8686, Test F1 = 0.8761\n","\n","Time = 1.580 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.5745479867227914\n","Training Accuracy = 0.9978, time = 22.119 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.225068211555481\n","Test Accuracy = 0.8882, Test Precision = 0.8879, Test Recall = 0.8729, Test F1 = 0.8803\n","\n","Time = 1.665 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.595773316859777\n","Training Accuracy = 0.9984, time = 22.088 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.210629552602768\n","Test Accuracy = 0.8902, Test Precision = 0.9095, Test Recall = 0.8517, Test F1 = 0.8796\n","\n","Time = 1.636 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.9419794530776926\n","Training Accuracy = 0.9976, time = 22.118 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.065553307533264\n","Test Accuracy = 0.8882, Test Precision = 0.9018, Test Recall = 0.8559, Test F1 = 0.8783\n","\n","Time = 1.584 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.347989126188736\n","Training Accuracy = 0.9991, time = 22.162 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.426467090845108\n","Test Accuracy = 0.8842, Test Precision = 0.8938, Test Recall = 0.8559, Test F1 = 0.8745\n","\n","Time = 1.683 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.501882680381641\n","Training Accuracy = 0.9991, time = 22.133 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.693464994430542\n","Test Accuracy = 0.8862, Test Precision = 0.9013, Test Recall = 0.8517, Test F1 = 0.8758\n","\n","Time = 1.646 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.414204551387229\n","Training Accuracy = 0.9989, time = 22.159 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.011683344841003\n","Test Accuracy = 0.8902, Test Precision = 0.8918, Test Recall = 0.8729, Test F1 = 0.8822\n","\n","Time = 1.595 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.22122368863097108\n","Training Accuracy = 0.9996, time = 22.091 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.117632448673248\n","Test Accuracy = 0.8842, Test Precision = 0.8870, Test Recall = 0.8644, Test F1 = 0.8755\n","\n","Time = 1.671 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.9276635184489805\n","Training Accuracy = 0.9976, time = 22.087 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.124822109937668\n","Test Accuracy = 0.8862, Test Precision = 0.9013, Test Recall = 0.8517, Test F1 = 0.8758\n","\n","Time = 1.658 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.6119118015180902\n","Training Accuracy = 0.9982, time = 22.082 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.457749903202057\n","Test Accuracy = 0.8842, Test Precision = 0.8973, Test Recall = 0.8517, Test F1 = 0.8739\n","\n","Time = 1.735 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.41597749609593393\n","Training Accuracy = 0.9982, time = 22.088 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.174784004688263\n","Test Accuracy = 0.8782, Test Precision = 0.8855, Test Recall = 0.8517, Test F1 = 0.8683\n","\n","Time = 1.659 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","0.28640250075204676\n","Training Accuracy = 0.9987, time = 22.116 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.135500073432922\n","Test Accuracy = 0.8862, Test Precision = 0.9013, Test Recall = 0.8517, Test F1 = 0.8758\n","\n","Time = 1.675 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.1317645901343596\n","Training Accuracy = 1.0000, time = 22.137 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.413412988185883\n","Test Accuracy = 0.8782, Test Precision = 0.8855, Test Recall = 0.8517, Test F1 = 0.8683\n","\n","Time = 1.683 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.5101491291561615\n","Training Accuracy = 0.9987, time = 22.141 seconds\n","\n","Running 501 samples:\n","Total loss:\n","17.86515784263611\n","Test Accuracy = 0.8882, Test Precision = 0.9018, Test Recall = 0.8559, Test F1 = 0.8783\n","\n","Time = 1.668 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.5638096274255986\n","Training Accuracy = 0.9984, time = 22.121 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.38015389442444\n","Test Accuracy = 0.8802, Test Precision = 0.9231, Test Recall = 0.8136, Test F1 = 0.8649\n","\n","Time = 1.593 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.9221379792548987\n","Training Accuracy = 0.9984, time = 22.126 seconds\n","\n","Running 501 samples:\n","Total loss:\n","17.276082158088684\n","Test Accuracy = 0.8842, Test Precision = 0.8973, Test Recall = 0.8517, Test F1 = 0.8739\n","\n","Time = 1.577 seconds\n","\n","Cross Validation step :  5\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","36.45104444026947\n","Training Accuracy = 0.7647, time = 25.180 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.840216487646103\n","Test Accuracy = 0.7445, Test Precision = 0.6802, Test Recall = 0.9617, Test F1 = 0.7968\n","\n","Time = 1.676 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","28.620599418878555\n","Training Accuracy = 0.8377, time = 22.357 seconds\n","\n","Running 501 samples:\n","Total loss:\n","2.998092919588089\n","Test Accuracy = 0.8463, Test Precision = 0.8358, Test Recall = 0.8774, Test F1 = 0.8561\n","\n","Time = 1.642 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","21.57441718876362\n","Training Accuracy = 0.8820, time = 21.911 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.188429430127144\n","Test Accuracy = 0.8563, Test Precision = 0.8593, Test Recall = 0.8659, Test F1 = 0.8626\n","\n","Time = 1.638 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","15.374631401151419\n","Training Accuracy = 0.9204, time = 21.933 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.40973362326622\n","Test Accuracy = 0.8403, Test Precision = 0.8175, Test Recall = 0.8927, Test F1 = 0.8535\n","\n","Time = 1.646 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","10.5666583776474\n","Training Accuracy = 0.9459, time = 22.160 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.30215048789978\n","Test Accuracy = 0.8583, Test Precision = 0.8493, Test Recall = 0.8851, Test F1 = 0.8668\n","\n","Time = 1.650 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","9.069482360035181\n","Training Accuracy = 0.9543, time = 22.077 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.616808533668518\n","Test Accuracy = 0.8563, Test Precision = 0.8270, Test Recall = 0.9157, Test F1 = 0.8691\n","\n","Time = 1.699 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","7.108322286978364\n","Training Accuracy = 0.9636, time = 21.950 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.6066297590732574\n","Test Accuracy = 0.8443, Test Precision = 0.8060, Test Recall = 0.9234, Test F1 = 0.8607\n","\n","Time = 1.575 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","6.463849587831646\n","Training Accuracy = 0.9701, time = 22.007 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.23709499835968\n","Test Accuracy = 0.8663, Test Precision = 0.8702, Test Recall = 0.8736, Test F1 = 0.8719\n","\n","Time = 1.608 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","4.474583959206939\n","Training Accuracy = 0.9794, time = 22.039 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.250957280397415\n","Test Accuracy = 0.8703, Test Precision = 0.8769, Test Recall = 0.8736, Test F1 = 0.8752\n","\n","Time = 1.586 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","3.625068749766797\n","Training Accuracy = 0.9856, time = 22.040 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.978327751159668\n","Test Accuracy = 0.8703, Test Precision = 0.8798, Test Recall = 0.8697, Test F1 = 0.8748\n","\n","Time = 1.593 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","3.24198443768546\n","Training Accuracy = 0.9874, time = 22.017 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.59467762708664\n","Test Accuracy = 0.8723, Test Precision = 0.9020, Test Recall = 0.8467, Test F1 = 0.8735\n","\n","Time = 1.614 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","2.966073004179634\n","Training Accuracy = 0.9876, time = 22.012 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.536738395690918\n","Test Accuracy = 0.8723, Test Precision = 0.8833, Test Recall = 0.8697, Test F1 = 0.8764\n","\n","Time = 1.599 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","2.1560491507989354\n","Training Accuracy = 0.9922, time = 22.030 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.488803923130035\n","Test Accuracy = 0.8723, Test Precision = 0.8774, Test Recall = 0.8774, Test F1 = 0.8774\n","\n","Time = 1.586 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","3.09835165576078\n","Training Accuracy = 0.9891, time = 22.047 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.43178279697895\n","Test Accuracy = 0.8743, Test Precision = 0.8837, Test Recall = 0.8736, Test F1 = 0.8786\n","\n","Time = 1.609 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","2.344193337718025\n","Training Accuracy = 0.9927, time = 22.043 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.7061288356781\n","Test Accuracy = 0.8523, Test Precision = 0.8258, Test Recall = 0.9080, Test F1 = 0.8650\n","\n","Time = 1.609 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","1.9988743452704512\n","Training Accuracy = 0.9931, time = 22.025 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.105435520410538\n","Test Accuracy = 0.8762, Test Precision = 0.8783, Test Recall = 0.8851, Test F1 = 0.8817\n","\n","Time = 1.631 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","1.5078328260569833\n","Training Accuracy = 0.9951, time = 22.050 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.009246468544006\n","Test Accuracy = 0.8723, Test Precision = 0.8803, Test Recall = 0.8736, Test F1 = 0.8769\n","\n","Time = 1.605 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.4794071505966713\n","Training Accuracy = 0.9949, time = 22.043 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.512013673782349\n","Test Accuracy = 0.8663, Test Precision = 0.8731, Test Recall = 0.8697, Test F1 = 0.8714\n","\n","Time = 1.611 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.25947011300741\n","Training Accuracy = 0.9951, time = 22.068 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.78749218583107\n","Test Accuracy = 0.8703, Test Precision = 0.8769, Test Recall = 0.8736, Test F1 = 0.8752\n","\n","Time = 1.633 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","1.173204644524958\n","Training Accuracy = 0.9967, time = 22.033 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.556922286748886\n","Test Accuracy = 0.8683, Test Precision = 0.8884, Test Recall = 0.8544, Test F1 = 0.8711\n","\n","Time = 1.624 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","2.147070810839068\n","Training Accuracy = 0.9938, time = 22.025 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.812926054000854\n","Test Accuracy = 0.8683, Test Precision = 0.8598, Test Recall = 0.8927, Test F1 = 0.8759\n","\n","Time = 1.623 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","1.515265150286723\n","Training Accuracy = 0.9956, time = 21.994 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.2241892516613\n","Test Accuracy = 0.8743, Test Precision = 0.8722, Test Recall = 0.8889, Test F1 = 0.8805\n","\n","Time = 1.630 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","0.7172434701715247\n","Training Accuracy = 0.9971, time = 21.970 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.268900275230408\n","Test Accuracy = 0.8723, Test Precision = 0.8717, Test Recall = 0.8851, Test F1 = 0.8783\n","\n","Time = 1.690 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","0.8590885840567353\n","Training Accuracy = 0.9976, time = 21.986 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.41343867778778\n","Test Accuracy = 0.8683, Test Precision = 0.8916, Test Recall = 0.8506, Test F1 = 0.8706\n","\n","Time = 1.684 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","0.8769333207669661\n","Training Accuracy = 0.9976, time = 21.996 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.130415141582489\n","Test Accuracy = 0.8743, Test Precision = 0.8694, Test Recall = 0.8927, Test F1 = 0.8809\n","\n","Time = 1.722 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.9785095145343803\n","Training Accuracy = 0.9969, time = 22.018 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.326523840427399\n","Test Accuracy = 0.8683, Test Precision = 0.8445, Test Recall = 0.9157, Test F1 = 0.8787\n","\n","Time = 1.720 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","0.6229855613028121\n","Training Accuracy = 0.9978, time = 22.034 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.740354478359222\n","Test Accuracy = 0.8703, Test Precision = 0.8603, Test Recall = 0.8966, Test F1 = 0.8780\n","\n","Time = 1.697 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","1.19284372767288\n","Training Accuracy = 0.9971, time = 22.023 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.202005445957184\n","Test Accuracy = 0.8623, Test Precision = 0.8636, Test Recall = 0.8736, Test F1 = 0.8686\n","\n","Time = 1.702 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.7723966494959313\n","Training Accuracy = 0.9969, time = 21.993 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.661137640476227\n","Test Accuracy = 0.8623, Test Precision = 0.8504, Test Recall = 0.8927, Test F1 = 0.8710\n","\n","Time = 1.711 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.5637643845393541\n","Training Accuracy = 0.9984, time = 21.948 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.618451654911041\n","Test Accuracy = 0.8643, Test Precision = 0.8845, Test Recall = 0.8506, Test F1 = 0.8672\n","\n","Time = 1.625 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","1.6880296887597979\n","Training Accuracy = 0.9967, time = 21.937 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.667549908161163\n","Test Accuracy = 0.8623, Test Precision = 0.8664, Test Recall = 0.8697, Test F1 = 0.8681\n","\n","Time = 1.696 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.43985355365066425\n","Training Accuracy = 0.9989, time = 21.926 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.015641272068024\n","Test Accuracy = 0.8703, Test Precision = 0.8712, Test Recall = 0.8812, Test F1 = 0.8762\n","\n","Time = 1.678 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.45964749012910033\n","Training Accuracy = 0.9976, time = 21.964 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.987832963466644\n","Test Accuracy = 0.8683, Test Precision = 0.8736, Test Recall = 0.8736, Test F1 = 0.8736\n","\n","Time = 1.716 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.6616640367874425\n","Training Accuracy = 0.9980, time = 21.967 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.328410148620605\n","Test Accuracy = 0.8683, Test Precision = 0.8679, Test Recall = 0.8812, Test F1 = 0.8745\n","\n","Time = 1.655 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.441411923452506\n","Training Accuracy = 0.9984, time = 21.979 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.869104743003845\n","Test Accuracy = 0.8723, Test Precision = 0.8774, Test Recall = 0.8774, Test F1 = 0.8774\n","\n","Time = 1.695 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.3046710760972928\n","Training Accuracy = 0.9993, time = 21.985 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.976873636245728\n","Test Accuracy = 0.8623, Test Precision = 0.8840, Test Recall = 0.8467, Test F1 = 0.8650\n","\n","Time = 1.828 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","1.2085932962791048\n","Training Accuracy = 0.9973, time = 21.974 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.068166673183441\n","Test Accuracy = 0.8643, Test Precision = 0.8876, Test Recall = 0.8467, Test F1 = 0.8667\n","\n","Time = 1.676 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.9577784930122561\n","Training Accuracy = 0.9969, time = 21.992 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.3296919465065\n","Test Accuracy = 0.8782, Test Precision = 0.8906, Test Recall = 0.8736, Test F1 = 0.8820\n","\n","Time = 1.712 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.6818562403786927\n","Training Accuracy = 0.9971, time = 21.924 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.353182673454285\n","Test Accuracy = 0.8762, Test Precision = 0.8812, Test Recall = 0.8812, Test F1 = 0.8812\n","\n","Time = 1.658 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.8347314410821127\n","Training Accuracy = 0.9980, time = 21.955 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.787864536046982\n","Test Accuracy = 0.8663, Test Precision = 0.8911, Test Recall = 0.8467, Test F1 = 0.8684\n","\n","Time = 1.658 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","1.1521909110824708\n","Training Accuracy = 0.9967, time = 21.961 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.556348502635956\n","Test Accuracy = 0.8643, Test Precision = 0.8755, Test Recall = 0.8621, Test F1 = 0.8687\n","\n","Time = 1.712 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.8919186814682689\n","Training Accuracy = 0.9980, time = 21.984 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.865468740463257\n","Test Accuracy = 0.8723, Test Precision = 0.8745, Test Recall = 0.8812, Test F1 = 0.8779\n","\n","Time = 1.654 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.31391287840847326\n","Training Accuracy = 0.9989, time = 21.988 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.373777449131012\n","Test Accuracy = 0.8723, Test Precision = 0.8774, Test Recall = 0.8774, Test F1 = 0.8774\n","\n","Time = 1.744 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.47687300065626914\n","Training Accuracy = 0.9982, time = 21.983 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.764264523983002\n","Test Accuracy = 0.8603, Test Precision = 0.9064, Test Recall = 0.8161, Test F1 = 0.8589\n","\n","Time = 1.631 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","1.4164246787877346\n","Training Accuracy = 0.9971, time = 21.955 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.543438643217087\n","Test Accuracy = 0.8623, Test Precision = 0.8934, Test Recall = 0.8352, Test F1 = 0.8634\n","\n","Time = 1.667 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","1.0493581939736032\n","Training Accuracy = 0.9969, time = 21.978 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.400947242975235\n","Test Accuracy = 0.8683, Test Precision = 0.8707, Test Recall = 0.8774, Test F1 = 0.8740\n","\n","Time = 1.691 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.6171588709785283\n","Training Accuracy = 0.9980, time = 22.029 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.413264274597168\n","Test Accuracy = 0.8643, Test Precision = 0.8669, Test Recall = 0.8736, Test F1 = 0.8702\n","\n","Time = 1.725 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.5397446335632594\n","Training Accuracy = 0.9989, time = 22.037 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.022714138031006\n","Test Accuracy = 0.8703, Test Precision = 0.8684, Test Recall = 0.8851, Test F1 = 0.8767\n","\n","Time = 1.700 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.36380692070127907\n","Training Accuracy = 0.9987, time = 21.982 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.277721047401428\n","Test Accuracy = 0.8663, Test Precision = 0.8593, Test Recall = 0.8889, Test F1 = 0.8738\n","\n","Time = 1.703 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.569533207769382\n","Training Accuracy = 0.9984, time = 21.957 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.714610695838928\n","Test Accuracy = 0.8683, Test Precision = 0.8707, Test Recall = 0.8774, Test F1 = 0.8740\n","\n","Time = 1.699 seconds\n","\n","Cross Validation step :  6\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","37.14559197425842\n","Training Accuracy = 0.7578, time = 25.440 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.959436386823654\n","Test Accuracy = 0.7565, Test Precision = 0.7006, Test Recall = 0.9141, Test F1 = 0.7932\n","\n","Time = 1.527 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","29.70850831270218\n","Training Accuracy = 0.8255, time = 22.707 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.079224497079849\n","Test Accuracy = 0.8443, Test Precision = 0.8803, Test Recall = 0.8047, Test F1 = 0.8408\n","\n","Time = 1.450 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","20.091937109827995\n","Training Accuracy = 0.8902, time = 22.166 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.24045604467392\n","Test Accuracy = 0.8583, Test Precision = 0.8745, Test Recall = 0.8438, Test F1 = 0.8588\n","\n","Time = 1.504 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","15.201986618340015\n","Training Accuracy = 0.9204, time = 22.214 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.098564326763153\n","Test Accuracy = 0.8104, Test Precision = 0.7766, Test Recall = 0.8828, Test F1 = 0.8263\n","\n","Time = 1.458 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","10.684576153755188\n","Training Accuracy = 0.9450, time = 22.425 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.555236041545868\n","Test Accuracy = 0.8323, Test Precision = 0.8094, Test Recall = 0.8789, Test F1 = 0.8427\n","\n","Time = 1.499 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","9.232144795358181\n","Training Accuracy = 0.9596, time = 22.344 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.748591959476471\n","Test Accuracy = 0.8423, Test Precision = 0.8266, Test Recall = 0.8750, Test F1 = 0.8501\n","\n","Time = 1.486 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","6.990178513340652\n","Training Accuracy = 0.9674, time = 22.266 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.92291796207428\n","Test Accuracy = 0.8583, Test Precision = 0.8715, Test Recall = 0.8477, Test F1 = 0.8594\n","\n","Time = 1.474 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","8.39202325232327\n","Training Accuracy = 0.9687, time = 22.273 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.7100090980529785\n","Test Accuracy = 0.8603, Test Precision = 0.8811, Test Recall = 0.8398, Test F1 = 0.8600\n","\n","Time = 1.515 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","5.339481305330992\n","Training Accuracy = 0.9785, time = 22.350 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.068107783794403\n","Test Accuracy = 0.8403, Test Precision = 0.8284, Test Recall = 0.8672, Test F1 = 0.8473\n","\n","Time = 1.480 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","3.7430980235803872\n","Training Accuracy = 0.9843, time = 22.352 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.218535423278809\n","Test Accuracy = 0.8623, Test Precision = 0.8848, Test Recall = 0.8398, Test F1 = 0.8617\n","\n","Time = 1.472 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","2.994709867518395\n","Training Accuracy = 0.9891, time = 22.342 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.283614337444305\n","Test Accuracy = 0.8523, Test Precision = 0.8583, Test Recall = 0.8516, Test F1 = 0.8549\n","\n","Time = 1.507 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","2.548934481339529\n","Training Accuracy = 0.9905, time = 22.344 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.563317716121674\n","Test Accuracy = 0.8483, Test Precision = 0.8435, Test Recall = 0.8633, Test F1 = 0.8533\n","\n","Time = 1.504 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","3.108277778257616\n","Training Accuracy = 0.9896, time = 22.328 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.896325051784515\n","Test Accuracy = 0.8543, Test Precision = 0.8704, Test Recall = 0.8398, Test F1 = 0.8549\n","\n","Time = 1.493 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","2.5211842871503904\n","Training Accuracy = 0.9911, time = 22.333 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.965753853321075\n","Test Accuracy = 0.8603, Test Precision = 0.8661, Test Recall = 0.8594, Test F1 = 0.8627\n","\n","Time = 1.492 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","2.2140482320101\n","Training Accuracy = 0.9927, time = 22.317 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.108444929122925\n","Test Accuracy = 0.8563, Test Precision = 0.8770, Test Recall = 0.8359, Test F1 = 0.8560\n","\n","Time = 1.532 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","1.8727952452609316\n","Training Accuracy = 0.9931, time = 22.301 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.655206859111786\n","Test Accuracy = 0.8523, Test Precision = 0.8760, Test Recall = 0.8281, Test F1 = 0.8514\n","\n","Time = 1.500 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","1.7117030201188754\n","Training Accuracy = 0.9945, time = 22.255 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.72548544406891\n","Test Accuracy = 0.8523, Test Precision = 0.8957, Test Recall = 0.8047, Test F1 = 0.8477\n","\n","Time = 1.491 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.3249631885555573\n","Training Accuracy = 0.9960, time = 22.290 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.73280668258667\n","Test Accuracy = 0.8523, Test Precision = 0.8760, Test Recall = 0.8281, Test F1 = 0.8514\n","\n","Time = 1.509 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","2.3548738012905233\n","Training Accuracy = 0.9925, time = 22.282 seconds\n","\n","Running 501 samples:\n","Total loss:\n","20.540196895599365\n","Test Accuracy = 0.8603, Test Precision = 0.9079, Test Recall = 0.8086, Test F1 = 0.8554\n","\n","Time = 1.510 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","2.278561805243953\n","Training Accuracy = 0.9931, time = 22.312 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.101460218429565\n","Test Accuracy = 0.8503, Test Precision = 0.8694, Test Recall = 0.8320, Test F1 = 0.8503\n","\n","Time = 1.501 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","1.4699721662182128\n","Training Accuracy = 0.9953, time = 22.326 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.85740065574646\n","Test Accuracy = 0.8483, Test Precision = 0.8629, Test Recall = 0.8359, Test F1 = 0.8492\n","\n","Time = 1.516 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","0.74690961863962\n","Training Accuracy = 0.9980, time = 22.300 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.50875949859619\n","Test Accuracy = 0.8503, Test Precision = 0.8851, Test Recall = 0.8125, Test F1 = 0.8473\n","\n","Time = 1.503 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","0.7473975487810094\n","Training Accuracy = 0.9982, time = 22.239 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.368661999702454\n","Test Accuracy = 0.8583, Test Precision = 0.8745, Test Recall = 0.8438, Test F1 = 0.8588\n","\n","Time = 1.519 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","0.8468954689278689\n","Training Accuracy = 0.9982, time = 22.211 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.880754828453064\n","Test Accuracy = 0.8583, Test Precision = 0.8715, Test Recall = 0.8477, Test F1 = 0.8594\n","\n","Time = 1.518 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","0.7364679239108227\n","Training Accuracy = 0.9976, time = 22.214 seconds\n","\n","Running 501 samples:\n","Total loss:\n","21.728721857070923\n","Test Accuracy = 0.8563, Test Precision = 0.9000, Test Recall = 0.8086, Test F1 = 0.8519\n","\n","Time = 1.518 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.5311851812075474\n","Training Accuracy = 0.9978, time = 22.234 seconds\n","\n","Running 501 samples:\n","Total loss:\n","20.582343101501465\n","Test Accuracy = 0.8643, Test Precision = 0.8790, Test Recall = 0.8516, Test F1 = 0.8651\n","\n","Time = 1.503 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","1.0922236885553502\n","Training Accuracy = 0.9976, time = 22.243 seconds\n","\n","Running 501 samples:\n","Total loss:\n","21.894707918167114\n","Test Accuracy = 0.8583, Test Precision = 0.8838, Test Recall = 0.8320, Test F1 = 0.8571\n","\n","Time = 1.519 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","0.9473413245050324\n","Training Accuracy = 0.9962, time = 22.281 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.444796442985535\n","Test Accuracy = 0.8623, Test Precision = 0.8696, Test Recall = 0.8594, Test F1 = 0.8644\n","\n","Time = 1.530 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","1.4320185720789596\n","Training Accuracy = 0.9971, time = 22.272 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.848121762275696\n","Test Accuracy = 0.8483, Test Precision = 0.8516, Test Recall = 0.8516, Test F1 = 0.8516\n","\n","Time = 1.510 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.7878166960454109\n","Training Accuracy = 0.9980, time = 22.313 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.15823006629944\n","Test Accuracy = 0.8623, Test Precision = 0.9013, Test Recall = 0.8203, Test F1 = 0.8589\n","\n","Time = 1.516 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","0.7398947459114424\n","Training Accuracy = 0.9969, time = 22.255 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.26256573200226\n","Test Accuracy = 0.8603, Test Precision = 0.8908, Test Recall = 0.8281, Test F1 = 0.8583\n","\n","Time = 1.524 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.9054356273040867\n","Training Accuracy = 0.9978, time = 22.194 seconds\n","\n","Running 501 samples:\n","Total loss:\n","20.210009574890137\n","Test Accuracy = 0.8603, Test Precision = 0.9079, Test Recall = 0.8086, Test F1 = 0.8554\n","\n","Time = 1.532 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.4931134973321605\n","Training Accuracy = 0.9987, time = 22.241 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.483654141426086\n","Test Accuracy = 0.8623, Test Precision = 0.8912, Test Recall = 0.8320, Test F1 = 0.8606\n","\n","Time = 1.519 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.35299556265999854\n","Training Accuracy = 0.9987, time = 22.274 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.85911065340042\n","Test Accuracy = 0.8603, Test Precision = 0.8750, Test Recall = 0.8477, Test F1 = 0.8611\n","\n","Time = 1.535 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.33141368887140743\n","Training Accuracy = 0.9991, time = 22.301 seconds\n","\n","Running 501 samples:\n","Total loss:\n","21.335055112838745\n","Test Accuracy = 0.8563, Test Precision = 0.8866, Test Recall = 0.8242, Test F1 = 0.8543\n","\n","Time = 1.534 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.5400089146128266\n","Training Accuracy = 0.9989, time = 22.269 seconds\n","\n","Running 501 samples:\n","Total loss:\n","21.10991072654724\n","Test Accuracy = 0.8543, Test Precision = 0.8797, Test Recall = 0.8281, Test F1 = 0.8531\n","\n","Time = 1.510 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.4057836104893795\n","Training Accuracy = 0.9991, time = 22.249 seconds\n","\n","Running 501 samples:\n","Total loss:\n","22.859418034553528\n","Test Accuracy = 0.8603, Test Precision = 0.8875, Test Recall = 0.8320, Test F1 = 0.8589\n","\n","Time = 1.539 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.8261023192462744\n","Training Accuracy = 0.9978, time = 22.235 seconds\n","\n","Running 501 samples:\n","Total loss:\n","22.758156538009644\n","Test Accuracy = 0.8683, Test Precision = 0.8958, Test Recall = 0.8398, Test F1 = 0.8669\n","\n","Time = 1.537 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.3785247428736511\n","Training Accuracy = 0.9993, time = 22.249 seconds\n","\n","Running 501 samples:\n","Total loss:\n","22.48634660243988\n","Test Accuracy = 0.8623, Test Precision = 0.8755, Test Recall = 0.8516, Test F1 = 0.8634\n","\n","Time = 1.512 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.5542381426373595\n","Training Accuracy = 0.9980, time = 22.241 seconds\n","\n","Running 501 samples:\n","Total loss:\n","24.021571397781372\n","Test Accuracy = 0.8503, Test Precision = 0.8467, Test Recall = 0.8633, Test F1 = 0.8549\n","\n","Time = 1.519 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.5846908340686809\n","Training Accuracy = 0.9984, time = 22.252 seconds\n","\n","Running 501 samples:\n","Total loss:\n","25.662932634353638\n","Test Accuracy = 0.8503, Test Precision = 0.8787, Test Recall = 0.8203, Test F1 = 0.8485\n","\n","Time = 1.531 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.24962099507820312\n","Training Accuracy = 0.9991, time = 22.258 seconds\n","\n","Running 501 samples:\n","Total loss:\n","26.380391120910645\n","Test Accuracy = 0.8523, Test Precision = 0.8792, Test Recall = 0.8242, Test F1 = 0.8508\n","\n","Time = 1.516 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.5442933245331005\n","Training Accuracy = 0.9982, time = 22.257 seconds\n","\n","Running 501 samples:\n","Total loss:\n","27.0675128698349\n","Test Accuracy = 0.8563, Test Precision = 0.8866, Test Recall = 0.8242, Test F1 = 0.8543\n","\n","Time = 1.526 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.5736181608764355\n","Training Accuracy = 0.9980, time = 22.261 seconds\n","\n","Running 501 samples:\n","Total loss:\n","26.179145574569702\n","Test Accuracy = 0.8523, Test Precision = 0.8824, Test Recall = 0.8203, Test F1 = 0.8502\n","\n","Time = 1.603 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","1.3918848514881574\n","Training Accuracy = 0.9980, time = 22.269 seconds\n","\n","Running 501 samples:\n","Total loss:\n","26.113188982009888\n","Test Accuracy = 0.8603, Test Precision = 0.9043, Test Recall = 0.8125, Test F1 = 0.8560\n","\n","Time = 1.527 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","2.0173666454211343\n","Training Accuracy = 0.9947, time = 22.276 seconds\n","\n","Running 501 samples:\n","Total loss:\n","22.160620093345642\n","Test Accuracy = 0.8523, Test Precision = 0.8730, Test Recall = 0.8320, Test F1 = 0.8520\n","\n","Time = 1.570 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.9969949630285555\n","Training Accuracy = 0.9976, time = 22.283 seconds\n","\n","Running 501 samples:\n","Total loss:\n","22.66794991493225\n","Test Accuracy = 0.8483, Test Precision = 0.8600, Test Recall = 0.8398, Test F1 = 0.8498\n","\n","Time = 1.528 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.5710849182814854\n","Training Accuracy = 0.9976, time = 22.272 seconds\n","\n","Running 501 samples:\n","Total loss:\n","27.287699699401855\n","Test Accuracy = 0.8503, Test Precision = 0.8952, Test Recall = 0.8008, Test F1 = 0.8454\n","\n","Time = 1.525 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.5295378523369436\n","Training Accuracy = 0.9984, time = 22.256 seconds\n","\n","Running 501 samples:\n","Total loss:\n","25.352528929710388\n","Test Accuracy = 0.8503, Test Precision = 0.8694, Test Recall = 0.8320, Test F1 = 0.8503\n","\n","Time = 1.545 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","1.160188459721553\n","Training Accuracy = 0.9980, time = 22.254 seconds\n","\n","Running 501 samples:\n","Total loss:\n","23.583395838737488\n","Test Accuracy = 0.8423, Test Precision = 0.8242, Test Recall = 0.8789, Test F1 = 0.8507\n","\n","Time = 1.538 seconds\n","\n","Cross Validation step :  7\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","36.93771007657051\n","Training Accuracy = 0.7574, time = 24.382 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.821669191122055\n","Test Accuracy = 0.7285, Test Precision = 0.6306, Test Recall = 0.9417, Test F1 = 0.7554\n","\n","Time = 1.770 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","27.761906430125237\n","Training Accuracy = 0.8334, time = 22.023 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.8889312148094177\n","Test Accuracy = 0.7705, Test Precision = 0.6720, Test Recall = 0.9462, Test F1 = 0.7858\n","\n","Time = 1.753 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","21.23278333619237\n","Training Accuracy = 0.8767, time = 21.491 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.535838335752487\n","Test Accuracy = 0.8164, Test Precision = 0.7399, Test Recall = 0.9058, Test F1 = 0.8145\n","\n","Time = 1.730 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","15.219160184264183\n","Training Accuracy = 0.9188, time = 21.514 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.247647613286972\n","Test Accuracy = 0.8024, Test Precision = 0.7123, Test Recall = 0.9327, Test F1 = 0.8078\n","\n","Time = 1.750 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","12.673970721662045\n","Training Accuracy = 0.9286, time = 21.706 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.981760412454605\n","Test Accuracy = 0.8643, Test Precision = 0.8063, Test Recall = 0.9148, Test F1 = 0.8571\n","\n","Time = 1.736 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","8.643928907811642\n","Training Accuracy = 0.9543, time = 21.643 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.81074121594429\n","Test Accuracy = 0.8643, Test Precision = 0.8216, Test Recall = 0.8879, Test F1 = 0.8534\n","\n","Time = 1.752 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","6.93826911598444\n","Training Accuracy = 0.9654, time = 21.576 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.0591680109500885\n","Test Accuracy = 0.8583, Test Precision = 0.8040, Test Recall = 0.9013, Test F1 = 0.8499\n","\n","Time = 1.742 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","5.30504488479346\n","Training Accuracy = 0.9745, time = 21.621 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.688594669103622\n","Test Accuracy = 0.8643, Test Precision = 0.8384, Test Recall = 0.8610, Test F1 = 0.8496\n","\n","Time = 1.808 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","4.056398655287921\n","Training Accuracy = 0.9796, time = 21.632 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.470981240272522\n","Test Accuracy = 0.8683, Test Precision = 0.8428, Test Recall = 0.8655, Test F1 = 0.8540\n","\n","Time = 1.750 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","4.143518636003137\n","Training Accuracy = 0.9836, time = 21.679 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.458695292472839\n","Test Accuracy = 0.8782, Test Precision = 0.8403, Test Recall = 0.8969, Test F1 = 0.8677\n","\n","Time = 1.776 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","4.18070845073089\n","Training Accuracy = 0.9849, time = 21.660 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.028703302145004\n","Test Accuracy = 0.8942, Test Precision = 0.9087, Test Recall = 0.8475, Test F1 = 0.8770\n","\n","Time = 1.788 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","3.8693041705992073\n","Training Accuracy = 0.9858, time = 21.655 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.806203782558441\n","Test Accuracy = 0.8483, Test Precision = 0.7753, Test Recall = 0.9283, Test F1 = 0.8449\n","\n","Time = 1.787 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","4.229881122242659\n","Training Accuracy = 0.9858, time = 21.658 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.316307753324509\n","Test Accuracy = 0.8663, Test Precision = 0.8071, Test Recall = 0.9193, Test F1 = 0.8595\n","\n","Time = 1.800 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","2.526191545650363\n","Training Accuracy = 0.9900, time = 21.655 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.2303343415260315\n","Test Accuracy = 0.8842, Test Precision = 0.8511, Test Recall = 0.8969, Test F1 = 0.8734\n","\n","Time = 1.811 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","1.894867751892889\n","Training Accuracy = 0.9922, time = 21.652 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.492143213748932\n","Test Accuracy = 0.8583, Test Precision = 0.7923, Test Recall = 0.9238, Test F1 = 0.8530\n","\n","Time = 1.814 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","2.826702667749487\n","Training Accuracy = 0.9920, time = 21.629 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.975691914558411\n","Test Accuracy = 0.8782, Test Precision = 0.8320, Test Recall = 0.9103, Test F1 = 0.8694\n","\n","Time = 1.769 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","1.363874562899582\n","Training Accuracy = 0.9953, time = 21.640 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.501989424228668\n","Test Accuracy = 0.8703, Test Precision = 0.8160, Test Recall = 0.9148, Test F1 = 0.8626\n","\n","Time = 1.795 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.4040939627957414\n","Training Accuracy = 0.9945, time = 21.625 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.169174253940582\n","Test Accuracy = 0.8902, Test Precision = 0.8529, Test Recall = 0.9103, Test F1 = 0.8807\n","\n","Time = 1.821 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.1054248496657237\n","Training Accuracy = 0.9960, time = 21.642 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.364787995815277\n","Test Accuracy = 0.8703, Test Precision = 0.8135, Test Recall = 0.9193, Test F1 = 0.8632\n","\n","Time = 1.792 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","1.2048638350970577\n","Training Accuracy = 0.9956, time = 21.624 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.880074143409729\n","Test Accuracy = 0.8882, Test Precision = 0.8615, Test Recall = 0.8924, Test F1 = 0.8767\n","\n","Time = 1.955 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","0.9181396379535727\n","Training Accuracy = 0.9971, time = 21.621 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.840509474277496\n","Test Accuracy = 0.8922, Test Precision = 0.8536, Test Recall = 0.9148, Test F1 = 0.8831\n","\n","Time = 1.818 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","0.5945558451603574\n","Training Accuracy = 0.9984, time = 21.627 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.384612619876862\n","Test Accuracy = 0.8882, Test Precision = 0.8436, Test Recall = 0.9193, Test F1 = 0.8798\n","\n","Time = 1.934 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","1.0518158687045798\n","Training Accuracy = 0.9978, time = 21.605 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.396404206752777\n","Test Accuracy = 0.8862, Test Precision = 0.8547, Test Recall = 0.8969, Test F1 = 0.8753\n","\n","Time = 2.005 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","1.0290170247326387\n","Training Accuracy = 0.9976, time = 21.631 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.639875411987305\n","Test Accuracy = 0.8802, Test Precision = 0.8498, Test Recall = 0.8879, Test F1 = 0.8684\n","\n","Time = 1.914 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","1.933999970118748\n","Training Accuracy = 0.9942, time = 21.572 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.155344605445862\n","Test Accuracy = 0.8862, Test Precision = 0.8640, Test Recall = 0.8834, Test F1 = 0.8736\n","\n","Time = 1.861 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","2.034969782740518\n","Training Accuracy = 0.9947, time = 21.577 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.026371419429779\n","Test Accuracy = 0.8623, Test Precision = 0.8105, Test Recall = 0.9013, Test F1 = 0.8535\n","\n","Time = 1.955 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","1.1936800147523172\n","Training Accuracy = 0.9960, time = 21.595 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.723652184009552\n","Test Accuracy = 0.8663, Test Precision = 0.8197, Test Recall = 0.8969, Test F1 = 0.8565\n","\n","Time = 1.785 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","1.017871701194963\n","Training Accuracy = 0.9978, time = 21.597 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.595239877700806\n","Test Accuracy = 0.8683, Test Precision = 0.8340, Test Recall = 0.8789, Test F1 = 0.8559\n","\n","Time = 1.954 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.680064625958039\n","Training Accuracy = 0.9978, time = 21.648 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.972510993480682\n","Test Accuracy = 0.8683, Test Precision = 0.8340, Test Recall = 0.8789, Test F1 = 0.8559\n","\n","Time = 1.948 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","1.5264736493263626\n","Training Accuracy = 0.9960, time = 21.608 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.38135051727295\n","Test Accuracy = 0.8743, Test Precision = 0.8448, Test Recall = 0.8789, Test F1 = 0.8615\n","\n","Time = 1.937 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","1.3510149723442737\n","Training Accuracy = 0.9969, time = 21.568 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.891000092029572\n","Test Accuracy = 0.8723, Test Precision = 0.8472, Test Recall = 0.8700, Test F1 = 0.8584\n","\n","Time = 1.828 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","1.4648876530263806\n","Training Accuracy = 0.9949, time = 21.608 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.214543402194977\n","Test Accuracy = 0.8703, Test Precision = 0.8292, Test Recall = 0.8924, Test F1 = 0.8596\n","\n","Time = 1.962 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.8458020482976281\n","Training Accuracy = 0.9976, time = 21.607 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.520539343357086\n","Test Accuracy = 0.8663, Test Precision = 0.8223, Test Recall = 0.8924, Test F1 = 0.8559\n","\n","Time = 2.005 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.61823826948239\n","Training Accuracy = 0.9976, time = 21.594 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.328888356685638\n","Test Accuracy = 0.8663, Test Precision = 0.8223, Test Recall = 0.8924, Test F1 = 0.8559\n","\n","Time = 1.960 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.9808171102486085\n","Training Accuracy = 0.9978, time = 21.634 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.781036972999573\n","Test Accuracy = 0.8643, Test Precision = 0.8270, Test Recall = 0.8789, Test F1 = 0.8522\n","\n","Time = 1.822 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.3750048588694881\n","Training Accuracy = 0.9982, time = 21.611 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.149381816387177\n","Test Accuracy = 0.8683, Test Precision = 0.8340, Test Recall = 0.8789, Test F1 = 0.8559\n","\n","Time = 1.940 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.7754095523450815\n","Training Accuracy = 0.9980, time = 21.610 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.81151419878006\n","Test Accuracy = 0.8723, Test Precision = 0.8299, Test Recall = 0.8969, Test F1 = 0.8621\n","\n","Time = 1.930 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","1.0105752960698737\n","Training Accuracy = 0.9971, time = 21.592 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.402767419815063\n","Test Accuracy = 0.8743, Test Precision = 0.8226, Test Recall = 0.9148, Test F1 = 0.8662\n","\n","Time = 1.990 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.8236035706922848\n","Training Accuracy = 0.9976, time = 21.607 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.432459712028503\n","Test Accuracy = 0.8723, Test Precision = 0.8326, Test Recall = 0.8924, Test F1 = 0.8615\n","\n","Time = 1.979 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.4821555163252924\n","Training Accuracy = 0.9984, time = 21.599 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.207854568958282\n","Test Accuracy = 0.8782, Test Precision = 0.8522, Test Recall = 0.8789, Test F1 = 0.8653\n","\n","Time = 1.967 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.889828263614163\n","Training Accuracy = 0.9980, time = 21.606 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.52703982591629\n","Test Accuracy = 0.8683, Test Precision = 0.8153, Test Recall = 0.9103, Test F1 = 0.8602\n","\n","Time = 1.963 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.6641194989642827\n","Training Accuracy = 0.9980, time = 21.607 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.413931250572205\n","Test Accuracy = 0.8583, Test Precision = 0.8040, Test Recall = 0.9013, Test F1 = 0.8499\n","\n","Time = 1.859 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.7311896472492663\n","Training Accuracy = 0.9969, time = 21.618 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.843529045581818\n","Test Accuracy = 0.8723, Test Precision = 0.8565, Test Recall = 0.8565, Test F1 = 0.8565\n","\n","Time = 1.968 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","1.067743369206255\n","Training Accuracy = 0.9978, time = 21.622 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.523511737585068\n","Test Accuracy = 0.8842, Test Precision = 0.8667, Test Recall = 0.8744, Test F1 = 0.8705\n","\n","Time = 1.961 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.44044157994085253\n","Training Accuracy = 0.9987, time = 21.623 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.891664385795593\n","Test Accuracy = 0.8743, Test Precision = 0.8390, Test Recall = 0.8879, Test F1 = 0.8627\n","\n","Time = 1.840 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","0.4071802640073656\n","Training Accuracy = 0.9984, time = 21.591 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.199209153652191\n","Test Accuracy = 0.8703, Test Precision = 0.8292, Test Recall = 0.8924, Test F1 = 0.8596\n","\n","Time = 1.970 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.3714171520241507\n","Training Accuracy = 0.9989, time = 21.587 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.644501030445099\n","Test Accuracy = 0.8882, Test Precision = 0.8711, Test Recall = 0.8789, Test F1 = 0.8750\n","\n","Time = 1.793 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","1.4564992384093784\n","Training Accuracy = 0.9973, time = 21.577 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.734587907791138\n","Test Accuracy = 0.8762, Test Precision = 0.8340, Test Recall = 0.9013, Test F1 = 0.8664\n","\n","Time = 1.804 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.8083371438369795\n","Training Accuracy = 0.9982, time = 21.614 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.137258648872375\n","Test Accuracy = 0.8723, Test Precision = 0.8354, Test Recall = 0.8879, Test F1 = 0.8609\n","\n","Time = 1.959 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","1.158751223305444\n","Training Accuracy = 0.9980, time = 21.647 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.905183613300323\n","Test Accuracy = 0.8743, Test Precision = 0.8478, Test Recall = 0.8744, Test F1 = 0.8609\n","\n","Time = 2.045 seconds\n","\n","Cross Validation step :  8\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","37.81643587350845\n","Training Accuracy = 0.7501, time = 24.845 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.7051288187503815\n","Test Accuracy = 0.8323, Test Precision = 0.8150, Test Recall = 0.8484, Test F1 = 0.8313\n","\n","Time = 1.918 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","27.014595955610275\n","Training Accuracy = 0.8388, time = 21.905 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.3519898056983948\n","Test Accuracy = 0.8323, Test Precision = 0.7963, Test Recall = 0.8811, Test F1 = 0.8366\n","\n","Time = 1.972 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","20.13596349954605\n","Training Accuracy = 0.8887, time = 21.404 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.6152991950511932\n","Test Accuracy = 0.8224, Test Precision = 0.7592, Test Recall = 0.9303, Test F1 = 0.8361\n","\n","Time = 1.929 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","15.010567426681519\n","Training Accuracy = 0.9184, time = 21.480 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.8917759507894516\n","Test Accuracy = 0.8483, Test Precision = 0.8158, Test Recall = 0.8893, Test F1 = 0.8510\n","\n","Time = 1.965 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","12.710038462653756\n","Training Accuracy = 0.9379, time = 21.644 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.268441528081894\n","Test Accuracy = 0.8024, Test Precision = 0.7287, Test Recall = 0.9467, Test F1 = 0.8235\n","\n","Time = 2.044 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","10.44520378485322\n","Training Accuracy = 0.9497, time = 21.576 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.183158054947853\n","Test Accuracy = 0.8443, Test Precision = 0.8097, Test Recall = 0.8893, Test F1 = 0.8477\n","\n","Time = 1.973 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","8.479123037308455\n","Training Accuracy = 0.9599, time = 21.502 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.732430994510651\n","Test Accuracy = 0.8623, Test Precision = 0.8661, Test Recall = 0.8484, Test F1 = 0.8571\n","\n","Time = 1.981 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","5.547127868514508\n","Training Accuracy = 0.9743, time = 21.527 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.822559356689453\n","Test Accuracy = 0.8603, Test Precision = 0.8750, Test Recall = 0.8320, Test F1 = 0.8529\n","\n","Time = 1.859 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","4.422095217276365\n","Training Accuracy = 0.9816, time = 21.563 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.982810914516449\n","Test Accuracy = 0.8563, Test Precision = 0.9057, Test Recall = 0.7869, Test F1 = 0.8421\n","\n","Time = 1.995 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","3.642343932762742\n","Training Accuracy = 0.9847, time = 21.592 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.218072265386581\n","Test Accuracy = 0.8603, Test Precision = 0.8655, Test Recall = 0.8443, Test F1 = 0.8548\n","\n","Time = 1.991 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","2.855322966584936\n","Training Accuracy = 0.9887, time = 21.610 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.237818777561188\n","Test Accuracy = 0.8463, Test Precision = 0.8436, Test Recall = 0.8402, Test F1 = 0.8419\n","\n","Time = 1.863 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","3.058810660499148\n","Training Accuracy = 0.9909, time = 21.565 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.811434924602509\n","Test Accuracy = 0.8603, Test Precision = 0.9143, Test Recall = 0.7869, Test F1 = 0.8458\n","\n","Time = 1.996 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","2.587036559358239\n","Training Accuracy = 0.9907, time = 21.571 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.309117496013641\n","Test Accuracy = 0.8583, Test Precision = 0.8560, Test Recall = 0.8525, Test F1 = 0.8542\n","\n","Time = 2.005 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","3.224292198079638\n","Training Accuracy = 0.9896, time = 21.561 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.207230240106583\n","Test Accuracy = 0.8703, Test Precision = 0.8978, Test Recall = 0.8279, Test F1 = 0.8614\n","\n","Time = 1.914 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","2.6902989521622658\n","Training Accuracy = 0.9902, time = 21.570 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.701643228530884\n","Test Accuracy = 0.8663, Test Precision = 0.8642, Test Recall = 0.8607, Test F1 = 0.8624\n","\n","Time = 2.000 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","2.214371112117078\n","Training Accuracy = 0.9947, time = 21.591 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.26130560040474\n","Test Accuracy = 0.8663, Test Precision = 0.8969, Test Recall = 0.8197, Test F1 = 0.8565\n","\n","Time = 2.028 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","1.6170756977517158\n","Training Accuracy = 0.9942, time = 21.541 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.91172057390213\n","Test Accuracy = 0.8643, Test Precision = 0.9037, Test Recall = 0.8074, Test F1 = 0.8528\n","\n","Time = 1.860 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.4397822873434052\n","Training Accuracy = 0.9958, time = 21.579 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.662351727485657\n","Test Accuracy = 0.8703, Test Precision = 0.8594, Test Recall = 0.8770, Test F1 = 0.8682\n","\n","Time = 1.947 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.146123311133124\n","Training Accuracy = 0.9967, time = 21.541 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.990505456924438\n","Test Accuracy = 0.8643, Test Precision = 0.8729, Test Recall = 0.8443, Test F1 = 0.8583\n","\n","Time = 2.011 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","1.7045918110379716\n","Training Accuracy = 0.9949, time = 21.510 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.363851130008698\n","Test Accuracy = 0.8623, Test Precision = 0.8431, Test Recall = 0.8811, Test F1 = 0.8617\n","\n","Time = 1.998 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","1.8168605661849142\n","Training Accuracy = 0.9951, time = 21.499 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.57839047908783\n","Test Accuracy = 0.8703, Test Precision = 0.8456, Test Recall = 0.8975, Test F1 = 0.8708\n","\n","Time = 2.042 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","2.3202861967438366\n","Training Accuracy = 0.9929, time = 21.498 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.11415809392929\n","Test Accuracy = 0.8683, Test Precision = 0.8678, Test Recall = 0.8607, Test F1 = 0.8642\n","\n","Time = 2.056 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","0.8798903892129601\n","Training Accuracy = 0.9978, time = 21.558 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.56748479604721\n","Test Accuracy = 0.8743, Test Precision = 0.8787, Test Recall = 0.8607, Test F1 = 0.8696\n","\n","Time = 2.045 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","0.7773982264716324\n","Training Accuracy = 0.9978, time = 21.565 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.048173874616623\n","Test Accuracy = 0.8703, Test Precision = 0.8714, Test Recall = 0.8607, Test F1 = 0.8660\n","\n","Time = 2.034 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","0.5711122438592611\n","Training Accuracy = 0.9982, time = 21.513 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.281409949064255\n","Test Accuracy = 0.8782, Test Precision = 0.8828, Test Recall = 0.8648, Test F1 = 0.8737\n","\n","Time = 1.944 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.7200989007651515\n","Training Accuracy = 0.9978, time = 21.489 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.275958895683289\n","Test Accuracy = 0.8683, Test Precision = 0.8648, Test Recall = 0.8648, Test F1 = 0.8648\n","\n","Time = 2.092 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","1.1128463526538326\n","Training Accuracy = 0.9969, time = 21.510 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.518483012914658\n","Test Accuracy = 0.8802, Test Precision = 0.9071, Test Recall = 0.8402, Test F1 = 0.8723\n","\n","Time = 2.019 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","0.6154423557256905\n","Training Accuracy = 0.9976, time = 21.518 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.257308959960938\n","Test Accuracy = 0.8723, Test Precision = 0.8782, Test Recall = 0.8566, Test F1 = 0.8672\n","\n","Time = 2.034 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.7360694724138739\n","Training Accuracy = 0.9982, time = 21.508 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.36039924621582\n","Test Accuracy = 0.8822, Test Precision = 0.9039, Test Recall = 0.8484, Test F1 = 0.8753\n","\n","Time = 1.902 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.6735677001806835\n","Training Accuracy = 0.9976, time = 21.483 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.144160449504852\n","Test Accuracy = 0.8703, Test Precision = 0.8653, Test Recall = 0.8689, Test F1 = 0.8671\n","\n","Time = 1.918 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","0.8265571477495541\n","Training Accuracy = 0.9982, time = 21.502 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.041164577007294\n","Test Accuracy = 0.8782, Test Precision = 0.8861, Test Recall = 0.8607, Test F1 = 0.8732\n","\n","Time = 2.083 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.7820253549361951\n","Training Accuracy = 0.9971, time = 21.479 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.342373430728912\n","Test Accuracy = 0.8762, Test Precision = 0.8760, Test Recall = 0.8689, Test F1 = 0.8724\n","\n","Time = 2.027 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.8198759734798386\n","Training Accuracy = 0.9987, time = 21.506 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.813247799873352\n","Test Accuracy = 0.8862, Test Precision = 0.9119, Test Recall = 0.8484, Test F1 = 0.8790\n","\n","Time = 2.035 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.6049723867872672\n","Training Accuracy = 0.9982, time = 21.488 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.691515684127808\n","Test Accuracy = 0.8762, Test Precision = 0.8824, Test Recall = 0.8607, Test F1 = 0.8714\n","\n","Time = 1.912 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.575651244985238\n","Training Accuracy = 0.9976, time = 21.506 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.205531895160675\n","Test Accuracy = 0.8723, Test Precision = 0.8629, Test Recall = 0.8770, Test F1 = 0.8699\n","\n","Time = 1.931 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","1.827192599153932\n","Training Accuracy = 0.9951, time = 21.530 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.938300430774689\n","Test Accuracy = 0.8603, Test Precision = 0.8480, Test Recall = 0.8689, Test F1 = 0.8583\n","\n","Time = 2.182 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","1.5118916656977035\n","Training Accuracy = 0.9967, time = 21.508 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.59279602766037\n","Test Accuracy = 0.8523, Test Precision = 0.8664, Test Recall = 0.8238, Test F1 = 0.8445\n","\n","Time = 2.069 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","1.428298582389175\n","Training Accuracy = 0.9962, time = 21.530 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.52823007106781\n","Test Accuracy = 0.8603, Test Precision = 0.8508, Test Recall = 0.8648, Test F1 = 0.8577\n","\n","Time = 2.044 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.7722107939284797\n","Training Accuracy = 0.9973, time = 21.524 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.639510959386826\n","Test Accuracy = 0.8563, Test Precision = 0.8440, Test Recall = 0.8648, Test F1 = 0.8543\n","\n","Time = 2.043 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.9360117741259728\n","Training Accuracy = 0.9973, time = 21.539 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.731553256511688\n","Test Accuracy = 0.8743, Test Precision = 0.8819, Test Recall = 0.8566, Test F1 = 0.8690\n","\n","Time = 2.007 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.3878336704025287\n","Training Accuracy = 0.9982, time = 21.521 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.419025152921677\n","Test Accuracy = 0.8703, Test Precision = 0.8745, Test Recall = 0.8566, Test F1 = 0.8654\n","\n","Time = 1.936 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.4488012965557573\n","Training Accuracy = 0.9984, time = 21.525 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.785810708999634\n","Test Accuracy = 0.8603, Test Precision = 0.8537, Test Recall = 0.8607, Test F1 = 0.8571\n","\n","Time = 2.115 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.2401495992882019\n","Training Accuracy = 0.9993, time = 21.519 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.233326017856598\n","Test Accuracy = 0.8743, Test Precision = 0.8787, Test Recall = 0.8607, Test F1 = 0.8696\n","\n","Time = 2.071 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.17936879029366537\n","Training Accuracy = 0.9996, time = 21.509 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.681585669517517\n","Test Accuracy = 0.8743, Test Precision = 0.8787, Test Recall = 0.8607, Test F1 = 0.8696\n","\n","Time = 2.049 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.18095955081662396\n","Training Accuracy = 0.9996, time = 21.499 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.08364462852478\n","Test Accuracy = 0.8743, Test Precision = 0.8787, Test Recall = 0.8607, Test F1 = 0.8696\n","\n","Time = 2.048 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","0.2351701272113189\n","Training Accuracy = 0.9991, time = 21.489 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.451843440532684\n","Test Accuracy = 0.8723, Test Precision = 0.8782, Test Recall = 0.8566, Test F1 = 0.8672\n","\n","Time = 2.079 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.6987326404176883\n","Training Accuracy = 0.9984, time = 21.474 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.332770228385925\n","Test Accuracy = 0.8743, Test Precision = 0.8884, Test Recall = 0.8484, Test F1 = 0.8679\n","\n","Time = 2.034 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.8892603128679184\n","Training Accuracy = 0.9980, time = 21.495 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.78666552901268\n","Test Accuracy = 0.8782, Test Precision = 0.9178, Test Recall = 0.8238, Test F1 = 0.8683\n","\n","Time = 2.119 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.5975034380196576\n","Training Accuracy = 0.9984, time = 21.481 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.217191725969315\n","Test Accuracy = 0.8683, Test Precision = 0.8560, Test Recall = 0.8770, Test F1 = 0.8664\n","\n","Time = 2.132 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","1.4459322869643074\n","Training Accuracy = 0.9962, time = 21.483 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.147888422012329\n","Test Accuracy = 0.8323, Test Precision = 0.7941, Test Recall = 0.8852, Test F1 = 0.8372\n","\n","Time = 2.024 seconds\n","\n","Cross Validation step :  9\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","37.30761221051216\n","Training Accuracy = 0.7665, time = 25.732 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.1863362193107605\n","Test Accuracy = 0.8523, Test Precision = 0.8439, Test Recall = 0.8764, Test F1 = 0.8598\n","\n","Time = 1.561 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","27.263242542743683\n","Training Accuracy = 0.8368, time = 22.755 seconds\n","\n","Running 501 samples:\n","Total loss:\n","2.96255524456501\n","Test Accuracy = 0.8583, Test Precision = 0.8507, Test Recall = 0.8803, Test F1 = 0.8653\n","\n","Time = 1.659 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","20.663290724158287\n","Training Accuracy = 0.8818, time = 22.204 seconds\n","\n","Running 501 samples:\n","Total loss:\n","2.7699306309223175\n","Test Accuracy = 0.8842, Test Precision = 0.8792, Test Recall = 0.8996, Test F1 = 0.8893\n","\n","Time = 1.697 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","14.900181524455547\n","Training Accuracy = 0.9204, time = 22.269 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.072441354393959\n","Test Accuracy = 0.8962, Test Precision = 0.9124, Test Recall = 0.8842, Test F1 = 0.8980\n","\n","Time = 1.635 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","11.449628300964832\n","Training Accuracy = 0.9430, time = 22.450 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.603177517652512\n","Test Accuracy = 0.8483, Test Precision = 0.7980, Test Recall = 0.9459, Test F1 = 0.8657\n","\n","Time = 1.618 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","9.752114780247211\n","Training Accuracy = 0.9519, time = 22.379 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.3055102229118347\n","Test Accuracy = 0.8962, Test Precision = 0.9059, Test Recall = 0.8919, Test F1 = 0.8988\n","\n","Time = 1.714 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","8.25097350589931\n","Training Accuracy = 0.9603, time = 22.296 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.373567849397659\n","Test Accuracy = 0.9022, Test Precision = 0.9526, Test Recall = 0.8533, Test F1 = 0.9002\n","\n","Time = 1.680 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","5.6110769668594\n","Training Accuracy = 0.9732, time = 22.323 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.310705989599228\n","Test Accuracy = 0.8962, Test Precision = 0.9331, Test Recall = 0.8610, Test F1 = 0.8956\n","\n","Time = 1.706 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","5.402354729827493\n","Training Accuracy = 0.9763, time = 22.402 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.465787887573242\n","Test Accuracy = 0.9082, Test Precision = 0.9049, Test Recall = 0.9189, Test F1 = 0.9119\n","\n","Time = 1.681 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","4.22105582151562\n","Training Accuracy = 0.9838, time = 22.396 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.46989169716835\n","Test Accuracy = 0.8842, Test Precision = 0.9589, Test Recall = 0.8108, Test F1 = 0.8787\n","\n","Time = 1.572 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","3.574736835435033\n","Training Accuracy = 0.9854, time = 22.367 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.5287675857543945\n","Test Accuracy = 0.8962, Test Precision = 0.9157, Test Recall = 0.8803, Test F1 = 0.8976\n","\n","Time = 1.712 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","2.9136353647336364\n","Training Accuracy = 0.9874, time = 22.304 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.699782073497772\n","Test Accuracy = 0.8962, Test Precision = 0.8935, Test Recall = 0.9073, Test F1 = 0.9004\n","\n","Time = 1.707 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","2.2185812508687377\n","Training Accuracy = 0.9918, time = 22.301 seconds\n","\n","Running 501 samples:\n","Total loss:\n","6.961471438407898\n","Test Accuracy = 0.8942, Test Precision = 0.9087, Test Recall = 0.8842, Test F1 = 0.8963\n","\n","Time = 1.574 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","2.160920581431128\n","Training Accuracy = 0.9927, time = 22.356 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.250924915075302\n","Test Accuracy = 0.8942, Test Precision = 0.9440, Test Recall = 0.8456, Test F1 = 0.8921\n","\n","Time = 1.619 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","2.1794716593576595\n","Training Accuracy = 0.9918, time = 22.371 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.187605202198029\n","Test Accuracy = 0.8902, Test Precision = 0.9286, Test Recall = 0.8533, Test F1 = 0.8893\n","\n","Time = 1.591 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","3.3023847092408687\n","Training Accuracy = 0.9898, time = 22.315 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.518451869487762\n","Test Accuracy = 0.8922, Test Precision = 0.8868, Test Recall = 0.9073, Test F1 = 0.8969\n","\n","Time = 1.763 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","2.437032447254751\n","Training Accuracy = 0.9925, time = 22.264 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.415440618991852\n","Test Accuracy = 0.9002, Test Precision = 0.9300, Test Recall = 0.8726, Test F1 = 0.9004\n","\n","Time = 1.778 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.4523338438011706\n","Training Accuracy = 0.9956, time = 22.305 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.029990524053574\n","Test Accuracy = 0.8922, Test Precision = 0.9116, Test Recall = 0.8764, Test F1 = 0.8937\n","\n","Time = 1.736 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.153938223636942\n","Training Accuracy = 0.9962, time = 22.335 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.435108929872513\n","Test Accuracy = 0.8962, Test Precision = 0.9224, Test Recall = 0.8726, Test F1 = 0.8968\n","\n","Time = 1.722 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","1.0520832257752772\n","Training Accuracy = 0.9958, time = 22.355 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.532157063484192\n","Test Accuracy = 0.8982, Test Precision = 0.9031, Test Recall = 0.8996, Test F1 = 0.9014\n","\n","Time = 1.719 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","1.1956590481422609\n","Training Accuracy = 0.9960, time = 22.322 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.649258494377136\n","Test Accuracy = 0.8942, Test Precision = 0.8872, Test Recall = 0.9112, Test F1 = 0.8990\n","\n","Time = 1.739 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","2.126028431870509\n","Training Accuracy = 0.9947, time = 22.324 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.144269704818726\n","Test Accuracy = 0.8862, Test Precision = 0.8855, Test Recall = 0.8958, Test F1 = 0.8906\n","\n","Time = 1.588 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","1.2080298518267227\n","Training Accuracy = 0.9958, time = 22.347 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.83319103717804\n","Test Accuracy = 0.8842, Test Precision = 0.8911, Test Recall = 0.8842, Test F1 = 0.8876\n","\n","Time = 1.593 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","1.1074553282160196\n","Training Accuracy = 0.9967, time = 22.334 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.929748594760895\n","Test Accuracy = 0.8902, Test Precision = 0.9113, Test Recall = 0.8726, Test F1 = 0.8915\n","\n","Time = 1.832 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","0.8736436787003186\n","Training Accuracy = 0.9971, time = 22.317 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.707411289215088\n","Test Accuracy = 0.8962, Test Precision = 0.9157, Test Recall = 0.8803, Test F1 = 0.8976\n","\n","Time = 1.722 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.9969147949086619\n","Training Accuracy = 0.9971, time = 22.298 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.812904417514801\n","Test Accuracy = 0.8942, Test Precision = 0.9364, Test Recall = 0.8533, Test F1 = 0.8929\n","\n","Time = 1.711 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","0.651602456378896\n","Training Accuracy = 0.9978, time = 22.289 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.500250160694122\n","Test Accuracy = 0.8882, Test Precision = 0.9177, Test Recall = 0.8610, Test F1 = 0.8884\n","\n","Time = 1.598 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","1.3289490600145655\n","Training Accuracy = 0.9971, time = 22.318 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.374089568853378\n","Test Accuracy = 0.8902, Test Precision = 0.9474, Test Recall = 0.8340, Test F1 = 0.8871\n","\n","Time = 1.738 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","2.8407753075662185\n","Training Accuracy = 0.9933, time = 22.348 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.58049875497818\n","Test Accuracy = 0.8882, Test Precision = 0.9432, Test Recall = 0.8340, Test F1 = 0.8852\n","\n","Time = 1.754 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","1.363214971133857\n","Training Accuracy = 0.9960, time = 22.361 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.789687037467957\n","Test Accuracy = 0.9022, Test Precision = 0.9375, Test Recall = 0.8687, Test F1 = 0.9018\n","\n","Time = 1.601 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","1.001089499302907\n","Training Accuracy = 0.9978, time = 22.312 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.553536295890808\n","Test Accuracy = 0.9002, Test Precision = 0.9372, Test Recall = 0.8649, Test F1 = 0.8996\n","\n","Time = 1.738 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.9131620288608246\n","Training Accuracy = 0.9971, time = 22.270 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.273607790470123\n","Test Accuracy = 0.8862, Test Precision = 0.8855, Test Recall = 0.8958, Test F1 = 0.8906\n","\n","Time = 1.720 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.8394411859335378\n","Training Accuracy = 0.9969, time = 22.248 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.770585387945175\n","Test Accuracy = 0.8902, Test Precision = 0.9146, Test Recall = 0.8687, Test F1 = 0.8911\n","\n","Time = 1.581 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.9550824783509597\n","Training Accuracy = 0.9976, time = 22.245 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.718074381351471\n","Test Accuracy = 0.8882, Test Precision = 0.9109, Test Recall = 0.8687, Test F1 = 0.8893\n","\n","Time = 1.771 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.7291193474811735\n","Training Accuracy = 0.9980, time = 22.325 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.676386922597885\n","Test Accuracy = 0.8822, Test Precision = 0.8968, Test Recall = 0.8726, Test F1 = 0.8845\n","\n","Time = 1.739 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.8295426962868078\n","Training Accuracy = 0.9976, time = 22.351 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.984828025102615\n","Test Accuracy = 0.8922, Test Precision = 0.9150, Test Recall = 0.8726, Test F1 = 0.8933\n","\n","Time = 1.588 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.6128481974051283\n","Training Accuracy = 0.9980, time = 22.298 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.226380407810211\n","Test Accuracy = 0.8882, Test Precision = 0.8830, Test Recall = 0.9035, Test F1 = 0.8931\n","\n","Time = 1.783 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.44246842444772483\n","Training Accuracy = 0.9980, time = 22.291 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.5069979429245\n","Test Accuracy = 0.8902, Test Precision = 0.8953, Test Recall = 0.8919, Test F1 = 0.8936\n","\n","Time = 1.749 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.27037473936252354\n","Training Accuracy = 0.9991, time = 22.278 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.912277460098267\n","Test Accuracy = 0.8882, Test Precision = 0.8919, Test Recall = 0.8919, Test F1 = 0.8919\n","\n","Time = 1.698 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.2700451314703969\n","Training Accuracy = 0.9991, time = 22.287 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.699523270130157\n","Test Accuracy = 0.8922, Test Precision = 0.8988, Test Recall = 0.8919, Test F1 = 0.8953\n","\n","Time = 1.764 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.3392968465334434\n","Training Accuracy = 0.9989, time = 22.303 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.602574646472931\n","Test Accuracy = 0.8922, Test Precision = 0.9253, Test Recall = 0.8610, Test F1 = 0.8920\n","\n","Time = 1.706 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.48127314359589946\n","Training Accuracy = 0.9984, time = 22.291 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.143621861934662\n","Test Accuracy = 0.8922, Test Precision = 0.9020, Test Recall = 0.8880, Test F1 = 0.8949\n","\n","Time = 1.599 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.48177797050630033\n","Training Accuracy = 0.9987, time = 22.294 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.279039740562439\n","Test Accuracy = 0.8962, Test Precision = 0.9027, Test Recall = 0.8958, Test F1 = 0.8992\n","\n","Time = 1.748 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.5477264979949723\n","Training Accuracy = 0.9987, time = 22.305 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.9528306722641\n","Test Accuracy = 0.8982, Test Precision = 0.8910, Test Recall = 0.9151, Test F1 = 0.9029\n","\n","Time = 1.738 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.4702023647778333\n","Training Accuracy = 0.9982, time = 22.254 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.520312905311584\n","Test Accuracy = 0.8802, Test Precision = 0.9270, Test Recall = 0.8340, Test F1 = 0.8780\n","\n","Time = 1.599 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","0.4272465265752885\n","Training Accuracy = 0.9993, time = 22.259 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.43869149684906\n","Test Accuracy = 0.8842, Test Precision = 0.9136, Test Recall = 0.8571, Test F1 = 0.8845\n","\n","Time = 1.766 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.6492987536712462\n","Training Accuracy = 0.9984, time = 22.266 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.648687779903412\n","Test Accuracy = 0.8982, Test Precision = 0.9262, Test Recall = 0.8726, Test F1 = 0.8986\n","\n","Time = 1.727 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.2785034868470575\n","Training Accuracy = 0.9993, time = 22.265 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.875894784927368\n","Test Accuracy = 0.8842, Test Precision = 0.9313, Test Recall = 0.8378, Test F1 = 0.8821\n","\n","Time = 1.809 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.37541585322446736\n","Training Accuracy = 0.9989, time = 22.284 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.92774885892868\n","Test Accuracy = 0.8822, Test Precision = 0.9348, Test Recall = 0.8301, Test F1 = 0.8793\n","\n","Time = 1.756 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.20320679734987834\n","Training Accuracy = 0.9984, time = 22.271 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.981628060340881\n","Test Accuracy = 0.8922, Test Precision = 0.9020, Test Recall = 0.8880, Test F1 = 0.8949\n","\n","Time = 1.743 seconds\n","\n","Cross Validation step :  10\n","word2vec shape:  (18415, 300)\n","Epoch: 1 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","36.56440609693527\n","Training Accuracy = 0.7669, time = 25.363 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.4638240337371826\n","Test Accuracy = 0.8263, Test Precision = 0.8894, Test Recall = 0.7643, Test F1 = 0.8221\n","\n","Time = 1.775 seconds\n","\n","Epoch: 2 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","25.81674474477768\n","Training Accuracy = 0.8403, time = 22.462 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.792414426803589\n","Test Accuracy = 0.8563, Test Precision = 0.9283, Test Recall = 0.7871, Test F1 = 0.8519\n","\n","Time = 1.700 seconds\n","\n","Epoch: 3 Learning rate: 0.00100\n","Running 4509 samples:\n","Total loss:\n","20.57485955953598\n","Training Accuracy = 0.8811, time = 21.966 seconds\n","\n","Running 501 samples:\n","Total loss:\n","3.3886633813381195\n","Test Accuracy = 0.8683, Test Precision = 0.9053, Test Recall = 0.8365, Test F1 = 0.8696\n","\n","Time = 1.602 seconds\n","\n","Epoch: 4 Learning rate: 0.00095\n","Running 4509 samples:\n","Total loss:\n","14.80249011516571\n","Training Accuracy = 0.9182, time = 22.031 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.282086253166199\n","Test Accuracy = 0.8503, Test Precision = 0.8197, Test Recall = 0.9163, Test F1 = 0.8654\n","\n","Time = 1.734 seconds\n","\n","Epoch: 5 Learning rate: 0.00090\n","Running 4509 samples:\n","Total loss:\n","11.76204902306199\n","Training Accuracy = 0.9399, time = 22.223 seconds\n","\n","Running 501 samples:\n","Total loss:\n","4.476555317640305\n","Test Accuracy = 0.8623, Test Precision = 0.8514, Test Recall = 0.8935, Test F1 = 0.8720\n","\n","Time = 1.717 seconds\n","\n","Epoch: 6 Learning rate: 0.00086\n","Running 4509 samples:\n","Total loss:\n","9.161944434046745\n","Training Accuracy = 0.9519, time = 22.133 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.205612987279892\n","Test Accuracy = 0.8822, Test Precision = 0.8923, Test Recall = 0.8821, Test F1 = 0.8872\n","\n","Time = 1.728 seconds\n","\n","Epoch: 7 Learning rate: 0.00081\n","Running 4509 samples:\n","Total loss:\n","7.0749766025692225\n","Training Accuracy = 0.9678, time = 22.070 seconds\n","\n","Running 501 samples:\n","Total loss:\n","5.895560026168823\n","Test Accuracy = 0.8703, Test Precision = 0.8511, Test Recall = 0.9125, Test F1 = 0.8807\n","\n","Time = 1.652 seconds\n","\n","Epoch: 8 Learning rate: 0.00077\n","Running 4509 samples:\n","Total loss:\n","6.381415594369173\n","Training Accuracy = 0.9729, time = 22.105 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.4117768704891205\n","Test Accuracy = 0.8743, Test Precision = 0.9167, Test Recall = 0.8365, Test F1 = 0.8748\n","\n","Time = 1.770 seconds\n","\n","Epoch: 9 Learning rate: 0.00074\n","Running 4509 samples:\n","Total loss:\n","4.680741417221725\n","Training Accuracy = 0.9792, time = 22.120 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.23502430319786\n","Test Accuracy = 0.8802, Test Precision = 0.9552, Test Recall = 0.8099, Test F1 = 0.8765\n","\n","Time = 1.791 seconds\n","\n","Epoch: 10 Learning rate: 0.00070\n","Running 4509 samples:\n","Total loss:\n","4.67576318513602\n","Training Accuracy = 0.9834, time = 22.117 seconds\n","\n","Running 501 samples:\n","Total loss:\n","9.229897797107697\n","Test Accuracy = 0.8723, Test Precision = 0.9383, Test Recall = 0.8099, Test F1 = 0.8694\n","\n","Time = 1.771 seconds\n","\n","Epoch: 11 Learning rate: 0.00066\n","Running 4509 samples:\n","Total loss:\n","4.026256514247507\n","Training Accuracy = 0.9829, time = 22.144 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.302941411733627\n","Test Accuracy = 0.8603, Test Precision = 0.9004, Test Recall = 0.8251, Test F1 = 0.8611\n","\n","Time = 1.789 seconds\n","\n","Epoch: 12 Learning rate: 0.00063\n","Running 4509 samples:\n","Total loss:\n","2.5720537996385247\n","Training Accuracy = 0.9894, time = 22.150 seconds\n","\n","Running 501 samples:\n","Total loss:\n","7.922692537307739\n","Test Accuracy = 0.8703, Test Precision = 0.8960, Test Recall = 0.8517, Test F1 = 0.8733\n","\n","Time = 1.669 seconds\n","\n","Epoch: 13 Learning rate: 0.00060\n","Running 4509 samples:\n","Total loss:\n","2.6943122800439596\n","Training Accuracy = 0.9911, time = 22.148 seconds\n","\n","Running 501 samples:\n","Total loss:\n","8.000961422920227\n","Test Accuracy = 0.8703, Test Precision = 0.8779, Test Recall = 0.8745, Test F1 = 0.8762\n","\n","Time = 1.828 seconds\n","\n","Epoch: 14 Learning rate: 0.00057\n","Running 4509 samples:\n","Total loss:\n","2.65481254295446\n","Training Accuracy = 0.9914, time = 22.146 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.017890989780426\n","Test Accuracy = 0.8563, Test Precision = 0.8835, Test Recall = 0.8365, Test F1 = 0.8594\n","\n","Time = 1.811 seconds\n","\n","Epoch: 15 Learning rate: 0.00054\n","Running 4509 samples:\n","Total loss:\n","1.9339054201263934\n","Training Accuracy = 0.9929, time = 22.079 seconds\n","\n","Running 501 samples:\n","Total loss:\n","10.359914362430573\n","Test Accuracy = 0.8703, Test Precision = 0.8722, Test Recall = 0.8821, Test F1 = 0.8771\n","\n","Time = 1.789 seconds\n","\n","Epoch: 16 Learning rate: 0.00051\n","Running 4509 samples:\n","Total loss:\n","1.3618400659179315\n","Training Accuracy = 0.9958, time = 22.063 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.826347768306732\n","Test Accuracy = 0.8703, Test Precision = 0.9267, Test Recall = 0.8175, Test F1 = 0.8687\n","\n","Time = 1.792 seconds\n","\n","Epoch: 17 Learning rate: 0.00049\n","Running 4509 samples:\n","Total loss:\n","1.2335108078550547\n","Training Accuracy = 0.9951, time = 22.104 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.847590744495392\n","Test Accuracy = 0.8782, Test Precision = 0.9139, Test Recall = 0.8479, Test F1 = 0.8797\n","\n","Time = 1.833 seconds\n","\n","Epoch: 18 Learning rate: 0.00046\n","Running 4509 samples:\n","Total loss:\n","1.209052745020017\n","Training Accuracy = 0.9953, time = 22.126 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.843514204025269\n","Test Accuracy = 0.8762, Test Precision = 0.9389, Test Recall = 0.8175, Test F1 = 0.8740\n","\n","Time = 1.803 seconds\n","\n","Epoch: 19 Learning rate: 0.00044\n","Running 4509 samples:\n","Total loss:\n","1.4392932318733074\n","Training Accuracy = 0.9967, time = 22.088 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.741744697093964\n","Test Accuracy = 0.8743, Test Precision = 0.9000, Test Recall = 0.8555, Test F1 = 0.8772\n","\n","Time = 1.809 seconds\n","\n","Epoch: 20 Learning rate: 0.00042\n","Running 4509 samples:\n","Total loss:\n","1.0480095839884598\n","Training Accuracy = 0.9967, time = 22.065 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.742592334747314\n","Test Accuracy = 0.8663, Test Precision = 0.8798, Test Recall = 0.8631, Test F1 = 0.8714\n","\n","Time = 1.786 seconds\n","\n","Epoch: 21 Learning rate: 0.00040\n","Running 4509 samples:\n","Total loss:\n","2.3621772000333294\n","Training Accuracy = 0.9927, time = 22.073 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.131741046905518\n","Test Accuracy = 0.8683, Test Precision = 0.9301, Test Recall = 0.8099, Test F1 = 0.8659\n","\n","Time = 1.837 seconds\n","\n","Epoch: 22 Learning rate: 0.00038\n","Running 4509 samples:\n","Total loss:\n","1.7209626253752504\n","Training Accuracy = 0.9945, time = 22.088 seconds\n","\n","Running 501 samples:\n","Total loss:\n","11.852462232112885\n","Test Accuracy = 0.8603, Test Precision = 0.8697, Test Recall = 0.8631, Test F1 = 0.8664\n","\n","Time = 1.792 seconds\n","\n","Epoch: 23 Learning rate: 0.00036\n","Running 4509 samples:\n","Total loss:\n","0.8043115898908582\n","Training Accuracy = 0.9984, time = 22.078 seconds\n","\n","Running 501 samples:\n","Total loss:\n","12.694003522396088\n","Test Accuracy = 0.8603, Test Precision = 0.8845, Test Recall = 0.8441, Test F1 = 0.8638\n","\n","Time = 1.796 seconds\n","\n","Epoch: 24 Learning rate: 0.00034\n","Running 4509 samples:\n","Total loss:\n","1.111672322047525\n","Training Accuracy = 0.9971, time = 22.074 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.812245547771454\n","Test Accuracy = 0.8723, Test Precision = 0.8812, Test Recall = 0.8745, Test F1 = 0.8779\n","\n","Time = 1.831 seconds\n","\n","Epoch: 25 Learning rate: 0.00032\n","Running 4509 samples:\n","Total loss:\n","1.192010200000368\n","Training Accuracy = 0.9967, time = 22.029 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.958085358142853\n","Test Accuracy = 0.8762, Test Precision = 0.8972, Test Recall = 0.8631, Test F1 = 0.8798\n","\n","Time = 1.761 seconds\n","\n","Epoch: 26 Learning rate: 0.00031\n","Running 4509 samples:\n","Total loss:\n","0.9976049066572159\n","Training Accuracy = 0.9969, time = 21.991 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.69119182229042\n","Test Accuracy = 0.8762, Test Precision = 0.8655, Test Recall = 0.9049, Test F1 = 0.8848\n","\n","Time = 1.866 seconds\n","\n","Epoch: 27 Learning rate: 0.00029\n","Running 4509 samples:\n","Total loss:\n","1.0008471962974\n","Training Accuracy = 0.9969, time = 22.075 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.855731308460236\n","Test Accuracy = 0.8563, Test Precision = 0.9030, Test Recall = 0.8137, Test F1 = 0.8560\n","\n","Time = 1.806 seconds\n","\n","Epoch: 28 Learning rate: 0.00028\n","Running 4509 samples:\n","Total loss:\n","2.1829265477317676\n","Training Accuracy = 0.9947, time = 22.118 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.420385956764221\n","Test Accuracy = 0.8623, Test Precision = 0.8674, Test Recall = 0.8707, Test F1 = 0.8691\n","\n","Time = 1.765 seconds\n","\n","Epoch: 29 Learning rate: 0.00026\n","Running 4509 samples:\n","Total loss:\n","0.8268333156884182\n","Training Accuracy = 0.9982, time = 22.129 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.958525836467743\n","Test Accuracy = 0.8663, Test Precision = 0.8984, Test Recall = 0.8403, Test F1 = 0.8684\n","\n","Time = 1.767 seconds\n","\n","Epoch: 30 Learning rate: 0.00025\n","Running 4509 samples:\n","Total loss:\n","0.9863045253005112\n","Training Accuracy = 0.9971, time = 22.078 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.932154417037964\n","Test Accuracy = 0.8663, Test Precision = 0.8952, Test Recall = 0.8441, Test F1 = 0.8689\n","\n","Time = 1.753 seconds\n","\n","Epoch: 31 Learning rate: 0.00024\n","Running 4509 samples:\n","Total loss:\n","0.8848093358828919\n","Training Accuracy = 0.9973, time = 22.060 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.468794405460358\n","Test Accuracy = 0.8703, Test Precision = 0.9091, Test Recall = 0.8365, Test F1 = 0.8713\n","\n","Time = 1.643 seconds\n","\n","Epoch: 32 Learning rate: 0.00023\n","Running 4509 samples:\n","Total loss:\n","0.810946081939619\n","Training Accuracy = 0.9976, time = 22.058 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.953086256980896\n","Test Accuracy = 0.8683, Test Precision = 0.8863, Test Recall = 0.8593, Test F1 = 0.8726\n","\n","Time = 1.654 seconds\n","\n","Epoch: 33 Learning rate: 0.00021\n","Running 4509 samples:\n","Total loss:\n","0.8433884660335025\n","Training Accuracy = 0.9980, time = 22.057 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.006670475006104\n","Test Accuracy = 0.8723, Test Precision = 0.9234, Test Recall = 0.8251, Test F1 = 0.8715\n","\n","Time = 1.689 seconds\n","\n","Epoch: 34 Learning rate: 0.00020\n","Running 4509 samples:\n","Total loss:\n","0.6352143781841733\n","Training Accuracy = 0.9978, time = 22.073 seconds\n","\n","Running 501 samples:\n","Total loss:\n","14.853957772254944\n","Test Accuracy = 0.8683, Test Precision = 0.8924, Test Recall = 0.8517, Test F1 = 0.8716\n","\n","Time = 1.726 seconds\n","\n","Epoch: 35 Learning rate: 0.00019\n","Running 4509 samples:\n","Total loss:\n","0.48117830959381536\n","Training Accuracy = 0.9982, time = 22.095 seconds\n","\n","Running 501 samples:\n","Total loss:\n","13.928229749202728\n","Test Accuracy = 0.8762, Test Precision = 0.8792, Test Recall = 0.8859, Test F1 = 0.8826\n","\n","Time = 1.840 seconds\n","\n","Epoch: 36 Learning rate: 0.00018\n","Running 4509 samples:\n","Total loss:\n","0.5646513469037018\n","Training Accuracy = 0.9978, time = 22.066 seconds\n","\n","Running 501 samples:\n","Total loss:\n","15.110512733459473\n","Test Accuracy = 0.8782, Test Precision = 0.8945, Test Recall = 0.8707, Test F1 = 0.8825\n","\n","Time = 1.804 seconds\n","\n","Epoch: 37 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.33870049689539883\n","Training Accuracy = 0.9987, time = 22.069 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.50936645269394\n","Test Accuracy = 0.8743, Test Precision = 0.8968, Test Recall = 0.8593, Test F1 = 0.8777\n","\n","Time = 1.827 seconds\n","\n","Epoch: 38 Learning rate: 0.00017\n","Running 4509 samples:\n","Total loss:\n","0.9578819012303939\n","Training Accuracy = 0.9980, time = 22.066 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.01306927204132\n","Test Accuracy = 0.8723, Test Precision = 0.9061, Test Recall = 0.8441, Test F1 = 0.8740\n","\n","Time = 1.810 seconds\n","\n","Epoch: 39 Learning rate: 0.00016\n","Running 4509 samples:\n","Total loss:\n","0.7805016106776748\n","Training Accuracy = 0.9973, time = 22.087 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.563642740249634\n","Test Accuracy = 0.8603, Test Precision = 0.9289, Test Recall = 0.7947, Test F1 = 0.8566\n","\n","Time = 1.835 seconds\n","\n","Epoch: 40 Learning rate: 0.00015\n","Running 4509 samples:\n","Total loss:\n","0.48374392653568066\n","Training Accuracy = 0.9978, time = 22.082 seconds\n","\n","Running 501 samples:\n","Total loss:\n","16.753610014915466\n","Test Accuracy = 0.8723, Test Precision = 0.8964, Test Recall = 0.8555, Test F1 = 0.8755\n","\n","Time = 1.827 seconds\n","\n","Epoch: 41 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.38014196152471413\n","Training Accuracy = 0.9984, time = 22.063 seconds\n","\n","Running 501 samples:\n","Total loss:\n","17.539335191249847\n","Test Accuracy = 0.8683, Test Precision = 0.8988, Test Recall = 0.8441, Test F1 = 0.8706\n","\n","Time = 1.794 seconds\n","\n","Epoch: 42 Learning rate: 0.00014\n","Running 4509 samples:\n","Total loss:\n","0.5216864392677962\n","Training Accuracy = 0.9987, time = 22.047 seconds\n","\n","Running 501 samples:\n","Total loss:\n","17.50163835287094\n","Test Accuracy = 0.8743, Test Precision = 0.9098, Test Recall = 0.8441, Test F1 = 0.8757\n","\n","Time = 1.814 seconds\n","\n","Epoch: 43 Learning rate: 0.00013\n","Running 4509 samples:\n","Total loss:\n","0.3304364749335491\n","Training Accuracy = 0.9987, time = 22.062 seconds\n","\n","Running 501 samples:\n","Total loss:\n","17.665404617786407\n","Test Accuracy = 0.8703, Test Precision = 0.8837, Test Recall = 0.8669, Test F1 = 0.8752\n","\n","Time = 1.827 seconds\n","\n","Epoch: 44 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.3071216693133465\n","Training Accuracy = 0.9993, time = 22.073 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.73935353755951\n","Test Accuracy = 0.8703, Test Precision = 0.9057, Test Recall = 0.8403, Test F1 = 0.8718\n","\n","Time = 1.812 seconds\n","\n","Epoch: 45 Learning rate: 0.00012\n","Running 4509 samples:\n","Total loss:\n","0.30082179553028254\n","Training Accuracy = 0.9989, time = 22.042 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.8463796377182\n","Test Accuracy = 0.8623, Test Precision = 0.9042, Test Recall = 0.8251, Test F1 = 0.8628\n","\n","Time = 1.828 seconds\n","\n","Epoch: 46 Learning rate: 0.00011\n","Running 4509 samples:\n","Total loss:\n","0.21739741917917854\n","Training Accuracy = 0.9991, time = 22.054 seconds\n","\n","Running 501 samples:\n","Total loss:\n","21.894718289375305\n","Test Accuracy = 0.8743, Test Precision = 0.9167, Test Recall = 0.8365, Test F1 = 0.8748\n","\n","Time = 1.664 seconds\n","\n","Epoch: 47 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.21263035737320024\n","Training Accuracy = 0.9996, time = 22.026 seconds\n","\n","Running 501 samples:\n","Total loss:\n","23.170735597610474\n","Test Accuracy = 0.8703, Test Precision = 0.9267, Test Recall = 0.8175, Test F1 = 0.8687\n","\n","Time = 1.816 seconds\n","\n","Epoch: 48 Learning rate: 0.00010\n","Running 4509 samples:\n","Total loss:\n","0.43842581939588854\n","Training Accuracy = 0.9989, time = 21.978 seconds\n","\n","Running 501 samples:\n","Total loss:\n","25.834869027137756\n","Test Accuracy = 0.8723, Test Precision = 0.9129, Test Recall = 0.8365, Test F1 = 0.8730\n","\n","Time = 1.791 seconds\n","\n","Epoch: 49 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.5052470367099886\n","Training Accuracy = 0.9987, time = 21.998 seconds\n","\n","Running 501 samples:\n","Total loss:\n","18.791913509368896\n","Test Accuracy = 0.8862, Test Precision = 0.9087, Test Recall = 0.8707, Test F1 = 0.8893\n","\n","Time = 1.704 seconds\n","\n","Epoch: 50 Learning rate: 0.00009\n","Running 4509 samples:\n","Total loss:\n","0.7983195688036631\n","Training Accuracy = 0.9980, time = 22.012 seconds\n","\n","Running 501 samples:\n","Total loss:\n","19.613415360450745\n","Test Accuracy = 0.8842, Test Precision = 0.9362, Test Recall = 0.8365, Test F1 = 0.8835\n","\n","Time = 1.720 seconds\n","\n","final Accuracy :  0.8698203592814371\n","final precision :  0.8681254406768797\n","final recall :  0.8702730923694779\n","final f1 :  0.8691979398976366\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H7toABPhbeV5","colab_type":"text"},"source":["### Maximum accuracy fasttext without removing puncuations **85.66**\n","\n","Results from fast text after removing puncuations\n","final Accuracy :  0.8738522954091816\n","final precision :  0.9021142508639968\n","final recall :  0.850095785440613\n","final f1 :  0.8753328730644049"]},{"cell_type":"markdown","metadata":{"id":"LoYNLo9pOZBk","colab_type":"text"},"source":["# Cross Validation"]},{"cell_type":"code","metadata":{"id":"ccCXP2m9NJwf","colab_type":"code","colab":{}},"source":["def start_test_epoches(config, session,classifier, train_dataset, test_dataset):\n","    #record max\n","    #max_val_acc=-1\n","    #max_test_acc=-1\n"," \n","    all_actual = []\n","    all_predictions = []\n","    for i in range(config.max_max_epoch):\n","        actual, prediction = test_model(config, i, session, classifier, train_dataset, test_dataset)\n","        all_actual.extend(actual)\n","        all_predictions.extend(prediction)\n","\n","\n","    TN, FP, FN, TP = confusion_matrix(all_actual, all_predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","    accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","    print(\"final Accuracy : \",accuracy)\n","    print(\"final precision : \",precision)\n","    print(\"final recall : \",recall)\n","    print(\"final f1 : \",f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqeU8wHGNbeR","colab_type":"code","colab":{}},"source":["def test_model(config, i, session, model, train_dataset,test_dataset):\n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","\n","    #testing\n","    start_time = time.time()\n","    test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    #return valid_acc, test_acc\n","\n","    return test_actual,test_predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bvR6QttXOXAN","colab":{}},"source":["tf.reset_default_graph()\n","\n","with tf.Graph().as_default(), tf.Session() as session:\n","  #initializer = tf.random_normal_initializer(0, 0.05)\n","\n","  classifier= Classifer(config=config, session=session)\n","  saver = tf.train.Saver()\n","  \n","  init = tf.global_variables_initializer()\n","  session.run(init)\n","\n","  saver = tf.train.Saver()\n","\n","  tf.train.Saver().restore(session,tf.train.latest_checkpoint(\"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/100epochs/\") )\n","  print(\"Model restored.\")\n","\n","  word_to_vec(matrix, session,config, classifier)\n","  start_test_epoches(config, session,classifier, train_dataset, test_dataset)\n"],"execution_count":0,"outputs":[]}]}