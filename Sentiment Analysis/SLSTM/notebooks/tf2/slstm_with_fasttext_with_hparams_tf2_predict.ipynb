{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"slstm_with_fasttext_with_hparams_tf2_predict.ipynb","provenance":[],"collapsed_sections":["-J-6HAXmY78b","Sn9fadSHZExx","1zSRrIPzZQXy","RQ5Kb9rnacLJ","g4-cYBMrah8L","vO5CPE1Tapnb","Rc0JOQkaa1V2","klCxrycBbGhx"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"tDHeO3U1UBbV","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6FJm8-eWiiI","colab_type":"code","outputId":"6d3bcc6f-a55a-45b2-8800-c41af408da21","executionInfo":{"status":"ok","timestamp":1591163735654,"user_tz":-330,"elapsed":37707,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"epyqre6kr9EK","colab_type":"code","outputId":"97a9747a-a765-4502-a7e2-33f1fef560f7","executionInfo":{"status":"ok","timestamp":1591164782841,"user_tz":-330,"elapsed":1499,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-J-6HAXmY78b","colab_type":"text"},"source":["# Automated Conversion of *.py file"]},{"cell_type":"code","metadata":{"id":"3_-EkhuLTi_z","colab_type":"code","colab":{}},"source":["# # -*- coding: utf-8 -*-\n","# \"\"\"slstm_with_fasttext_with_skopt.ipynb\n","\n","# Automatically generated by Colaboratory.\n","\n","# Original file is located at\n","#     https://colab.research.google.com/drive/1idQaa6eyIsPYYY2jbxVvAd7KP7Um_e8v\n","# \"\"\"\n","\n","# # %tensorflow_version 2.x\n","\n","# # import tensorflow.compat.v1 as tf\n","# # tf.disable_v2_behavior()\n","\n","# # from google.colab import drive\n","# # drive.mount('/content/drive')\n","\n","# path='/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/SLSTM/parsed_data/from_fasttext/data_set'\n","# vector_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/SLSTM/parsed_data/from_fasttext/fasttext_vectors'\n","# # run from lahiru1st@gmail.com\n","# # path='/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/data_set'\n","# # vector_path = '/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/fasttext_vectors'\n","\n","# \"\"\"# Imports\n","\n","# ## Standard Imports\n","# \"\"\"\n","\n","# from __future__ import print_function\n","# # import six.moves.cPickle as pickle\n","# # from collections import OrderedDict\n","# import sys\n","# import time\n","# import numpy as np\n","# import tensorflow as tf\n","# # import read_data\n","# # from random import shuffle\n","# # import random\n","# import pickle\n","# # import math\n","# from keras.preprocessing.sequence import pad_sequences\n","# from sklearn.metrics import confusion_matrix\n","# # from general_utils import Progbar\n","# # import tensorflow.contrib.slim as slim\n","# # from sst_config import Config\n","\n","# \"\"\"## config\"\"\"\n","\n","# class Config(object):\n","#     vocab_size=15000\n","#     max_grad_norm = 5\n","#     init_scale = 0.05\n","#     hidden_size = 300\n","#     lr_decay = 0.95\n","#     valid_portion=0.0\n","#     batch_size=5\n","#     keep_prob = 0.5\n","#     #0.05\n","#     learning_rate = 0.002\n","#     max_epoch =2\n","#     # max_max_epoch =40\n","#     max_max_epoch = 30\n","#     num_label=5\n","#     attention_iteration=3\n","#     random_initialize=False\n","#     embedding_trainable=True\n","#     l2_beta=0.0\n","\n","# \"\"\"## Progbar\"\"\"\n","\n","# import time\n","# import sys\n","# import logging\n","# import numpy as np\n","\n","\n","# class Progbar(object):\n","#     \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n","\n","#     Displays a progress bar.\n","#     Small edit : added strict arg to update\n","#     # Arguments\n","#         target: Total number of steps expected.\n","#         interval: Minimum visual progress update interval (in seconds).\n","#     \"\"\"\n","\n","#     def __init__(self, target, width=30, verbose=0):\n","#         self.width = width\n","#         self.target = target\n","#         self.sum_values = {}\n","#         self.unique_values = []\n","#         self.start = time.time()\n","#         self.total_width = 0\n","#         self.seen_so_far = 0\n","#         self.verbose = verbose\n","\n","#     def update(self, current, values=[], exact=[], strict=[]):\n","#         \"\"\"\n","#         Updates the progress bar.\n","#         # Arguments\n","#             current: Index of current step.\n","#             values: List of tuples (name, value_for_last_step).\n","#                 The progress bar will display averages for these values.\n","#             exact: List of tuples (name, value_for_last_step).\n","#                 The progress bar will display these values directly.\n","#         \"\"\"\n","\n","#         for k, v in values:\n","#             if k not in self.sum_values:\n","#                 self.sum_values[k] = [v * (current - self.seen_so_far),\n","#                                       current - self.seen_so_far]\n","#                 self.unique_values.append(k)\n","#             else:\n","#                 self.sum_values[k][0] += v * (current - self.seen_so_far)\n","#                 self.sum_values[k][1] += (current - self.seen_so_far)\n","#         for k, v in exact:\n","#             if k not in self.sum_values:\n","#                 self.unique_values.append(k)\n","#             self.sum_values[k] = [v, 1]\n","\n","#         for k, v in strict:\n","#             if k not in self.sum_values:\n","#                 self.unique_values.append(k)\n","#             self.sum_values[k] = v\n","\n","#         self.seen_so_far = current\n","\n","#         now = time.time()\n","#         if self.verbose == 1:\n","#             prev_total_width = self.total_width\n","#             sys.stdout.write(\"\\b\" * prev_total_width)\n","#             sys.stdout.write(\"\\r\")\n","\n","#             numdigits = int(np.floor(np.log10(self.target))) + 1\n","#             barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n","#             bar = barstr % (current, self.target)\n","#             prog = float(current)/self.target\n","#             prog_width = int(self.width*prog)\n","#             if prog_width > 0:\n","#                 bar += ('='*(prog_width-1))\n","#                 if current < self.target:\n","#                     bar += '>'\n","#                 else:\n","#                     bar += '='\n","#             bar += ('.'*(self.width-prog_width))\n","#             bar += ']'\n","#             sys.stdout.write(bar)\n","#             self.total_width = len(bar)\n","\n","#             if current:\n","#                 time_per_unit = (now - self.start) / current\n","#             else:\n","#                 time_per_unit = 0\n","#             eta = time_per_unit*(self.target - current)\n","#             info = ''\n","#             if current < self.target:\n","#                 info += ' - ETA: %ds' % eta\n","#             else:\n","#                 info += ' - %ds' % (now - self.start)\n","#             for k in self.unique_values:\n","#                 if type(self.sum_values[k]) is list:\n","#                     info += ' - %s: %.4f' % (k,\n","#                         self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","#                 else:\n","#                     info += ' - %s: %s' % (k, self.sum_values[k])\n","\n","#             self.total_width += len(info)\n","#             if prev_total_width > self.total_width:\n","#                 info += ((prev_total_width-self.total_width) * \" \")\n","\n","#             sys.stdout.write(info)\n","#             sys.stdout.flush()\n","\n","#             if current >= self.target:\n","#                 sys.stdout.write(\"\\n\")\n","\n","#         if self.verbose == 2:\n","#             if current >= self.target:\n","#                 info = '%ds' % (now - self.start)\n","#                 for k in self.unique_values:\n","#                     info += ' - %s: %.4f' % (k,\n","#                         self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","#                 sys.stdout.write(info + \"\\n\")\n","\n","#     def add(self, n, values=[]):\n","#         self.update(self.seen_so_far+n, values)\n","\n","# \"\"\"## Read Data\"\"\"\n","\n","# from __future__ import print_function\n","# from six.moves import xrange\n","# import six.moves.cPickle as pickle\n","# import gzip\n","# import os\n","# import numpy\n","\n","# def generate_matrix(seqs, maxlen, lengths):\n","#     n_samples = len(seqs)\n","#     x= numpy.zeros((n_samples, maxlen)).astype('int64')\n","\n","#     for idx, s in enumerate(seqs):\n","#         if lengths[idx]>= maxlen:\n","#             s=s[:maxlen]\n","#         x[idx, :lengths[idx]] = s\n","#     return x\n","\n","# def prepare_data(seqs, labels):\n","#     lengths = [len(s) for s in seqs]\n","#     labels = numpy.array(labels).astype('int32')\n","#     return [numpy.array(seqs), labels, numpy.array(lengths).astype('int32')]\n","\n","# def remove_unk(x, n_words):\n","#     return [[1 if w >= n_words else w for w in sen] for sen in x] \n","\n","# def load_data(path, n_words):\n","#     with open(path, 'rb') as f:\n","#         dataset_x, dataset_label= pickle.load(f)\n","#         train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","#         # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","#         test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","#     #remove unknown words\n","#     train_set_x = remove_unk(train_set_x, n_words)\n","#     # valid_set_x = remove_unk(valid_set_x, n_words)\n","#     test_set_x = remove_unk(test_set_x, n_words)\n","\n","#     return [train_set_x, train_set_y],[test_set_x, test_set_y]\n","\n","# \"\"\"# Implementation\n","\n","# ## LSTM Layer\n","# \"\"\"\n","\n","# def lstm_layer(initial_hidden_states, config, keep_prob, mask):\n","#     with tf.compat.v1.variable_scope('forward'):\n","#         fw_lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","#         fw_lstm = tf.contrib.rnn.DropoutWrapper(fw_lstm, output_keep_prob=keep_prob)\n","\n","#     with tf.compat.v1.variable_scope('backward'):\n","#         bw_lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","#         bw_lstm = tf.contrib.rnn.DropoutWrapper(bw_lstm, output_keep_prob=keep_prob)\n","\n","#     #bidirectional rnn\n","#     with tf.compat.v1.variable_scope('bilstm'):\n","#         lstm_output=tf.compat.v1.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, inputs=initial_hidden_states, sequence_length=mask, time_major=False, dtype=tf.float32)\n","#         lstm_output=tf.concat(lstm_output[0], 2)\n","\n","#     return lstm_output\n","\n","# \"\"\"## Classifier\"\"\"\n","\n","# class Classifer(object):\n","\n","#     def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n","#         #padding zeros\n","#         padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","#         #remove last steps\n","#         displaced_hidden_states=hidden_states[:,:-step,:]\n","#         #concat padding\n","#         return tf.concat([padding, displaced_hidden_states], axis=1)\n","#         #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","#     def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n","#         #padding zeros\n","#         padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","#         #remove last steps\n","#         displaced_hidden_states=hidden_states[:,step:,:]\n","#         #concat padding\n","#         return tf.concat([displaced_hidden_states, padding], axis=1)\n","#         #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","#     def sum_together(self, l):\n","#         combined_state=None\n","#         for tensor in l:\n","#             if combined_state==None:\n","#                 combined_state=tensor\n","#             else:\n","#                 combined_state=combined_state+tensor\n","#         return combined_state\n","    \n","#     def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n","#         with tf.compat.v1.name_scope(name_scope_name):\n","#             #Word parameters \n","#             #forget gate for left \n","#             with tf.compat.v1.name_scope(\"f1_gate\"):\n","#                 #current\n","#                 Wxf1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","#                 #left right\n","#                 Whf1 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","#                 #initial state\n","#                 Wif1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","#                 #dummy node\n","#                 Wdf1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","#             #forget gate for right \n","#             with tf.compat.v1.name_scope(\"f2_gate\"):\n","#                 Wxf2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","#                 Whf2 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","#                 Wif2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","#                 Wdf2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","#             #forget gate for inital states     \n","#             with tf.compat.v1.name_scope(\"f3_gate\"):\n","#                 Wxf3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","#                 Whf3 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","#                 Wif3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","#                 Wdf3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","#             #forget gate for dummy states     \n","#             with tf.compat.v1.name_scope(\"f4_gate\"):\n","#                 Wxf4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","#                 Whf4 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","#                 Wif4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","#                 Wdf4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","#             #input gate for current state     \n","#             with tf.compat.v1.name_scope(\"i_gate\"):\n","#                 Wxi = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n","#                 Whi = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n","#                 Wii = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n","#                 Wdi = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n","#             #input gate for output gate\n","#             with tf.compat.v1.name_scope(\"o_gate\"):\n","#                 Wxo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","#                 Who = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","#                 Wio = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n","#                 Wdo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n","#             #bias for the gates    \n","#             with tf.compat.v1.name_scope(\"biases\"):\n","#                 bi = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","#                 bo = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","#                 bf1 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n","#                 bf2 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n","#                 bf3 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n","#                 bf4 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n","\n","#             #dummy node gated attention parameters\n","#             #input gate for dummy state\n","#             with tf.compat.v1.name_scope(\"gated_d_gate\"):\n","#                 gated_Wxd = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","#                 gated_Whd = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","#             #output gate\n","#             with tf.compat.v1.name_scope(\"gated_o_gate\"):\n","#                 gated_Wxo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","#                 gated_Who = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","#             #forget gate for states of word\n","#             with tf.compat.v1.name_scope(\"gated_f_gate\"):\n","#                 gated_Wxf = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","#                 gated_Whf = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","#             #biases\n","#             with tf.compat.v1.name_scope(\"gated_biases\"):\n","#                 gated_bd = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","#                 gated_bo = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","#                 gated_bf = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","\n","#         #filters for attention        \n","#         mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n","#         mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, axis=2)               \n","#         #filter invalid steps\n","#         sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n","#         #filter embedding states\n","#         initial_hidden_states=initial_hidden_states*sequence_mask\n","#         initial_cell_states=initial_cell_states*sequence_mask\n","#         #record shape of the batch\n","#         shape=tf.shape(input=initial_hidden_states)\n","        \n","#         #initial embedding states\n","#         embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n","#         embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","#         #randomly initialize the states\n","#         if config.random_initialize:\n","#             initial_hidden_states=tf.random.uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","#             initial_cell_states=tf.random.uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","#             #filter it\n","#             initial_hidden_states=initial_hidden_states*sequence_mask\n","#             initial_cell_states=initial_cell_states*sequence_mask\n","\n","#         #inital dummy node states\n","#         dummynode_hidden_states=tf.reduce_mean(input_tensor=initial_hidden_states, axis=1)\n","#         dummynode_cell_states=tf.reduce_mean(input_tensor=initial_cell_states, axis=1)\n","\n","#         for i in range(num_layers):\n","#             #update dummy node states\n","#             #average states\n","#             combined_word_hidden_state=tf.reduce_mean(input_tensor=initial_hidden_states, axis=1)\n","#             reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","#             #copy dummy states for computing forget gate\n","#             transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","#             #input gate\n","#             gated_d_t = tf.nn.sigmoid(\n","#                 tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n","#             )\n","#             #output gate\n","#             gated_o_t = tf.nn.sigmoid(\n","#                 tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n","#             )\n","#             #forget gate for hidden states\n","#             gated_f_t = tf.nn.sigmoid(\n","#                 tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n","#             )\n","\n","#             #softmax on each hidden dimension \n","#             reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n","#             gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, axis=1)], axis=1), axis=1)\n","#             #split the softmax scores\n","#             new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n","#             new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n","#             #new dummy states\n","#             dummy_c_t=tf.reduce_sum(input_tensor=new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n","#             dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n","\n","#             #update word node states\n","#             #get states before\n","#             initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","#             initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n","#             initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","#             initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n","#             #get states after\n","#             initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","#             initial_cell_states_before=self.sum_together(initial_cell_states_before)\n","#             initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","#             initial_cell_states_after=self.sum_together(initial_cell_states_after)\n","            \n","#             #reshape for matmul\n","#             initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","#             initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","#             #concat before and after hidden states\n","#             concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n","\n","#             #copy dummy node states \n","#             transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","#             transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","\n","#             f1_t = tf.nn.sigmoid(\n","#                 tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n","#                 tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n","#             )\n","\n","#             f2_t = tf.nn.sigmoid(\n","#                 tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n","#                 tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n","#             )\n","\n","#             f3_t = tf.nn.sigmoid(\n","#                 tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n","#                 tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n","#             )\n","\n","#             f4_t = tf.nn.sigmoid(\n","#                 tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n","#                 tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n","#             )\n","            \n","#             i_t = tf.nn.sigmoid(\n","#                 tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n","#                 tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n","#             )\n","            \n","#             o_t = tf.nn.sigmoid(\n","#                 tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n","#                 tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n","#             )\n","            \n","#             f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n","\n","\n","#             five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n","#             five_gates=tf.nn.softmax(five_gates, axis=1)\n","#             f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n","            \n","#             f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n","\n","#             c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n","            \n","#             h_t = o_t * tf.nn.tanh(c_t)\n","\n","#             #update states\n","#             initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n","#             initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n","#             initial_hidden_states=initial_hidden_states*sequence_mask\n","#             initial_cell_states=initial_cell_states*sequence_mask\n","\n","#             dummynode_hidden_states=dummy_h_t\n","#             dummynode_cell_states=dummy_c_t\n","\n","#         initial_hidden_states = tf.nn.dropout(initial_hidden_states,1 - (self.dropout))\n","#         initial_cell_states = tf.nn.dropout(initial_cell_states, 1 - (self.dropout))\n","\n","#         return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n","\n","\n","\n","#     def __init__(self, config, session):\n","#         #inputs: features, mask, keep_prob, labels\n","#         self.input_data = tf.compat.v1.placeholder(tf.int32, [None, None], name=\"inputs\")\n","#         self.labels=tf.compat.v1.placeholder(tf.int64, [None,], name=\"labels\")\n","#         self.mask=tf.compat.v1.placeholder(tf.int32, [None,], name=\"mask\")\n","#         self.dropout=self.keep_prob=keep_prob=tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n","#         self.config=config\n","#         shape=tf.shape(input=self.input_data)\n","#         #if sys.argv[4]=='lstm':\n","#         #    self.dummy_input = tf.placeholder(tf.float32, [None, None], name=\"dummy\")\n","#         #embedding\n","#         self.embedding=embedding = tf.Variable(tf.random.normal([config.vocab_size, config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"embedding\", trainable=config.embedding_trainable)\n","#         #apply embedding\n","#         initial_hidden_states=tf.nn.embedding_lookup(params=embedding, ids=self.input_data)\n","#         initial_cell_states=tf.identity(initial_hidden_states)\n","\n","#         initial_hidden_states = tf.nn.dropout(initial_hidden_states,1 - (keep_prob))\n","#         initial_cell_states = tf.nn.dropout(initial_cell_states, 1 - (keep_prob))\n","\n","#         #create layers \n","#         if argument4=='slstm':\n","#             new_hidden_states,new_cell_state, dummynode_hidden_states=self.slstm_cell(\"word_slstm\", config.hidden_size,self.mask, initial_hidden_states, initial_cell_states, config.layer)\n","            \n","#             softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","#             softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","#             #representation=dummynode_hidden_states\n","#             representation=tf.reduce_mean(input_tensor=tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n","            \n","#             softmax_w2 = tf.Variable(tf.random.normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","#             softmax_b2 = tf.Variable(tf.random.normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","#             representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","\n","#         elif argument4=='lstm':\n","#             initial_hidden_states=lstm_layer(initial_hidden_states,config, self.keep_prob, self.mask)\n","#             softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","#             softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","#             representation=tf.reduce_sum(input_tensor=initial_hidden_states,axis=1)\n","#             config.hidden_size_sum=2*config.hidden_size\n","#         elif argument4=='cnn':\n","#             initial_hidden_states=tf.reshape(initial_hidden_states, [-1, 700, config.hidden_size])            \n","#             initial_hidden_states = tf.expand_dims(initial_hidden_states, -1)\n","#             pooled_outputs = []\n","#             for i, filter_size in enumerate([3]):\n","#                 with tf.compat.v1.name_scope(\"conv-maxpool-%s\" % filter_size):\n","#                     # Convolution Layer\n","#                     filter_shape = [filter_size, config.hidden_size, 1, config.hidden_size]\n","#                     W = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","#                     b = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b\")\n","\n","#                     W2 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W2\")\n","#                     b2 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b2\")\n","\n","#                     W3 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W3\")\n","#                     b3 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b3\")\n","\n","#                     W4 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W4\")\n","#                     b4 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b4\")\n","\n","#                     conv = tf.nn.conv2d(\n","#                         input=initial_hidden_states,\n","#                         filters=W,\n","#                         strides=[1, 1, 1, 1],\n","#                         padding=\"VALID\",\n","#                         name=\"conv\")\n","#                     h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","#                     print(h.get_shape())\n","#                     h=tf.transpose(a=h, perm=[0,1,3,2])\n","#                     # Apply nonlinearity\n","\n","\n","#                     conv2 = tf.nn.conv2d(\n","#                         input=h,\n","#                         filters=W2,\n","#                         strides=[1, 1, 1, 1],\n","#                         padding=\"VALID\",\n","#                         name=\"conv2\")\n","#                     h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n","#                     print(h2.get_shape())\n","#                     h2=tf.transpose(a=h2, perm=[0,1,3,2])\n","\n","#                     conv3 = tf.nn.conv2d(\n","#                         input=h2,\n","#                         filters=W3,\n","#                         strides=[1, 1, 1, 1],\n","#                         padding=\"VALID\",\n","#                         name=\"conv3\")  \n","#                     h3 = tf.nn.relu(tf.nn.bias_add(conv3, b3), name=\"relu3\")\n","#                     print(h3.get_shape())\n","\n","#                     # Max-pooling over the outputs\n","#                     pooled = tf.nn.max_pool2d(\n","#                         input=h3,\n","#                         ksize=[1, 700 - 3*filter_size + 3, 1, 1],\n","#                         strides=[1, 1, 1, 1],\n","#                         padding='VALID',\n","#                         name=\"pool\")\n","#                     pooled_outputs.append(pooled)\n","#             # Combine all the pooled features\n","#             num_filters_total = 1 * config.hidden_size\n","#             self.h_pool = tf.concat(pooled_outputs, axis=3)\n","#             representation = tf.reshape(self.h_pool, [-1, num_filters_total])\n","\n","#             softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","#             softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","\n","#             softmax_w2 = tf.Variable(tf.random.normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","#             softmax_b2 = tf.Variable(tf.random.normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","#             representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","#         else:\n","#             print(\"Invalid model\")\n","#             exit(1)\n","        \n","#         self.logits=logits = tf.matmul(representation, softmax_w) + softmax_b\n","#         self.to_print=tf.nn.softmax(logits)\n","#         #operators for prediction\n","#         self.prediction=prediction=tf.argmax(input=logits,axis=1)\n","#         correct_prediction = tf.equal(prediction, self.labels)\n","#         self.accuracy = tf.reduce_sum(input_tensor=tf.cast(correct_prediction, tf.float32))\n","        \n","#         #cross entropy loss\n","#         loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits)\n","#         self.cost=cost=tf.reduce_mean(input_tensor=loss)+ config.l2_beta*tf.nn.l2_loss(embedding)\n","\n","#         #designate training variables\n","#         tvars=tf.compat.v1.trainable_variables()\n","#         self.lr = tf.Variable(0.0, trainable=False)\n","#         grads=tf.gradients(ys=cost, xs=tvars)\n","#         grads, _ = tf.clip_by_global_norm(grads,config.max_grad_norm)\n","#         self.grads=grads\n","#         optimizer = tf.compat.v1.train.AdamOptimizer(config.learning_rate)        \n","#         self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","#     #assign value to learning rate\n","#     def assign_lr(self, session, lr_value):\n","#         session.run(tf.compat.v1.assign(self.lr, lr_value))\n","\n","# \"\"\"## get_minibatches_idx()\"\"\"\n","\n","# def get_minibatches_idx(n, batch_size, shuffle=True):\n","#     idx_list = np.arange(n, dtype=\"int32\")\n","\n","#     if shuffle:\n","#         np.random.shuffle(idx_list)\n","\n","#     minibatches = []\n","#     minibatch_start = 0\n","#     for i in range(n // batch_size):\n","#         minibatches.append(idx_list[minibatch_start:\n","#                                     minibatch_start + batch_size])\n","#         minibatch_start += batch_size\n","#     if (minibatch_start != n):\n","#         # Make a minibatch out of what is left\n","#         minibatches.append(idx_list[minibatch_start:])\n","#     return minibatches\n","\n","# \"\"\"## run_epoch\"\"\"\n","\n","# def run_epoch(session, config, model, data, eval_op, keep_prob, is_training):\n","#     n_samples = len(data[0])\n","#     print(\"Running %d samples:\"%(n_samples))  \n","#     minibatches = get_minibatches_idx(n_samples, config.batch_size, shuffle=False)\n","\n","#     predictions = []\n","#     correct = 0.\n","#     total = 0\n","#     total_cost=0\n","#     prog = Progbar(target=len(minibatches))\n","#     #dummynode_hidden_states_collector=np.array([[0]*config.hidden_size])\n","\n","#     to_print_total=np.array([[0]*2])\n","#     for i, inds in enumerate(minibatches):\n","#         x = data[0][inds]\n","#         if argument4=='cnn':\n","#             x=pad_sequences(x, maxlen=700, dtype='int32',padding='post', truncating='post', value=0.)\n","#         else:\n","#             x=pad_sequences(x, maxlen=None, dtype='int32',padding='post', truncating='post', value=0.)\n","#         y = data[1][inds]\n","#         mask = data[2][inds]\n","\n","\n","\n","#         count, _, cost, to_print,prediction= \\\n","#         session.run([model.accuracy, eval_op,model.cost, model.to_print,model.prediction],\\\n","#             {model.input_data: x, model.labels: y, model.mask:mask, model.keep_prob:keep_prob}) \n","        \n","\n","#         if not is_training:\n","#             to_print_total=np.concatenate((to_print_total, to_print),axis=0)\n","#         correct += count \n","#         total += len(inds)\n","#         total_cost+=cost\n","#         predictions.extend(prediction.tolist())\n","#         prog.update(i + 1, [(\"train loss\", cost)])\n","#     #if not is_training:\n","#     #    print(to_print_total[:, 0].tolist())\n","#     #    print(data[1].tolist())\n","#     #    print(data[2].tolist())\n","\n","#     actual = data[1]\n","\n","#     TN, FP, FN, TP = confusion_matrix(actual, predictions).ravel()\n","\n","#     precision = TP / (TP + FP)\n","#     recall = TP / (TP + FN)\n","#     f1 = 2 * precision * recall / (precision + recall)\n","\n","#     print(\"Total loss:\")\n","#     print(total_cost)\n","\n","#     accuracy = correct/total\n","\n","#     return accuracy,precision,recall,f1, actual, predictions\n","\n","# \"\"\"## train_test_model\"\"\"\n","\n","# def train_test_model(config, i, session, model, train_dataset,test_dataset):\n","#     #compute lr_decay\n","#     lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","#     #update learning rate\n","#     model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","#     #training            \n","#     print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\n","#     start_time = time.time()\n","#     train_acc, train_precision, train_recall, train_f1,actual,predictions = run_epoch(session, config, model, train_dataset, model.train_op, config.keep_prob, True)\n","#     print(\"Training Accuracy = %.4f, time = %.3f seconds\\n\"%(train_acc, time.time()-start_time))\n","\n","#     # #valid \n","#     # valid_acc = run_epoch(session, config, model, valid_dataset, tf.no_op(),1, False)\n","#     # print(\"Valid Accuracy = %.4f\\n\" % valid_acc)\n","\n","#     #testing\n","#     start_time = time.time()\n","#     test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","#     print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","#     print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","#     print('confusion metric : ')\n","#     print((confusion_matrix(test_actual, test_predictions).ravel()))\n","#     #return valid_acc, test_acc\n","#     return test_actual,test_predictions\n","\n","# \"\"\"## start_epoches\"\"\"\n","\n","# def start_epoches(config, session,classifier, train_dataset, test_dataset):\n","#     #record max\n","#     #max_val_acc=-1\n","#     #max_test_acc=-1\n","#     # saver = tf.train.Saver()\n","#     all_actual = []\n","#     all_predictions = []\n","\n","#     for i in range(config.max_max_epoch):\n","#       test_actual,test_predictions = train_test_model(config, i, session, classifier, train_dataset, test_dataset)\n","      \n","#       all_actual.extend(test_actual)\n","#       all_predictions.extend(test_predictions)\n","\n","\n","#       # if (i % 1 == 0 and i != 0):\n","#       #   save_path = saver.save(session, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/pretrained_lstm.ckpt\", global_step=i)\n","#       #     print(\"saved to %s\" % save_path)\n","\n","#     TN, FP, FN, TP = confusion_matrix(all_actual, all_predictions).ravel()\n","\n","#     precision = TP / (TP + FP)\n","#     recall = TP / (TP + FN)\n","#     f1 = 2 * precision * recall / (precision + recall)\n","#     accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","#     return accuracy\n","    \n","#     # print(\"final Accuracy : \",accuracy)\n","#     # print(\"final precision : \",precision)\n","#     # print(\"final recall : \",recall)\n","#     # print(\"final f1 : \",f1)\n","\n","# \"\"\"## word_to_vec\"\"\"\n","\n","# def word_to_vec(matrix, session,config, *args):\n","    \n","#     print(\"word2vec shape: \", matrix.shape)\n","    \n","#     for model in args:\n","#         session.run(tf.compat.v1.assign(model.embedding, matrix))\n","\n","# \"\"\"# Main\n","\n","# ## configs\n","# \"\"\"\n","\n","# argument1 = \"7\" # slstm iteration\n","# argument2 = \"2\" #num window_size\n","# argument3 = \"sinhala_news\" #dataset_name\n","# argument4 = \"slstm\" #model_name\n","\n","# config = Config()\n","# config.layer=int(argument1)\n","# config.step=int(argument2)\n","# config.vocab_size=(18413) # number of words in fastText model\n","# print(\"dataset: \"+argument3)\n","# print(\"iteration: \"+str(config.layer))\n","# print(\"step: \"+str(config.step))\n","# print(\"model: \"+str(argument4))\n","\n","# \"\"\"## Load Embedding Matrix\"\"\"\n","\n","# f = open(vector_path, 'rb')\n","# matrix= np.array(pickle.load(f))\n","# config.vocab_size=matrix.shape[0]\n","# print(config.vocab_size)\n","\n","# \"\"\"## Load Dataset\"\"\"\n","\n","# # convert_to_vectors()\n","# # train_data_vectors, train_data_labels, test_data_vectors, test_data_labels = load_vectors()\n","\n","# train_dataset, test_dataset = load_data(path=path,n_words=config.vocab_size)\n","# config.num_label= len(set(train_dataset[1]))\n","\n","# print(\"number label: \"+str(config.num_label))\n","# train_dataset = prepare_data(train_dataset[0], train_dataset[1])\n","# # valid_dataset = prepare_data(valid_dataset[0], valid_dataset[1])\n","# test_dataset = prepare_data(test_dataset[0], test_dataset[1])\n","\n","# print(train_dataset)\n","\n","# \"\"\"## Train_Test Model\"\"\"\n","\n","# with tf.Graph().as_default(), tf.compat.v1.Session() as session:\n","#     initializer = tf.compat.v1.random_normal_initializer(0, 0.05)\n","\n","#     classifier= Classifer(config=config, session=session)\n","#     saver = tf.compat.v1.train.Saver()\n","\n","#     total=0\n","#     #print trainable variables\n","#     # for v in tf.compat.v1.trainable_variables():\n","#     #     print(v.name)\n","#     #     shape=v.get_shape()\n","#     #     try:\n","#     #         size=shape[0].value*shape[1].value\n","#     #     except:\n","#     #         size=shape[0].value\n","#     #     total+=size\n","#     # print(total)\n","#     #initialize\n","#     init = tf.compat.v1.global_variables_initializer()\n","\n","#     session.run(init)\n","#     #train test model\n","\n","#     print (\"model_test\",matrix)\n","\n","\n","\n","#     word_to_vec(matrix, session,config, classifier)\n","#     start_epoches(config, session,classifier, train_dataset, test_dataset)\n","#     # # Add ops to save and restore all the variables.\n","    \n","#     # save_path = saver.save(session, \"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/500epochs/1/\")\n","#     # print(\"Model saved in path: %s\" % save_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sn9fadSHZExx","colab_type":"text"},"source":["# Data Paths"]},{"cell_type":"code","metadata":{"id":"QxUeR-ALY1Bc","colab_type":"code","colab":{}},"source":["path='/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/SLSTM/parsed_data/from_fasttext/data_set'\n","vector_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/SLSTM/parsed_data/from_fasttext/fasttext_vectors'\n","# run from lahiru1st@gmail.com\n","# path='/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/data_set'\n","# vector_path = '/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/classfication/parsed_data/from_fasttext/fasttext_vectors'\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zSRrIPzZQXy","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"_hXkbjpZZUEr","colab_type":"code","outputId":"153697a6-e9ab-4cef-d6cc-1668ad673165","executionInfo":{"status":"ok","timestamp":1591163805272,"user_tz":-330,"elapsed":8872,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["# !pip install xlutils\n","# !pip install xlrd\n","from __future__ import print_function\n","import tensorflow as tf\n","import pickle\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import confusion_matrix\n","import numpy\n","import six.moves.cPickle as pickle\n","import time\n","import sys\n","import numpy as np\n","from tensorboard.plugins.hparams import api as hp\n","import xlwt \n","from xlwt import Workbook \n","from xlutils.copy import copy # http://pypi.python.org/pypi/xlutils\n","from xlrd import open_workbook # http://pypi.python.org/pypi/xlrd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting xlutils\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/55/e22ac73dbb316cabb5db28bef6c87044a95914f713a6e81b593f8a0d2f79/xlutils-2.0.0-py2.py3-none-any.whl (55kB)\n","\r\u001b[K     |██████                          | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: xlwt>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from xlutils) (1.3.0)\n","Requirement already satisfied: xlrd>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from xlutils) (1.1.0)\n","Installing collected packages: xlutils\n","Successfully installed xlutils-2.0.0\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CQ0ZZbeUZkfe","colab_type":"text"},"source":["# config"]},{"cell_type":"markdown","metadata":{"id":"RdnVJRILcTCT","colab_type":"text"},"source":["## Config Obj"]},{"cell_type":"code","metadata":{"id":"9WWTIUyBZhFv","colab_type":"code","colab":{}},"source":["class Config(object):\n","    vocab_size=15000\n","    max_grad_norm = 5\n","    init_scale = 0.05\n","    hidden_size = 300\n","    lr_decay = 0.95 #0.95-0.99\n","    valid_portion=0.0\n","    batch_size=8  #4-8-16-32\n","    keep_prob = 0.8\n","    learning_rate = 0.002\n","    max_epoch =2\n","    max_max_epoch = 15\n","    num_label=2\n","    attention_iteration=3\n","    random_initialize=False\n","    embedding_trainable=True\n","    l2_beta=0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7Pc2AR-bWY4","colab_type":"text"},"source":["# Impementation"]},{"cell_type":"markdown","metadata":{"id":"RQ5Kb9rnacLJ","colab_type":"text"},"source":["## Progress Bar"]},{"cell_type":"code","metadata":{"id":"A_IHi6E_aeqS","colab_type":"code","colab":{}},"source":["class Progbar(object):\n","    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n","\n","    Displays a progress bar.\n","    Small edit : added strict arg to update\n","    # Arguments\n","        target: Total number of steps expected.\n","        interval: Minimum visual progress update interval (in seconds).\n","    \"\"\"\n","\n","    def __init__(self, target, width=30, verbose=0):\n","        self.width = width\n","        self.target = target\n","        self.sum_values = {}\n","        self.unique_values = []\n","        self.start = time.time()\n","        self.total_width = 0\n","        self.seen_so_far = 0\n","        self.verbose = verbose\n","\n","    def update(self, current, values=[], exact=[], strict=[]):\n","        \"\"\"\n","        Updates the progress bar.\n","        # Arguments\n","            current: Index of current step.\n","            values: List of tuples (name, value_for_last_step).\n","                The progress bar will display averages for these values.\n","            exact: List of tuples (name, value_for_last_step).\n","                The progress bar will display these values directly.\n","        \"\"\"\n","\n","        for k, v in values:\n","            if k not in self.sum_values:\n","                self.sum_values[k] = [v * (current - self.seen_so_far),\n","                                      current - self.seen_so_far]\n","                self.unique_values.append(k)\n","            else:\n","                self.sum_values[k][0] += v * (current - self.seen_so_far)\n","                self.sum_values[k][1] += (current - self.seen_so_far)\n","        for k, v in exact:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = [v, 1]\n","\n","        for k, v in strict:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = v\n","\n","        self.seen_so_far = current\n","\n","        now = time.time()\n","        if self.verbose == 1:\n","            prev_total_width = self.total_width\n","            sys.stdout.write(\"\\b\" * prev_total_width)\n","            sys.stdout.write(\"\\r\")\n","\n","            numdigits = int(np.floor(np.log10(self.target))) + 1\n","            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n","            bar = barstr % (current, self.target)\n","            prog = float(current)/self.target\n","            prog_width = int(self.width*prog)\n","            if prog_width > 0:\n","                bar += ('='*(prog_width-1))\n","                if current < self.target:\n","                    bar += '>'\n","                else:\n","                    bar += '='\n","            bar += ('.'*(self.width-prog_width))\n","            bar += ']'\n","            sys.stdout.write(bar)\n","            self.total_width = len(bar)\n","\n","            if current:\n","                time_per_unit = (now - self.start) / current\n","            else:\n","                time_per_unit = 0\n","            eta = time_per_unit*(self.target - current)\n","            info = ''\n","            if current < self.target:\n","                info += ' - ETA: %ds' % eta\n","            else:\n","                info += ' - %ds' % (now - self.start)\n","            for k in self.unique_values:\n","                if type(self.sum_values[k]) is list:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                else:\n","                    info += ' - %s: %s' % (k, self.sum_values[k])\n","\n","            self.total_width += len(info)\n","            if prev_total_width > self.total_width:\n","                info += ((prev_total_width-self.total_width) * \" \")\n","\n","            sys.stdout.write(info)\n","            sys.stdout.flush()\n","\n","            if current >= self.target:\n","                sys.stdout.write(\"\\n\")\n","\n","        if self.verbose == 2:\n","            if current >= self.target:\n","                info = '%ds' % (now - self.start)\n","                for k in self.unique_values:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                sys.stdout.write(info + \"\\n\")\n","\n","    def add(self, n, values=[]):\n","        self.update(self.seen_so_far+n, values)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4-cYBMrah8L","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"code","metadata":{"id":"y9tUuE3-aoxu","colab_type":"code","colab":{}},"source":["def generate_matrix(seqs, maxlen, lengths):\n","    n_samples = len(seqs)\n","    x= numpy.zeros((n_samples, maxlen)).astype('int64')\n","\n","    for idx, s in enumerate(seqs):\n","        if lengths[idx]>= maxlen:\n","            s=s[:maxlen]\n","        x[idx, :lengths[idx]] = s\n","    return x\n","\n","def prepare_data(seqs, labels):\n","    lengths = [len(s) for s in seqs]\n","    labels = numpy.array(labels).astype('int32')\n","    return [numpy.array(seqs), labels, numpy.array(lengths).astype('int32')]\n","\n","def remove_unk(x, n_words):\n","    return [[1 if w >= n_words else w for w in sen] for sen in x] \n","\n","def load_data(path, n_words):\n","    with open(path, 'rb') as f:\n","        dataset_x, dataset_label= pickle.load(f)\n","        train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","        # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","        test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","    train_set_x = remove_unk(train_set_x, n_words)\n","    # valid_set_x = remove_unk(valid_set_x, n_words)\n","    test_set_x = remove_unk(test_set_x, n_words)\n","\n","    return [train_set_x, train_set_y],[test_set_x, test_set_y]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vO5CPE1Tapnb","colab_type":"text"},"source":["## SLSTM layer"]},{"cell_type":"code","metadata":{"id":"L-WPNuB5azON","colab_type":"code","colab":{}},"source":["def lstm_layer(initial_hidden_states, config, keep_prob, mask):\n","    with tf.compat.v1.variable_scope('forward'):\n","        fw_lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        fw_lstm = tf.contrib.rnn.DropoutWrapper(fw_lstm, output_keep_prob=keep_prob)\n","\n","    with tf.compat.v1.variable_scope('backward'):\n","        bw_lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        bw_lstm = tf.contrib.rnn.DropoutWrapper(bw_lstm, output_keep_prob=keep_prob)\n","\n","    #bidirectional rnn\n","    with tf.compat.v1.variable_scope('bilstm'):\n","        lstm_output=tf.compat.v1.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, inputs=initial_hidden_states, sequence_length=mask, time_major=False, dtype=tf.float32)\n","        lstm_output=tf.concat(lstm_output[0], 2)\n","\n","    return lstm_output\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rc0JOQkaa1V2","colab_type":"text"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"M9YIDjDXa8BP","colab_type":"code","colab":{}},"source":["class Classifer(object):\n","\n","    def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,:-step,:]\n","        #concat padding\n","        return tf.concat([padding, displaced_hidden_states], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,step:,:]\n","        #concat padding\n","        return tf.concat([displaced_hidden_states, padding], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def sum_together(self, l):\n","        combined_state=None\n","        for tensor in l:\n","            if combined_state==None:\n","                combined_state=tensor\n","            else:\n","                combined_state=combined_state+tensor\n","        return combined_state\n","    \n","    def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n","        with tf.compat.v1.name_scope(name_scope_name):\n","            #Word parameters \n","            #forget gate for left \n","            with tf.compat.v1.name_scope(\"f1_gate\"):\n","                #current\n","                Wxf1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                #left right\n","                Whf1 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                #initial state\n","                Wif1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                #dummy node\n","                Wdf1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for right \n","            with tf.compat.v1.name_scope(\"f2_gate\"):\n","                Wxf2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf2 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for inital states     \n","            with tf.compat.v1.name_scope(\"f3_gate\"):\n","                Wxf3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf3 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for dummy states     \n","            with tf.compat.v1.name_scope(\"f4_gate\"):\n","                Wxf4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf4 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #input gate for current state     \n","            with tf.compat.v1.name_scope(\"i_gate\"):\n","                Wxi = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n","                Whi = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n","                Wii = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n","                Wdi = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n","            #input gate for output gate\n","            with tf.compat.v1.name_scope(\"o_gate\"):\n","                Wxo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                Who = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","                Wio = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n","                Wdo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n","            #bias for the gates    \n","            with tf.compat.v1.name_scope(\"biases\"):\n","                bi = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                bo = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                bf1 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n","                bf2 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n","                bf3 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n","                bf4 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n","\n","            #dummy node gated attention parameters\n","            #input gate for dummy state\n","            with tf.compat.v1.name_scope(\"gated_d_gate\"):\n","                gated_Wxd = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                gated_Whd = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","            #output gate\n","            with tf.compat.v1.name_scope(\"gated_o_gate\"):\n","                gated_Wxo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Who = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #forget gate for states of word\n","            with tf.compat.v1.name_scope(\"gated_f_gate\"):\n","                gated_Wxf = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Whf = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #biases\n","            with tf.compat.v1.name_scope(\"gated_biases\"):\n","                gated_bd = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                gated_bo = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                gated_bf = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","\n","        #filters for attention        \n","        mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n","        mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, axis=2)               \n","        #filter invalid steps\n","        sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n","        #filter embedding states\n","        initial_hidden_states=initial_hidden_states*sequence_mask\n","        initial_cell_states=initial_cell_states*sequence_mask\n","        #record shape of the batch\n","        shape=tf.shape(input=initial_hidden_states)\n","        \n","        #initial embedding states\n","        embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n","        embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","        #randomly initialize the states\n","        if config.random_initialize:\n","            initial_hidden_states=tf.random.uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            initial_cell_states=tf.random.uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            #filter it\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","        #inital dummy node states\n","        dummynode_hidden_states=tf.reduce_mean(input_tensor=initial_hidden_states, axis=1)\n","        dummynode_cell_states=tf.reduce_mean(input_tensor=initial_cell_states, axis=1)\n","\n","        for i in range(num_layers):\n","            #update dummy node states\n","            #average states\n","            combined_word_hidden_state=tf.reduce_mean(input_tensor=initial_hidden_states, axis=1)\n","            reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            #copy dummy states for computing forget gate\n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            #input gate\n","            gated_d_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n","            )\n","            #output gate\n","            gated_o_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n","            )\n","            #forget gate for hidden states\n","            gated_f_t = tf.nn.sigmoid(\n","                tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n","            )\n","\n","            #softmax on each hidden dimension \n","            reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n","            gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, axis=1)], axis=1), axis=1)\n","            #split the softmax scores\n","            new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n","            new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n","            #new dummy states\n","            dummy_c_t=tf.reduce_sum(input_tensor=new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n","            dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n","\n","            #update word node states\n","            #get states before\n","            initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n","            initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n","            #get states after\n","            initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_before=self.sum_together(initial_cell_states_before)\n","            initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_after=self.sum_together(initial_cell_states_after)\n","            \n","            #reshape for matmul\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","            #concat before and after hidden states\n","            concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n","\n","            #copy dummy node states \n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","\n","            f1_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n","                tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n","            )\n","\n","            f2_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n","                tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n","            )\n","\n","            f3_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n","                tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n","            )\n","\n","            f4_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n","                tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n","            )\n","            \n","            i_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n","                tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n","            )\n","            \n","            o_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n","                tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n","            )\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n","\n","\n","            five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n","            five_gates=tf.nn.softmax(five_gates, axis=1)\n","            f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n","\n","            c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n","            \n","            h_t = o_t * tf.nn.tanh(c_t)\n","\n","            #update states\n","            initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n","            initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","            dummynode_hidden_states=dummy_h_t\n","            dummynode_cell_states=dummy_c_t\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,1 - (self.dropout))\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, 1 - (self.dropout))\n","\n","        return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n","\n","\n","\n","    def __init__(self, config, session):\n","        #inputs: features, mask, keep_prob, labels\n","        self.input_data = tf.compat.v1.placeholder(tf.int32, [None, None], name=\"inputs\")\n","        self.labels=tf.compat.v1.placeholder(tf.int64, [None,], name=\"labels\")\n","        self.mask=tf.compat.v1.placeholder(tf.int32, [None,], name=\"mask\")\n","        self.dropout=self.keep_prob=keep_prob=tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n","        self.config=config\n","        shape=tf.shape(input=self.input_data)\n","        #if sys.argv[4]=='lstm':\n","        #    self.dummy_input = tf.placeholder(tf.float32, [None, None], name=\"dummy\")\n","        #embedding\n","        self.embedding=embedding = tf.Variable(tf.random.normal([config.vocab_size, config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"embedding\", trainable=config.embedding_trainable)\n","        #apply embedding\n","        initial_hidden_states=tf.nn.embedding_lookup(params=embedding, ids=self.input_data)\n","        initial_cell_states=tf.identity(initial_hidden_states)\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,1 - (keep_prob))\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, 1 - (keep_prob))\n","\n","        #create layers \n","        if config.model_type=='slstm':\n","            new_hidden_states,new_cell_state, dummynode_hidden_states=self.slstm_cell(\"word_slstm\", config.hidden_size,self.mask, initial_hidden_states, initial_cell_states, config.layer)\n","            \n","            softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            #representation=dummynode_hidden_states\n","            representation=tf.reduce_mean(input_tensor=tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n","            \n","            softmax_w2 = tf.Variable(tf.random.normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random.normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","\n","        elif config.model_type=='lstm':\n","            initial_hidden_states=lstm_layer(initial_hidden_states,config, self.keep_prob, self.mask)\n","            softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            representation=tf.reduce_sum(input_tensor=initial_hidden_states,axis=1)\n","            config.hidden_size_sum=2*config.hidden_size\n","        elif config.model_type=='cnn':\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, 700, config.hidden_size])            \n","            initial_hidden_states = tf.expand_dims(initial_hidden_states, -1)\n","            pooled_outputs = []\n","            for i, filter_size in enumerate([3]):\n","                with tf.compat.v1.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                    # Convolution Layer\n","                    filter_shape = [filter_size, config.hidden_size, 1, config.hidden_size]\n","                    W = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","                    b = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b\")\n","\n","                    W2 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W2\")\n","                    b2 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b2\")\n","\n","                    W3 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W3\")\n","                    b3 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b3\")\n","\n","                    W4 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W4\")\n","                    b4 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b4\")\n","\n","                    conv = tf.nn.conv2d(\n","                        input=initial_hidden_states,\n","                        filters=W,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv\")\n","                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                    print(h.get_shape())\n","                    h=tf.transpose(a=h, perm=[0,1,3,2])\n","                    # Apply nonlinearity\n","\n","\n","                    conv2 = tf.nn.conv2d(\n","                        input=h,\n","                        filters=W2,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv2\")\n","                    h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n","                    print(h2.get_shape())\n","                    h2=tf.transpose(a=h2, perm=[0,1,3,2])\n","\n","                    conv3 = tf.nn.conv2d(\n","                        input=h2,\n","                        filters=W3,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv3\")  \n","                    h3 = tf.nn.relu(tf.nn.bias_add(conv3, b3), name=\"relu3\")\n","                    print(h3.get_shape())\n","\n","                    # Max-pooling over the outputs\n","                    pooled = tf.nn.max_pool2d(\n","                        input=h3,\n","                        ksize=[1, 700 - 3*filter_size + 3, 1, 1],\n","                        strides=[1, 1, 1, 1],\n","                        padding='VALID',\n","                        name=\"pool\")\n","                    pooled_outputs.append(pooled)\n","            # Combine all the pooled features\n","            num_filters_total = 1 * config.hidden_size\n","            self.h_pool = tf.concat(pooled_outputs, axis=3)\n","            representation = tf.reshape(self.h_pool, [-1, num_filters_total])\n","\n","            softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","\n","            softmax_w2 = tf.Variable(tf.random.normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random.normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","        else:\n","            print(\"Invalid model\")\n","            exit(1)\n","        \n","        self.logits=logits = tf.matmul(representation, softmax_w) + softmax_b\n","        self.to_print=tf.nn.softmax(logits)\n","        #operators for prediction\n","        self.prediction=prediction=tf.argmax(input=logits,axis=1)\n","        correct_prediction = tf.equal(prediction, self.labels)\n","        self.accuracy = tf.reduce_sum(input_tensor=tf.cast(correct_prediction, tf.float32))\n","        \n","        #cross entropy loss\n","        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits)\n","        self.cost=cost=tf.reduce_mean(input_tensor=loss)+ config.l2_beta*tf.nn.l2_loss(embedding)\n","\n","        #designate training variables\n","        tvars=tf.compat.v1.trainable_variables()\n","        self.lr = tf.Variable(0.0, trainable=False)\n","        grads=tf.gradients(ys=cost, xs=tvars)\n","        grads, _ = tf.clip_by_global_norm(grads,config.max_grad_norm)\n","        self.grads=grads\n","        optimizer = tf.compat.v1.train.AdamOptimizer(config.learning_rate)        \n","        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","    #assign value to learning rate\n","    def assign_lr(self, session, lr_value):\n","        session.run(tf.compat.v1.assign(self.lr, lr_value))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"klCxrycBbGhx","colab_type":"text"},"source":["## Train_Test Implementation"]},{"cell_type":"code","metadata":{"id":"Bk5hogQvbQ9u","colab_type":"code","colab":{}},"source":["def get_minibatches_idx(n, batch_size, shuffle=True):\n","    idx_list = np.arange(n, dtype=\"int32\")\n","\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","\n","    minibatches = []\n","    minibatch_start = 0\n","    for i in range(n // batch_size):\n","        minibatches.append(idx_list[minibatch_start:\n","                                    minibatch_start + batch_size])\n","        minibatch_start += batch_size\n","    if (minibatch_start != n):\n","        # Make a minibatch out of what is left\n","        minibatches.append(idx_list[minibatch_start:])\n","    return minibatches\n","\n","\"\"\"## run_epoch\"\"\"\n","\n","def run_epoch(session, config, model, data, eval_op, keep_prob, is_training):\n","    n_samples = len(data[0])\n","    # print(\"Running %d samples:\"%(n_samples))  \n","    minibatches = get_minibatches_idx(n_samples, config.batch_size, shuffle=False)\n","\n","    predictions = []\n","    correct = 0.\n","    total = 0\n","    total_cost=0\n","    prog = Progbar(target=len(minibatches))\n","    #dummynode_hidden_states_collector=np.array([[0]*config.hidden_size])\n","\n","    to_print_total=np.array([[0]*2])\n","    for i, inds in enumerate(minibatches):\n","        x = data[0][inds]\n","        if config.model_type=='cnn':\n","            x=pad_sequences(x, maxlen=700, dtype='int32',padding='post', truncating='post', value=0.)\n","        else:\n","            x=pad_sequences(x, maxlen=None, dtype='int32',padding='post', truncating='post', value=0.)\n","        y = data[1][inds]\n","        mask = data[2][inds]\n","\n","\n","\n","        count, _, cost, to_print,prediction= \\\n","        session.run([model.accuracy, eval_op,model.cost, model.to_print,model.prediction],\\\n","            {model.input_data: x, model.labels: y, model.mask:mask, model.keep_prob:keep_prob}) \n","        \n","\n","        if not is_training:\n","            to_print_total=np.concatenate((to_print_total, to_print),axis=0)\n","        correct += count \n","        total += len(inds)\n","        total_cost+=cost\n","        predictions.extend(prediction.tolist())\n","        prog.update(i + 1, [(\"train loss\", cost)])\n","    #if not is_training:\n","    #    print(to_print_total[:, 0].tolist())\n","    #    print(data[1].tolist())\n","    #    print(data[2].tolist())\n","\n","    actual = data[1]\n","\n","    TN, FP, FN, TP = confusion_matrix(actual, predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","    print(\"Total loss: \",total_cost)\n","\n","    accuracy = correct/total\n","\n","    return accuracy,precision,recall,f1, actual, predictions\n","\n","\"\"\"## train_test_model\"\"\"\n","\n","def train_model(config, i, session, model, train_dataset):\n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","    #training            \n","    # print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\n","    start_time = time.time()\n","    train_acc, train_precision, train_recall, train_f1,actual,predictions = run_epoch(session, config, model, train_dataset, model.train_op, config.keep_prob, True)\n","    print(\"Training Accuracy = %.4f, time = %.3f seconds\\n\"%(train_acc, time.time()-start_time))\n","\n","    # #valid \n","    # valid_acc = run_epoch(session, config, model, valid_dataset, tf.no_op(),1, False)\n","    # print(\"Valid Accuracy = %.4f\\n\" % valid_acc)\n","\n","    #testing\n","    # start_time = time.time()\n","    # test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    # print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    # print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    # print('confusion metric : ')\n","    # print((confusion_matrix(test_actual, test_predictions).ravel()))\n","\n","    #return valid_acc, test_acc\n","    # return test_actual,test_predictions\n","\n","\"\"\"## start_epoches\"\"\"\n","\n","def start_epoches(config, session,classifier, train_dataset):\n","    #record max\n","    #max_val_acc=-1\n","    #max_test_acc=-1\n","    # saver = tf.train.Saver()\n","    # all_actual = []\n","    # all_predictions = []\n","\n","    for i in range(config.max_max_epoch):\n","      train_model(config, i, session, classifier, train_dataset)\n","      \n","      # all_actual.extend(test_actual)\n","      # all_predictions.extend(test_predictions)\n","\n","\n","      # if (i % 1 == 0 and i != 0):\n","      #   save_path = saver.save(session, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/pretrained_lstm.ckpt\", global_step=i)\n","      #     print(\"saved to %s\" % save_path)\n","\n","    # TN, FP, FN, TP = confusion_matrix(all_actual, all_predictions).ravel()\n","\n","    # precision = TP / (TP + FP)\n","    # recall = TP / (TP + FN)\n","    # f1 = 2 * precision * recall / (precision + recall)\n","    # accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","    # return accuracy\n","    \n","    # print(\"final Accuracy : \",accuracy)\n","    # print(\"final precision : \",precision)\n","    # print(\"final recall : \",recall)\n","    # print(\"final f1 : \",f1)\n","\n","\"\"\"## word_to_vec\"\"\"\n","\n","def word_to_vec(matrix, session,config, *args):\n","    \n","    print(\"word2vec shape: \", matrix.shape)\n","    \n","    for model in args:\n","        session.run(tf.compat.v1.assign(model.embedding, matrix))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZvJUl3bblls","colab_type":"text"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"zuOHTLR9boV7","colab_type":"text"},"source":["## Set Configurations"]},{"cell_type":"code","metadata":{"id":"PbPLmTWIbx1R","colab_type":"code","colab":{}},"source":["# argument1 = \"7\" # slstm iteration\n","# argument2 = \"2\" #num window_size\n","# argument3 = \"sinhala_news\" #dataset_name\n","# argument4 = \"slstm\" #model_name\n","\n","config = Config()\n","# config.layer=int(argument1)\n","# config.step=int(argument2)\n","# print(\"dataset: \"+argument3)\n","# print(\"iteration: \"+str(config.layer))\n","# print(\"step: \"+str(config.step))\n","# print(\"model: \"+str(argument4))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vc9c9TU4b3T2","colab_type":"text"},"source":["## Load Dataset and Embedding"]},{"cell_type":"code","metadata":{"id":"4tdO27xpb91l","colab_type":"code","outputId":"6f27422b-96f2-4a2f-dbf7-a90434567a9a","executionInfo":{"status":"ok","timestamp":1591163863638,"user_tz":-330,"elapsed":2782,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["\n","f = open(vector_path, 'rb')\n","matrix= np.array(pickle.load(f))\n","config.vocab_size=matrix.shape[0]\n","# print(config.vocab_size)\n","\n","\"\"\"## Load Dataset\"\"\"\n","\n","# convert_to_vectors()\n","# train_data_vectors, train_data_labels, test_data_vectors, test_data_labels = load_vectors()\n","\n","train_dataset, test_dataset = load_data(path=path,n_words=config.vocab_size)\n","config.num_label= len(set(train_dataset[1]))\n","\n","# print(\"number label: \"+str(config.num_label))\n","train_dataset = prepare_data(train_dataset[0], train_dataset[1])\n","# valid_dataset = prepare_data(valid_dataset[0], valid_dataset[1])\n","test_dataset = prepare_data(test_dataset[0], test_dataset[1])\n","\n","print(train_dataset)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[array([list([133, 105, 445, 19, 207, 2, 6, 4429, 12, 4430, 1533, 375, 77, 160, 33, 7, 10, 423, 17, 2088, 155, 10, 3205]),\n","       list([204, 26]),\n","       list([318, 180, 4431, 303, 7096, 181, 2512, 7097, 7, 4432, 682, 7098, 723, 48, 7099, 7100, 2513, 4433, 1534, 887, 772, 7101, 7102, 1337, 7103, 1338, 7104, 7105, 34, 150, 127]),\n","       ...,\n","       list([5173, 8, 2340, 3150, 22, 297, 76, 6640, 16198, 420, 3586, 16199, 1531, 164, 16200, 1932, 625, 16201, 469, 133, 1980, 4291, 218, 5065, 16202]),\n","       list([1802, 6909, 716, 5437, 2938, 6613, 350, 16203, 6843, 1802, 5440, 395, 78]),\n","       list([329, 7, 1043, 64, 2180])], dtype=object), array([1, 0, 1, ..., 0, 0, 1], dtype=int32), array([23,  2, 31, ..., 25, 13,  5], dtype=int32)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ga__L9vb-m2","colab_type":"text"},"source":["## Train and Test Model"]},{"cell_type":"code","metadata":{"id":"sO8KQQO1sAd1","colab_type":"code","colab":{}},"source":["!rm -rf ./logs/ \n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiFlayfGcEjD","colab_type":"code","colab":{}},"source":["def main(run_dir,hparams,config):\n","  with tf.Graph().as_default(),tf.compat.v1.Session() as session:\n","\n","      initializer = tf.compat.v1.random_normal_initializer(0, 0.05)\n","\n","      classifier= Classifer(config=config, session=session)\n","      # saver = tf.compat.v1.train.Saver()\n","\n","      # total=0\n","      # #print trainable variables\n","      # for v in tf.compat.v1.trainable_variables():\n","      #     print(v.name)\n","      #     shape=v.get_shape()\n","      #     try:\n","      #         size=shape[0].value*shape[1].value\n","      #     except:\n","      #         size=shape[0].value\n","      #     total+=size\n","      # print(total)\n","      #initialize\n","      init = tf.compat.v1.global_variables_initializer()\n","\n","      session.run(init)\n","      #train test model\n","\n","      # print (\"model_test\",matrix)\n","\n","      word_to_vec(matrix, session,config, classifier)\n","      start_epoches(config, session,classifier, train_dataset)\n","\n","      test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, classifier, test_dataset, tf.no_op(),1, False)\n","      print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))\n","      \n","      with tf.summary.create_file_writer(run_dir).as_default():\n","        print(\"Run directory is : \",run_dir)\n","        hp.hparams(hparams)\n","        tf.summary.scalar(METRIC_ACCURACY, test_acc, step=1)\n","      return test_acc\n","      # # Add ops to save and restore all the variables.\n","      \n","      # save_path = saver.save(session, \"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/500epochs/1/\")\n","      # print(\"Model saved in path: %s\" % save_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVnWr6WaNtsO","colab_type":"text"},"source":["# HParams Tuning"]},{"cell_type":"code","metadata":{"id":"W9F1fVeeGIKR","colab_type":"code","colab":{}},"source":["HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.00001, 0.00002, 0.00003, 0.00004,0.00005]))\n","HP_LR_DECAY = hp.HParam('lr_decay', hp.RealInterval(0.95,0.99))\n","HP_EMBEDDING_TRAINABLE = hp.HParam('embedding_trainable', hp.Discrete([True, False]))\n","HP_RANDOM_INITIALIZE = hp.HParam('random_initialize',hp.Discrete([True]))\n","HP_ATTENTION_ITERATION = hp.HParam('attention_iteration',hp.IntInterval(3,4))\n","HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([8]))\n","HP_KEEP_PROB = hp.HParam('keep_prob', hp.Discrete([0.8]))\n","\n","METRIC_ACCURACY = 'accuracy'\n","\n","with tf.summary.create_file_writer('logs/hparam_tuning/').as_default():\n","  hp.hparams_config(\n","    hparams=[\n","             HP_LEARNING_RATE,HP_LR_DECAY, HP_EMBEDDING_TRAINABLE, \n","             HP_RANDOM_INITIALIZE, HP_ATTENTION_ITERATION, HP_BATCH_SIZE, HP_KEEP_PROB\n","             ],\n","    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJsLGR-HBT-y","colab_type":"code","colab":{}},"source":["# for learning_rate in tf.linspace(HP_ATTENTION_ITERATION.domain.min_value,HP_ATTENTION_ITERATION.domain.max_value,1):\n","#   print(learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jtPO49MSI06","colab_type":"code","colab":{}},"source":["def run(run_dir, hparams,config):\n","  argument1 = \"7\" # slstm iteration\n","  argument2 = \"2\" #num window_size\n","  argument3 = \"sinhala_news\" #dataset_name\n","  argument4 = \"slstm\" #model_name\n","  hparamsNew = hparams\n","  # config = Config()\n","  config.layer=int(argument1)\n","  config.step=int(argument2)\n","  config.max_max_epoch=30\n","  config.model_type = argument4\n","  config.learning_rate=hparams[HP_LEARNING_RATE]\n","  config.lr_decay=hparams[HP_LR_DECAY]\n","  # config.embedding_trainable=hparams[HP_EMBEDDING_TRAINABLE]\n","  config.random_initialize=hparams[HP_RANDOM_INITIALIZE]\n","  config.attention_iteration=hparams[HP_ATTENTION_ITERATION]\n","  config.batch_size=hparams[HP_BATCH_SIZE]\n","  config.keep_prob=hparams[HP_KEEP_PROB]\n","\n","  accuracy = main(run_dir,hparamsNew,config)\n","  return accuracy\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZdFh4CbUCNN","colab_type":"code","outputId":"cff5f29c-557f-4cbd-b3b1-a1f78ea656bb","executionInfo":{"status":"error","timestamp":1591170236620,"user_tz":-330,"elapsed":365942,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["session_num = 12\n","file_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Reports/slstm/hparams_results_1.xlsx'\n","rb = open_workbook(file_path)\n","r_sheet = rb.sheet_by_index(3) # read only copy to introspect the file\n","wb = copy(rb) # a writable copy (I can't read values out of this, only write to it)\n","sheet1 = wb.get_sheet(3) # the sheet to write to within the writable copy\n","sheet1.write(0, 0, 'learning_rate') \n","sheet1.write(0, 1, 'lr_decay') \n","sheet1.write(0, 2, 'random_nitialize') \n","sheet1.write(0, 3, 'attention_iteration') \n","sheet1.write(0, 4, 'batch_size')\n","sheet1.write(0, 5, 'keep_prob')\n","\n","# lr_decay = config.lr_decay\n","random_initialize = config.random_initialize\n","attention_iteration = config.attention_iteration\n","batch_size = config.batch_size\n","keep_prob = config.keep_prob\n","\n","sheet1.write(0, 6, 'accuracy')\n","for learning_rate in HP_LEARNING_RATE.domain.values:\n","  for lr_decay in (HP_LR_DECAY.domain.min_value, HP_LR_DECAY.domain.max_value):\n","    for random_initialize in HP_RANDOM_INITIALIZE.domain.values:\n","      for attention_iteration in (HP_ATTENTION_ITERATION .domain.min_value, HP_ATTENTION_ITERATION.domain.max_value):\n","        for batch_size in HP_BATCH_SIZE.domain.values:\n","          for keep_prob in HP_KEEP_PROB.domain.values:\n","            hparams = {\n","            HP_LEARNING_RATE: learning_rate,\n","            HP_LR_DECAY: lr_decay,\n","            # HP_EMBEDDING_TRAINABLE: embedding_trainable,\n","            HP_RANDOM_INITIALIZE : random_initialize,\n","            HP_ATTENTION_ITERATION : attention_iteration,\n","            HP_BATCH_SIZE:batch_size,\n","            HP_KEEP_PROB:keep_prob\n","            }\n","            run_name = \"run-%d\" % session_num\n","            print('--- Starting trial: %s' % run_name)\n","            print({h.name: hparams[h] for h in hparams})\n","            accuracy = run('logs/hparam_tuning/' + run_name, hparams,config)\n","            session_num += 1\n","\n","            sheet1.write(session_num+1, 0, learning_rate) \n","            sheet1.write(session_num+1, 1, lr_decay) \n","            sheet1.write(session_num+1, 2, random_initialize) \n","            sheet1.write(session_num+1, 3, attention_iteration) \n","            sheet1.write(session_num+1, 4, batch_size)\n","            sheet1.write(session_num+1, 5, keep_prob)\n","            sheet1.write(session_num+1, 6, accuracy )\n","            wb.save(file_path)\n","\n","\n","\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["--- Starting trial: run-12\n","{'learning_rate': 1e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","word2vec shape:  (18415, 300)\n","Total loss:  323.3078143298626\n","Training Accuracy = 0.6290, time = 19.496 seconds\n","\n","Total loss:  292.47181579470634\n","Training Accuracy = 0.7660, time = 14.856 seconds\n","\n","Total loss:  272.0905893445015\n","Training Accuracy = 0.7750, time = 14.797 seconds\n","\n","Total loss:  257.2448667138815\n","Training Accuracy = 0.7809, time = 14.808 seconds\n","\n","Total loss:  245.5293117314577\n","Training Accuracy = 0.7929, time = 14.751 seconds\n","\n","Total loss:  235.87691470980644\n","Training Accuracy = 0.8031, time = 14.774 seconds\n","\n","Total loss:  226.4699205905199\n","Training Accuracy = 0.8141, time = 14.620 seconds\n","\n","Total loss:  219.08898673951626\n","Training Accuracy = 0.8194, time = 14.668 seconds\n","\n","Total loss:  211.30884181708097\n","Training Accuracy = 0.8301, time = 14.589 seconds\n","\n","Total loss:  205.41439397633076\n","Training Accuracy = 0.8411, time = 14.618 seconds\n","\n","Total loss:  198.62060065567493\n","Training Accuracy = 0.8496, time = 14.938 seconds\n","\n","Total loss:  195.15096230059862\n","Training Accuracy = 0.8496, time = 14.831 seconds\n","\n","Total loss:  189.55697579681873\n","Training Accuracy = 0.8580, time = 14.599 seconds\n","\n","Total loss:  184.76180497556925\n","Training Accuracy = 0.8675, time = 14.626 seconds\n","\n","Total loss:  180.1581380814314\n","Training Accuracy = 0.8688, time = 14.675 seconds\n","\n","Total loss:  174.86608338728547\n","Training Accuracy = 0.8710, time = 14.654 seconds\n","\n","Total loss:  172.91762381792068\n","Training Accuracy = 0.8755, time = 14.590 seconds\n","\n","Total loss:  167.8143739439547\n","Training Accuracy = 0.8852, time = 14.559 seconds\n","\n","Total loss:  161.06733886525035\n","Training Accuracy = 0.8867, time = 14.539 seconds\n","\n","Total loss:  160.49119497090578\n","Training Accuracy = 0.8887, time = 14.450 seconds\n","\n","Total loss:  155.1673198416829\n","Training Accuracy = 0.8930, time = 14.582 seconds\n","\n","Total loss:  151.66553635522723\n","Training Accuracy = 0.8917, time = 14.568 seconds\n","\n","Total loss:  149.25899595022202\n","Training Accuracy = 0.8972, time = 14.488 seconds\n","\n","Total loss:  146.52390251122415\n","Training Accuracy = 0.9017, time = 14.490 seconds\n","\n","Total loss:  142.1375193670392\n","Training Accuracy = 0.9037, time = 14.543 seconds\n","\n","Total loss:  140.43792146258056\n","Training Accuracy = 0.9049, time = 14.481 seconds\n","\n","Total loss:  138.01293652877212\n","Training Accuracy = 0.9072, time = 14.540 seconds\n","\n","Total loss:  134.71945798397064\n","Training Accuracy = 0.9107, time = 14.453 seconds\n","\n","Total loss:  131.85922210477293\n","Training Accuracy = 0.9164, time = 14.512 seconds\n","\n","Total loss:  127.73034255206585\n","Training Accuracy = 0.9149, time = 14.452 seconds\n","\n","Total loss:  46.999115190925295\n","Test Accuracy = 0.8832, Test Precision = 0.9010, Test Recall = 0.8716, Test F1 = 0.8861\n","\n","Run directory is :  logs/hparam_tuning/run-12\n","--- Starting trial: run-13\n","{'learning_rate': 1e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  325.72344210743904\n","Training Accuracy = 0.6395, time = 17.792 seconds\n","\n","Total loss:  293.0993600785732\n","Training Accuracy = 0.7418, time = 14.687 seconds\n","\n","Total loss:  272.80519264936447\n","Training Accuracy = 0.7590, time = 14.411 seconds\n","\n","Total loss:  258.0685364603996\n","Training Accuracy = 0.7792, time = 14.432 seconds\n","\n","Total loss:  245.78438605368137\n","Training Accuracy = 0.7884, time = 14.437 seconds\n","\n","Total loss:  235.45689518749714\n","Training Accuracy = 0.8079, time = 14.421 seconds\n","\n","Total loss:  227.36629423499107\n","Training Accuracy = 0.8121, time = 14.433 seconds\n","\n","Total loss:  219.50259405374527\n","Training Accuracy = 0.8263, time = 14.389 seconds\n","\n","Total loss:  211.90337861329317\n","Training Accuracy = 0.8323, time = 14.423 seconds\n","\n","Total loss:  207.79746313393116\n","Training Accuracy = 0.8338, time = 14.393 seconds\n","\n","Total loss:  200.1068601384759\n","Training Accuracy = 0.8481, time = 14.390 seconds\n","\n","Total loss:  194.33002606779337\n","Training Accuracy = 0.8538, time = 14.417 seconds\n","\n","Total loss:  190.1387965604663\n","Training Accuracy = 0.8583, time = 14.392 seconds\n","\n","Total loss:  184.8140114620328\n","Training Accuracy = 0.8603, time = 14.424 seconds\n","\n","Total loss:  180.9949693158269\n","Training Accuracy = 0.8708, time = 14.355 seconds\n","\n","Total loss:  174.46926230937243\n","Training Accuracy = 0.8743, time = 14.536 seconds\n","\n","Total loss:  171.37043248862028\n","Training Accuracy = 0.8748, time = 14.412 seconds\n","\n","Total loss:  166.61219005286694\n","Training Accuracy = 0.8820, time = 14.497 seconds\n","\n","Total loss:  162.1646841838956\n","Training Accuracy = 0.8837, time = 14.589 seconds\n","\n","Total loss:  161.93057319149375\n","Training Accuracy = 0.8845, time = 14.523 seconds\n","\n","Total loss:  158.37598209269345\n","Training Accuracy = 0.8890, time = 14.408 seconds\n","\n","Total loss:  153.44761300832033\n","Training Accuracy = 0.8950, time = 14.420 seconds\n","\n","Total loss:  149.97349698841572\n","Training Accuracy = 0.8990, time = 14.667 seconds\n","\n","Total loss:  145.8284408133477\n","Training Accuracy = 0.9014, time = 14.395 seconds\n","\n","Total loss:  142.18833495303988\n","Training Accuracy = 0.9044, time = 14.376 seconds\n","\n","Total loss:  141.10687691159546\n","Training Accuracy = 0.9037, time = 14.421 seconds\n","\n","Total loss:  137.17329045943916\n","Training Accuracy = 0.9069, time = 14.371 seconds\n","\n","Total loss:  135.39271737076342\n","Training Accuracy = 0.9072, time = 14.411 seconds\n","\n","Total loss:  131.5713937599212\n","Training Accuracy = 0.9114, time = 14.403 seconds\n","\n","Total loss:  128.10502916015685\n","Training Accuracy = 0.9149, time = 14.371 seconds\n","\n","Total loss:  48.853861432471604\n","Test Accuracy = 0.8733, Test Precision = 0.9006, Test Recall = 0.8506, Test F1 = 0.8749\n","\n","Run directory is :  logs/hparam_tuning/run-13\n","--- Starting trial: run-14\n","{'learning_rate': 1e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  330.93841671943665\n","Training Accuracy = 0.6213, time = 17.666 seconds\n","\n","Total loss:  298.13899075984955\n","Training Accuracy = 0.7248, time = 14.542 seconds\n","\n","Total loss:  276.0379549264908\n","Training Accuracy = 0.7532, time = 14.478 seconds\n","\n","Total loss:  259.8987104296684\n","Training Accuracy = 0.7737, time = 14.532 seconds\n","\n","Total loss:  248.34857906401157\n","Training Accuracy = 0.7834, time = 14.448 seconds\n","\n","Total loss:  238.52321937680244\n","Training Accuracy = 0.7977, time = 14.453 seconds\n","\n","Total loss:  229.1407316327095\n","Training Accuracy = 0.8094, time = 14.448 seconds\n","\n","Total loss:  220.46176971495152\n","Training Accuracy = 0.8191, time = 14.460 seconds\n","\n","Total loss:  213.6548068523407\n","Training Accuracy = 0.8236, time = 14.499 seconds\n","\n","Total loss:  208.3163593262434\n","Training Accuracy = 0.8323, time = 14.471 seconds\n","\n","Total loss:  200.61082091927528\n","Training Accuracy = 0.8416, time = 14.535 seconds\n","\n","Total loss:  196.23674884438515\n","Training Accuracy = 0.8500, time = 14.520 seconds\n","\n","Total loss:  189.13762259483337\n","Training Accuracy = 0.8578, time = 14.717 seconds\n","\n","Total loss:  185.40375697612762\n","Training Accuracy = 0.8608, time = 14.495 seconds\n","\n","Total loss:  181.1809872239828\n","Training Accuracy = 0.8633, time = 14.511 seconds\n","\n","Total loss:  177.81406039744616\n","Training Accuracy = 0.8713, time = 14.471 seconds\n","\n","Total loss:  172.93323934823275\n","Training Accuracy = 0.8725, time = 14.458 seconds\n","\n","Total loss:  168.38605911284685\n","Training Accuracy = 0.8782, time = 14.458 seconds\n","\n","Total loss:  163.78647731617093\n","Training Accuracy = 0.8857, time = 14.476 seconds\n","\n","Total loss:  160.83006717264652\n","Training Accuracy = 0.8855, time = 14.549 seconds\n","\n","Total loss:  156.6147234775126\n","Training Accuracy = 0.8915, time = 14.487 seconds\n","\n","Total loss:  152.64083882793784\n","Training Accuracy = 0.8955, time = 14.495 seconds\n","\n","Total loss:  150.31412528455257\n","Training Accuracy = 0.8952, time = 14.514 seconds\n","\n","Total loss:  148.5226879529655\n","Training Accuracy = 0.8930, time = 14.495 seconds\n","\n","Total loss:  144.73625656962395\n","Training Accuracy = 0.9000, time = 14.537 seconds\n","\n","Total loss:  142.07914224639535\n","Training Accuracy = 0.9019, time = 14.560 seconds\n","\n","Total loss:  138.14473754912615\n","Training Accuracy = 0.9052, time = 14.503 seconds\n","\n","Total loss:  135.77191096358\n","Training Accuracy = 0.9094, time = 14.547 seconds\n","\n","Total loss:  133.18755042925477\n","Training Accuracy = 0.9107, time = 14.484 seconds\n","\n","Total loss:  128.8509636670351\n","Training Accuracy = 0.9147, time = 14.516 seconds\n","\n","Total loss:  47.69324324180343\n","Test Accuracy = 0.8752, Test Precision = 0.8994, Test Recall = 0.8563, Test F1 = 0.8773\n","\n","Run directory is :  logs/hparam_tuning/run-14\n","--- Starting trial: run-15\n","{'learning_rate': 1e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  340.300361007452\n","Training Accuracy = 0.5469, time = 17.692 seconds\n","\n","Total loss:  301.04963091015816\n","Training Accuracy = 0.6734, time = 14.341 seconds\n","\n","Total loss:  277.04949775338173\n","Training Accuracy = 0.7942, time = 14.535 seconds\n","\n","Total loss:  259.71457555890083\n","Training Accuracy = 0.8104, time = 14.327 seconds\n","\n","Total loss:  245.56614741683006\n","Training Accuracy = 0.8146, time = 14.403 seconds\n","\n","Total loss:  236.18564499914646\n","Training Accuracy = 0.8156, time = 14.355 seconds\n","\n","Total loss:  226.78523868322372\n","Training Accuracy = 0.8224, time = 14.330 seconds\n","\n","Total loss:  218.15978549420834\n","Training Accuracy = 0.8311, time = 14.309 seconds\n","\n","Total loss:  212.48989003896713\n","Training Accuracy = 0.8303, time = 14.356 seconds\n","\n","Total loss:  206.0135958492756\n","Training Accuracy = 0.8441, time = 14.342 seconds\n","\n","Total loss:  199.29418597370386\n","Training Accuracy = 0.8468, time = 14.361 seconds\n","\n","Total loss:  194.00331681966782\n","Training Accuracy = 0.8563, time = 14.341 seconds\n","\n","Total loss:  188.72610700130463\n","Training Accuracy = 0.8620, time = 14.338 seconds\n","\n","Total loss:  185.5148700028658\n","Training Accuracy = 0.8623, time = 14.347 seconds\n","\n","Total loss:  180.0319054722786\n","Training Accuracy = 0.8715, time = 14.298 seconds\n","\n","Total loss:  175.4331282749772\n","Training Accuracy = 0.8765, time = 14.348 seconds\n","\n","Total loss:  171.16966161876917\n","Training Accuracy = 0.8810, time = 14.411 seconds\n","\n","Total loss:  168.02792623639107\n","Training Accuracy = 0.8827, time = 14.362 seconds\n","\n","Total loss:  163.90184330195189\n","Training Accuracy = 0.8885, time = 14.321 seconds\n","\n","Total loss:  159.79168536514044\n","Training Accuracy = 0.8942, time = 14.358 seconds\n","\n","Total loss:  155.552291482687\n","Training Accuracy = 0.8965, time = 14.310 seconds\n","\n","Total loss:  153.38070106878877\n","Training Accuracy = 0.8960, time = 14.513 seconds\n","\n","Total loss:  150.01556086540222\n","Training Accuracy = 0.8970, time = 14.336 seconds\n","\n","Total loss:  147.372782189399\n","Training Accuracy = 0.9027, time = 14.456 seconds\n","\n","Total loss:  143.51232881471515\n","Training Accuracy = 0.9057, time = 14.491 seconds\n","\n","Total loss:  141.02558640390635\n","Training Accuracy = 0.9059, time = 14.354 seconds\n","\n","Total loss:  137.3367967531085\n","Training Accuracy = 0.9122, time = 14.403 seconds\n","\n","Total loss:  135.71550483629107\n","Training Accuracy = 0.9089, time = 14.380 seconds\n","\n","Total loss:  131.40940541401505\n","Training Accuracy = 0.9144, time = 14.355 seconds\n","\n","Total loss:  129.590232886374\n","Training Accuracy = 0.9174, time = 14.372 seconds\n","\n","Total loss:  47.0830250898689\n","Test Accuracy = 0.8812, Test Precision = 0.9006, Test Recall = 0.8678, Test F1 = 0.8839\n","\n","Run directory is :  logs/hparam_tuning/run-15\n","--- Starting trial: run-16\n","{'learning_rate': 2e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  307.52661073207855\n","Training Accuracy = 0.7058, time = 17.786 seconds\n","\n","Total loss:  265.95713806152344\n","Training Accuracy = 0.7799, time = 14.552 seconds\n","\n","Total loss:  241.12119090557098\n","Training Accuracy = 0.8006, time = 14.552 seconds\n","\n","Total loss:  224.218838006258\n","Training Accuracy = 0.8154, time = 14.450 seconds\n","\n","Total loss:  210.48352923244238\n","Training Accuracy = 0.8331, time = 14.695 seconds\n","\n","Total loss:  199.22333436459303\n","Training Accuracy = 0.8478, time = 14.612 seconds\n","\n","Total loss:  188.17262642085552\n","Training Accuracy = 0.8585, time = 14.510 seconds\n","\n","Total loss:  179.58708760142326\n","Training Accuracy = 0.8698, time = 14.806 seconds\n","\n","Total loss:  170.94984206557274\n","Training Accuracy = 0.8762, time = 14.508 seconds\n","\n","Total loss:  164.47567347809672\n","Training Accuracy = 0.8860, time = 14.487 seconds\n","\n","Total loss:  156.75406459718943\n","Training Accuracy = 0.8960, time = 14.672 seconds\n","\n","Total loss:  149.78105938248336\n","Training Accuracy = 0.8992, time = 15.163 seconds\n","\n","Total loss:  144.9396711345762\n","Training Accuracy = 0.8985, time = 15.151 seconds\n","\n","Total loss:  138.79838418588042\n","Training Accuracy = 0.9107, time = 15.213 seconds\n","\n","Total loss:  133.4858552441001\n","Training Accuracy = 0.9077, time = 14.915 seconds\n","\n","Total loss:  127.79601534456015\n","Training Accuracy = 0.9147, time = 14.581 seconds\n","\n","Total loss:  122.55151642858982\n","Training Accuracy = 0.9162, time = 14.759 seconds\n","\n","Total loss:  116.5449866997078\n","Training Accuracy = 0.9244, time = 15.062 seconds\n","\n","Total loss:  112.89617924112827\n","Training Accuracy = 0.9264, time = 15.131 seconds\n","\n","Total loss:  107.63232826441526\n","Training Accuracy = 0.9279, time = 14.889 seconds\n","\n","Total loss:  102.6980764400214\n","Training Accuracy = 0.9371, time = 15.179 seconds\n","\n","Total loss:  99.81144067272544\n","Training Accuracy = 0.9369, time = 15.308 seconds\n","\n","Total loss:  97.53734591277316\n","Training Accuracy = 0.9386, time = 15.434 seconds\n","\n","Total loss:  91.66192564321682\n","Training Accuracy = 0.9414, time = 15.266 seconds\n","\n","Total loss:  88.31513933697715\n","Training Accuracy = 0.9446, time = 15.322 seconds\n","\n","Total loss:  83.6416682349518\n","Training Accuracy = 0.9513, time = 15.201 seconds\n","\n","Total loss:  79.8508320900728\n","Training Accuracy = 0.9516, time = 15.166 seconds\n","\n","Total loss:  76.62365471292287\n","Training Accuracy = 0.9568, time = 15.217 seconds\n","\n","Total loss:  73.46284262521658\n","Training Accuracy = 0.9573, time = 15.222 seconds\n","\n","Total loss:  70.44698417905602\n","Training Accuracy = 0.9606, time = 15.125 seconds\n","\n","Total loss:  52.78193442337215\n","Test Accuracy = 0.8802, Test Precision = 0.9069, Test Recall = 0.8582, Test F1 = 0.8819\n","\n","Run directory is :  logs/hparam_tuning/run-16\n","--- Starting trial: run-17\n","{'learning_rate': 2e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  313.70552456378937\n","Training Accuracy = 0.6697, time = 18.910 seconds\n","\n","Total loss:  266.4679083228111\n","Training Accuracy = 0.7929, time = 14.505 seconds\n","\n","Total loss:  242.61232167482376\n","Training Accuracy = 0.8006, time = 14.550 seconds\n","\n","Total loss:  225.98588556051254\n","Training Accuracy = 0.8119, time = 14.749 seconds\n","\n","Total loss:  211.98261912167072\n","Training Accuracy = 0.8306, time = 14.443 seconds\n","\n","Total loss:  199.99498341232538\n","Training Accuracy = 0.8498, time = 14.419 seconds\n","\n","Total loss:  190.45093750208616\n","Training Accuracy = 0.8593, time = 14.410 seconds\n","\n","Total loss:  182.4945201650262\n","Training Accuracy = 0.8643, time = 14.477 seconds\n","\n","Total loss:  172.61317685991526\n","Training Accuracy = 0.8743, time = 14.426 seconds\n","\n","Total loss:  167.84394326806068\n","Training Accuracy = 0.8785, time = 14.361 seconds\n","\n","Total loss:  159.307715062052\n","Training Accuracy = 0.8847, time = 14.419 seconds\n","\n","Total loss:  151.71376967430115\n","Training Accuracy = 0.8900, time = 14.372 seconds\n","\n","Total loss:  147.25102496892214\n","Training Accuracy = 0.8965, time = 14.394 seconds\n","\n","Total loss:  138.79727915674448\n","Training Accuracy = 0.9057, time = 14.423 seconds\n","\n","Total loss:  133.3021066058427\n","Training Accuracy = 0.9112, time = 14.434 seconds\n","\n","Total loss:  128.0119280256331\n","Training Accuracy = 0.9117, time = 14.421 seconds\n","\n","Total loss:  122.80783221125603\n","Training Accuracy = 0.9209, time = 14.419 seconds\n","\n","Total loss:  117.33019259013236\n","Training Accuracy = 0.9234, time = 14.373 seconds\n","\n","Total loss:  110.76503262482584\n","Training Accuracy = 0.9271, time = 14.433 seconds\n","\n","Total loss:  105.748559855856\n","Training Accuracy = 0.9319, time = 14.483 seconds\n","\n","Total loss:  101.85496990848333\n","Training Accuracy = 0.9344, time = 14.457 seconds\n","\n","Total loss:  97.9953703051433\n","Training Accuracy = 0.9391, time = 14.438 seconds\n","\n","Total loss:  92.58317812113091\n","Training Accuracy = 0.9419, time = 14.405 seconds\n","\n","Total loss:  89.48429669986945\n","Training Accuracy = 0.9406, time = 14.576 seconds\n","\n","Total loss:  88.15946307359263\n","Training Accuracy = 0.9469, time = 15.491 seconds\n","\n","Total loss:  81.41963958879933\n","Training Accuracy = 0.9523, time = 15.116 seconds\n","\n","Total loss:  78.81403371249326\n","Training Accuracy = 0.9526, time = 15.156 seconds\n","\n","Total loss:  75.04539530328475\n","Training Accuracy = 0.9571, time = 15.163 seconds\n","\n","Total loss:  70.4646496779169\n","Training Accuracy = 0.9566, time = 15.082 seconds\n","\n","Total loss:  68.67738278058823\n","Training Accuracy = 0.9613, time = 15.123 seconds\n","\n","Total loss:  51.76721461908892\n","Test Accuracy = 0.8882, Test Precision = 0.9100, Test Recall = 0.8716, Test F1 = 0.8904\n","\n","Run directory is :  logs/hparam_tuning/run-17\n","--- Starting trial: run-18\n","{'learning_rate': 2e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  303.6451621353626\n","Training Accuracy = 0.7295, time = 19.207 seconds\n","\n","Total loss:  262.55310682952404\n","Training Accuracy = 0.7834, time = 15.153 seconds\n","\n","Total loss:  240.34084735810757\n","Training Accuracy = 0.7984, time = 15.217 seconds\n","\n","Total loss:  224.6200675368309\n","Training Accuracy = 0.8179, time = 15.274 seconds\n","\n","Total loss:  210.46486324071884\n","Training Accuracy = 0.8351, time = 15.284 seconds\n","\n","Total loss:  199.89175909757614\n","Training Accuracy = 0.8466, time = 15.014 seconds\n","\n","Total loss:  189.19794041663408\n","Training Accuracy = 0.8573, time = 14.584 seconds\n","\n","Total loss:  180.46681604906917\n","Training Accuracy = 0.8695, time = 14.466 seconds\n","\n","Total loss:  172.0907352603972\n","Training Accuracy = 0.8757, time = 14.515 seconds\n","\n","Total loss:  165.83793715015054\n","Training Accuracy = 0.8847, time = 14.587 seconds\n","\n","Total loss:  157.80042957514524\n","Training Accuracy = 0.8897, time = 14.542 seconds\n","\n","Total loss:  151.78232423961163\n","Training Accuracy = 0.8912, time = 14.496 seconds\n","\n","Total loss:  143.28729890286922\n","Training Accuracy = 0.9044, time = 14.476 seconds\n","\n","Total loss:  138.512090254575\n","Training Accuracy = 0.9049, time = 14.706 seconds\n","\n","Total loss:  132.81450520828366\n","Training Accuracy = 0.9114, time = 14.555 seconds\n","\n","Total loss:  125.7724727652967\n","Training Accuracy = 0.9212, time = 14.434 seconds\n","\n","Total loss:  122.72038312070072\n","Training Accuracy = 0.9199, time = 14.435 seconds\n","\n","Total loss:  116.63686375226825\n","Training Accuracy = 0.9256, time = 14.420 seconds\n","\n","Total loss:  111.91209561191499\n","Training Accuracy = 0.9271, time = 14.471 seconds\n","\n","Total loss:  107.87534534512088\n","Training Accuracy = 0.9301, time = 14.386 seconds\n","\n","Total loss:  102.53458153735846\n","Training Accuracy = 0.9324, time = 14.530 seconds\n","\n","Total loss:  98.5724426144734\n","Training Accuracy = 0.9401, time = 14.491 seconds\n","\n","Total loss:  92.86090418323874\n","Training Accuracy = 0.9389, time = 14.422 seconds\n","\n","Total loss:  88.35579663538374\n","Training Accuracy = 0.9466, time = 14.441 seconds\n","\n","Total loss:  84.58707410492934\n","Training Accuracy = 0.9459, time = 14.482 seconds\n","\n","Total loss:  81.65146016515791\n","Training Accuracy = 0.9486, time = 14.468 seconds\n","\n","Total loss:  78.14919359842315\n","Training Accuracy = 0.9561, time = 14.460 seconds\n","\n","Total loss:  73.6259917622665\n","Training Accuracy = 0.9546, time = 14.437 seconds\n","\n","Total loss:  70.97539441892877\n","Training Accuracy = 0.9563, time = 14.425 seconds\n","\n","Total loss:  67.51384788448922\n","Training Accuracy = 0.9596, time = 14.425 seconds\n","\n","Total loss:  53.7132177320309\n","Test Accuracy = 0.8882, Test Precision = 0.9236, Test Recall = 0.8563, Test F1 = 0.8887\n","\n","Run directory is :  logs/hparam_tuning/run-18\n","--- Starting trial: run-19\n","{'learning_rate': 2e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  310.3060836195946\n","Training Accuracy = 0.7098, time = 17.952 seconds\n","\n","Total loss:  265.62050695717335\n","Training Accuracy = 0.7730, time = 14.554 seconds\n","\n","Total loss:  241.43452282249928\n","Training Accuracy = 0.7937, time = 14.511 seconds\n","\n","Total loss:  224.7946941256523\n","Training Accuracy = 0.8171, time = 14.623 seconds\n","\n","Total loss:  211.55560155957937\n","Training Accuracy = 0.8308, time = 14.627 seconds\n","\n","Total loss:  197.54711186885834\n","Training Accuracy = 0.8498, time = 14.565 seconds\n","\n","Total loss:  188.6489765867591\n","Training Accuracy = 0.8593, time = 14.546 seconds\n","\n","Total loss:  178.42355993762612\n","Training Accuracy = 0.8713, time = 14.490 seconds\n","\n","Total loss:  170.16319681704044\n","Training Accuracy = 0.8785, time = 14.536 seconds\n","\n","Total loss:  165.24365719407797\n","Training Accuracy = 0.8805, time = 14.656 seconds\n","\n","Total loss:  159.74761732295156\n","Training Accuracy = 0.8850, time = 14.641 seconds\n","\n","Total loss:  151.25635427981615\n","Training Accuracy = 0.8937, time = 14.796 seconds\n","\n","Total loss:  144.5404206365347\n","Training Accuracy = 0.8997, time = 14.548 seconds\n","\n","Total loss:  139.17449313402176\n","Training Accuracy = 0.9047, time = 14.547 seconds\n","\n","Total loss:  135.11868369951844\n","Training Accuracy = 0.9079, time = 14.578 seconds\n","\n","Total loss:  127.70884613879025\n","Training Accuracy = 0.9179, time = 14.579 seconds\n","\n","Total loss:  120.80802686465904\n","Training Accuracy = 0.9242, time = 14.622 seconds\n","\n","Total loss:  117.86480790749192\n","Training Accuracy = 0.9199, time = 14.490 seconds\n","\n","Total loss:  114.13180829770863\n","Training Accuracy = 0.9281, time = 14.570 seconds\n","\n","Total loss:  106.27636834280565\n","Training Accuracy = 0.9331, time = 14.514 seconds\n","\n","Total loss:  103.95788998459466\n","Training Accuracy = 0.9329, time = 14.655 seconds\n","\n","Total loss:  99.08545927098021\n","Training Accuracy = 0.9359, time = 14.575 seconds\n","\n","Total loss:  93.94162177224644\n","Training Accuracy = 0.9411, time = 14.611 seconds\n","\n","Total loss:  89.11367284250446\n","Training Accuracy = 0.9426, time = 14.540 seconds\n","\n","Total loss:  85.97523220279254\n","Training Accuracy = 0.9474, time = 14.700 seconds\n","\n","Total loss:  83.0916416803957\n","Training Accuracy = 0.9496, time = 14.606 seconds\n","\n","Total loss:  81.38483953784453\n","Training Accuracy = 0.9466, time = 14.513 seconds\n","\n","Total loss:  76.59296900063055\n","Training Accuracy = 0.9523, time = 14.785 seconds\n","\n","Total loss:  73.51158026629128\n","Training Accuracy = 0.9556, time = 14.534 seconds\n","\n","Total loss:  68.53871539203101\n","Training Accuracy = 0.9576, time = 14.694 seconds\n","\n","Total loss:  53.48652865178883\n","Test Accuracy = 0.8792, Test Precision = 0.9084, Test Recall = 0.8544, Test F1 = 0.8806\n","\n","Run directory is :  logs/hparam_tuning/run-19\n","--- Starting trial: run-20\n","{'learning_rate': 3e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  311.8801140487194\n","Training Accuracy = 0.6682, time = 17.695 seconds\n","\n","Total loss:  254.61465893685818\n","Training Accuracy = 0.7799, time = 14.469 seconds\n","\n","Total loss:  228.36619643867016\n","Training Accuracy = 0.8071, time = 14.423 seconds\n","\n","Total loss:  207.46148009598255\n","Training Accuracy = 0.8321, time = 14.359 seconds\n","\n","Total loss:  192.85077687352896\n","Training Accuracy = 0.8476, time = 14.373 seconds\n","\n","Total loss:  177.97179036587477\n","Training Accuracy = 0.8690, time = 14.566 seconds\n","\n","Total loss:  166.57534958422184\n","Training Accuracy = 0.8810, time = 14.357 seconds\n","\n","Total loss:  156.13231988623738\n","Training Accuracy = 0.8930, time = 14.414 seconds\n","\n","Total loss:  145.76010408997536\n","Training Accuracy = 0.8995, time = 14.356 seconds\n","\n","Total loss:  136.86797695979476\n","Training Accuracy = 0.9084, time = 14.396 seconds\n","\n","Total loss:  128.4458795711398\n","Training Accuracy = 0.9117, time = 14.515 seconds\n","\n","Total loss:  119.99759790953249\n","Training Accuracy = 0.9209, time = 14.541 seconds\n","\n","Total loss:  112.11968656256795\n","Training Accuracy = 0.9256, time = 15.169 seconds\n","\n","Total loss:  104.07210599351674\n","Training Accuracy = 0.9346, time = 15.176 seconds\n","\n","Total loss:  96.17808944731951\n","Training Accuracy = 0.9411, time = 15.309 seconds\n","\n","Total loss:  90.44639935344458\n","Training Accuracy = 0.9394, time = 15.205 seconds\n","\n","Total loss:  85.63465692452155\n","Training Accuracy = 0.9471, time = 15.107 seconds\n","\n","Total loss:  78.13233785517514\n","Training Accuracy = 0.9538, time = 14.599 seconds\n","\n","Total loss:  75.24885108694434\n","Training Accuracy = 0.9531, time = 14.420 seconds\n","\n","Total loss:  70.22660421708133\n","Training Accuracy = 0.9571, time = 14.363 seconds\n","\n","Total loss:  63.73819667019416\n","Training Accuracy = 0.9636, time = 14.378 seconds\n","\n","Total loss:  59.65546927367541\n","Training Accuracy = 0.9683, time = 14.370 seconds\n","\n","Total loss:  55.883044423448155\n","Training Accuracy = 0.9698, time = 14.373 seconds\n","\n","Total loss:  52.14936121713254\n","Training Accuracy = 0.9698, time = 14.456 seconds\n","\n","Total loss:  48.83565348848788\n","Training Accuracy = 0.9713, time = 15.049 seconds\n","\n","Total loss:  45.12627336521837\n","Training Accuracy = 0.9750, time = 15.112 seconds\n","\n","Total loss:  44.91759547003312\n","Training Accuracy = 0.9741, time = 15.106 seconds\n","\n","Total loss:  39.26105082849244\n","Training Accuracy = 0.9795, time = 15.093 seconds\n","\n","Total loss:  38.184174366353545\n","Training Accuracy = 0.9810, time = 15.164 seconds\n","\n","Total loss:  34.30569002712946\n","Training Accuracy = 0.9830, time = 14.546 seconds\n","\n","Total loss:  75.61040294123814\n","Test Accuracy = 0.8723, Test Precision = 0.8988, Test Recall = 0.8506, Test F1 = 0.8740\n","\n","Run directory is :  logs/hparam_tuning/run-20\n","--- Starting trial: run-21\n","{'learning_rate': 3e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  315.9239572286606\n","Training Accuracy = 0.6589, time = 17.698 seconds\n","\n","Total loss:  259.62970815598965\n","Training Accuracy = 0.7720, time = 14.394 seconds\n","\n","Total loss:  231.46635377407074\n","Training Accuracy = 0.7974, time = 14.392 seconds\n","\n","Total loss:  210.6159974783659\n","Training Accuracy = 0.8288, time = 14.442 seconds\n","\n","Total loss:  192.74257189780474\n","Training Accuracy = 0.8500, time = 14.651 seconds\n","\n","Total loss:  179.16724595427513\n","Training Accuracy = 0.8678, time = 14.447 seconds\n","\n","Total loss:  166.329306460917\n","Training Accuracy = 0.8822, time = 14.408 seconds\n","\n","Total loss:  154.765377253294\n","Training Accuracy = 0.8925, time = 14.445 seconds\n","\n","Total loss:  145.85988957807422\n","Training Accuracy = 0.8990, time = 14.476 seconds\n","\n","Total loss:  135.43762115761638\n","Training Accuracy = 0.9094, time = 14.470 seconds\n","\n","Total loss:  128.526943648234\n","Training Accuracy = 0.9149, time = 14.371 seconds\n","\n","Total loss:  119.13563354220241\n","Training Accuracy = 0.9212, time = 14.394 seconds\n","\n","Total loss:  111.27914503589272\n","Training Accuracy = 0.9259, time = 14.428 seconds\n","\n","Total loss:  105.30672490410507\n","Training Accuracy = 0.9346, time = 14.674 seconds\n","\n","Total loss:  99.68378199264407\n","Training Accuracy = 0.9381, time = 14.521 seconds\n","\n","Total loss:  91.82081968104467\n","Training Accuracy = 0.9399, time = 14.369 seconds\n","\n","Total loss:  86.51838423172012\n","Training Accuracy = 0.9469, time = 14.386 seconds\n","\n","Total loss:  81.29798845062032\n","Training Accuracy = 0.9494, time = 14.415 seconds\n","\n","Total loss:  75.41697970870882\n","Training Accuracy = 0.9573, time = 14.630 seconds\n","\n","Total loss:  72.90029574447544\n","Training Accuracy = 0.9566, time = 14.401 seconds\n","\n","Total loss:  67.45697868493153\n","Training Accuracy = 0.9578, time = 14.509 seconds\n","\n","Total loss:  62.27902274846565\n","Training Accuracy = 0.9636, time = 14.402 seconds\n","\n","Total loss:  58.71493283318705\n","Training Accuracy = 0.9671, time = 14.382 seconds\n","\n","Total loss:  56.168711540085496\n","Training Accuracy = 0.9668, time = 14.367 seconds\n","\n","Total loss:  53.067850065475795\n","Training Accuracy = 0.9683, time = 15.012 seconds\n","\n","Total loss:  47.87843329855241\n","Training Accuracy = 0.9750, time = 15.598 seconds\n","\n","Total loss:  44.37516849882377\n","Training Accuracy = 0.9770, time = 15.242 seconds\n","\n","Total loss:  41.70969892588619\n","Training Accuracy = 0.9770, time = 15.199 seconds\n","\n","Total loss:  38.83491968110684\n","Training Accuracy = 0.9813, time = 14.704 seconds\n","\n","Total loss:  36.289270383291296\n","Training Accuracy = 0.9803, time = 14.629 seconds\n","\n","Total loss:  73.51877792511004\n","Test Accuracy = 0.8752, Test Precision = 0.8931, Test Recall = 0.8640, Test F1 = 0.8783\n","\n","Run directory is :  logs/hparam_tuning/run-21\n","--- Starting trial: run-22\n","{'learning_rate': 3e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  296.1596437692642\n","Training Accuracy = 0.7071, time = 17.892 seconds\n","\n","Total loss:  247.9528215676546\n","Training Accuracy = 0.7824, time = 14.566 seconds\n","\n","Total loss:  223.09258617460728\n","Training Accuracy = 0.8174, time = 14.646 seconds\n","\n","Total loss:  204.6259410083294\n","Training Accuracy = 0.8438, time = 14.525 seconds\n","\n","Total loss:  189.08630342036486\n","Training Accuracy = 0.8585, time = 14.586 seconds\n","\n","Total loss:  175.34830677509308\n","Training Accuracy = 0.8752, time = 14.556 seconds\n","\n","Total loss:  166.68954429775476\n","Training Accuracy = 0.8835, time = 14.567 seconds\n","\n","Total loss:  153.78655732050538\n","Training Accuracy = 0.8937, time = 14.539 seconds\n","\n","Total loss:  143.73724442906678\n","Training Accuracy = 0.8982, time = 14.574 seconds\n","\n","Total loss:  134.54684251360595\n","Training Accuracy = 0.9082, time = 14.642 seconds\n","\n","Total loss:  127.94888917170465\n","Training Accuracy = 0.9109, time = 14.619 seconds\n","\n","Total loss:  119.06721896864474\n","Training Accuracy = 0.9219, time = 14.576 seconds\n","\n","Total loss:  110.44952597562224\n","Training Accuracy = 0.9274, time = 14.514 seconds\n","\n","Total loss:  102.89840469136834\n","Training Accuracy = 0.9294, time = 14.508 seconds\n","\n","Total loss:  97.62789606675506\n","Training Accuracy = 0.9381, time = 14.649 seconds\n","\n","Total loss:  91.80843819212168\n","Training Accuracy = 0.9424, time = 14.873 seconds\n","\n","Total loss:  86.26282408693805\n","Training Accuracy = 0.9434, time = 14.751 seconds\n","\n","Total loss:  79.81850497843698\n","Training Accuracy = 0.9506, time = 14.508 seconds\n","\n","Total loss:  74.87121703947196\n","Training Accuracy = 0.9548, time = 14.541 seconds\n","\n","Total loss:  70.32634103507735\n","Training Accuracy = 0.9581, time = 14.536 seconds\n","\n","Total loss:  65.95398017269326\n","Training Accuracy = 0.9603, time = 14.677 seconds\n","\n","Total loss:  62.671853058447596\n","Training Accuracy = 0.9613, time = 14.566 seconds\n","\n","Total loss:  56.294145848209155\n","Training Accuracy = 0.9651, time = 14.524 seconds\n","\n","Total loss:  53.37729786946147\n","Training Accuracy = 0.9683, time = 14.509 seconds\n","\n","Total loss:  48.86126706690993\n","Training Accuracy = 0.9713, time = 14.542 seconds\n","\n","Total loss:  47.249507594053284\n","Training Accuracy = 0.9723, time = 14.586 seconds\n","\n","Total loss:  42.71393778924539\n","Training Accuracy = 0.9768, time = 15.261 seconds\n","\n","Total loss:  40.441672189208475\n","Training Accuracy = 0.9775, time = 15.277 seconds\n","\n","Total loss:  38.74845476581845\n","Training Accuracy = 0.9778, time = 15.236 seconds\n","\n","Total loss:  35.04553099448458\n","Training Accuracy = 0.9810, time = 15.291 seconds\n","\n","Total loss:  69.12356456986163\n","Test Accuracy = 0.8872, Test Precision = 0.9199, Test Recall = 0.8582, Test F1 = 0.8880\n","\n","Run directory is :  logs/hparam_tuning/run-22\n","--- Starting trial: run-23\n","{'learning_rate': 3e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  299.0708113014698\n","Training Accuracy = 0.7265, time = 18.888 seconds\n","\n","Total loss:  250.74625566601753\n","Training Accuracy = 0.7827, time = 14.566 seconds\n","\n","Total loss:  225.3420649394393\n","Training Accuracy = 0.8151, time = 14.566 seconds\n","\n","Total loss:  204.03102377057076\n","Training Accuracy = 0.8421, time = 14.518 seconds\n","\n","Total loss:  189.44576913118362\n","Training Accuracy = 0.8558, time = 14.540 seconds\n","\n","Total loss:  177.12041230127215\n","Training Accuracy = 0.8725, time = 14.792 seconds\n","\n","Total loss:  164.32191338390112\n","Training Accuracy = 0.8850, time = 14.543 seconds\n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-57dfb9fa78e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Starting trial: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/hparam_tuning/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-7e93b025fff8>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams, config)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHP_KEEP_PROB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhparamsNew\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-12306ac4a1f7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(run_dir, hparams, config)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mword_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mstart_epoches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_actual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-3fba45b3ff00>\u001b[0m in \u001b[0;36mstart_epoches\u001b[0;34m(config, session, classifier, train_dataset)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_max_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;31m# all_actual.extend(test_actual)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-3fba45b3ff00>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config, i, session, model, train_dataset)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy = %.4f, time = %.3f seconds\\n\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-3fba45b3ff00>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(session, config, model, data, eval_op, keep_prob, is_training)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_print\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_print\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}