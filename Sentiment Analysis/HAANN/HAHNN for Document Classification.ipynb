{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HAHNN for Document Classification.ipynb","provenance":[{"file_id":"1LH7xLroO6QWO9dC6Hipn7xHYxVchJiUt","timestamp":1591293377462}],"collapsed_sections":["dzZHZGMH4X5-","4_pgmoOw4gkH","IFyTXuEK9TJ3","5exIp4egmwwA","JLle9TkY1T_X","voCjs6ewiSoF","QTR1_43CqGSW","Iu_O63OD2h8P","724247Q4phPE","qsTKHWjKqXmo","lmmw7Qfvqx-f","D9Zcz1wv0wf3","qOZg-uB4HV1H"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EjNTyCbkZcKM","colab_type":"text"},"source":["# Hierarchical Attentional Hybrid Neural Networks for Document Classification\n","\n","Document classification is a challenging task with important applications. Deep learning approaches to the problem have gained much attention. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting dependent importance of words and sentences. In this paper, we propose a new approach based on convolutional neural networks, gated recurrent units and attention mechanisms for document classification tasks. The datasets IMDB Movie Reviews and Yelp were used in experiments. The proposed method improves the results of current attention-based approaches\n","\n","Please, cite:\n","\n","Abreu, J., Fred, L., MacÃªdo, D., & Zanchettin, C. (2019). Hierarchical Attentional Hybrid Neural Networks for Document Classification. arXiv preprint arXiv:1901.06610."]},{"cell_type":"code","metadata":{"id":"K3T5VpzOfqkC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1595773428841,"user_tz":-330,"elapsed":1424,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"5d95491c-b099-4b35-f1b8-c302546032da"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W30qJdK_rHIl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595773429791,"user_tz":-330,"elapsed":2336,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/kaggle\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"bh4bfoO1oiyw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1595773433809,"user_tz":-330,"elapsed":6326,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"0dee8dbb-ee80-465e-9b70-2f95b3ad8d56"},"source":["% cd /content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/HAANN/datasets\n","!ls\n","\n","folder_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/'\n","HANN_path = folder_path + 'Sentiment Analysis/HAANN/'\n","datasets_path = HANN_path + 'datasets/'\n","word_embedding_path = HANN_path + 'word_embedding/'\n","fasttext_path = word_embedding_path + 'fasttext/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/HAANN/datasets\n","hahnn-for-document-classification.zip  yelp_reviews.json\n","imdb_reviews.csv\t\t       yelp_reviews_sampling.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dzZHZGMH4X5-","colab_type":"text"},"source":["# Prompts for kaggle username and API key\n","\n","Displays a field in prompt and waits for the user to input your kaggle username. By press ENTER, other field prompts for input Kaggle API key."]},{"cell_type":"code","metadata":{"id":"J4z5K6u43ey_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595773433810,"user_tz":-330,"elapsed":6294,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["# import os\n","# from getpass import getpass\n","\n","# user = getpass('Kaggle Username: ')\n","# key = getpass('Kaggle API key: ')\n","\n","# if '.kaggle' not in os.listdir('/root'):\n","#     !mkdir ~/.kaggle\n","    \n","# !touch /root/.kaggle/kaggle.json\n","# !chmod 666 /root/.kaggle/kaggle.json\n","\n","# with open('/root/.kaggle/kaggle.json', 'w') as f:\n","#     f.write('{\"username\":\"%s\",\"key\":\"%s\"}' % (user, key))\n","# !chmod 600 /root/.kaggle/kaggle.json"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_pgmoOw4gkH","colab_type":"text"},"source":["# Download dataset"]},{"cell_type":"code","metadata":{"id":"n-mnEILI3hRR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595773433812,"user_tz":-330,"elapsed":6279,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["# !kaggle datasets download -d luisfredgs/hahnn-for-document-classification\n","# !unzip -o hahnn-for-document-classification.zip\n","# # fixing in 400k reviews\n","# !head -n400000 yelp_reviews.json > yelp_reviews_sampling.json"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDjwzhaf59uB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595773433814,"user_tz":-330,"elapsed":6260,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["# !kaggle datasets download -d luisfredgs/wiki-news-300d-1m-subword\n","# !unzip -o wiki-news-300d-1m-subword.zip"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"GOqzxs-ynuCb","colab_type":"code","trusted":true,"cellView":"form","colab":{},"executionInfo":{"status":"ok","timestamp":1595773433816,"user_tz":-330,"elapsed":6243,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["dataset = \"imdb\" #@param [\"yelp\", \"imdb\"]\n","\n","word_embedding_type = \"pre_trained\" #@param [\"from_scratch\", \"pre_trained\"]\n","word_vector_model = \"fasttext\" #@param [\"fasttext\"]\n","rnn_type = \"GRU\" #@param [\"LSTM\", \"GRU\"]\n","learning_rate = 0.001\n","epochs = 8\n","batch_size = 64"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFyTXuEK9TJ3","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"n4g5Ufug8kyi","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595773452203,"user_tz":-330,"elapsed":24612,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"b4e03c17-26e6-4d3f-9fe2-5ed7b7d051b9"},"source":["!pip -q install gensim\n","!python -m spacy download en_core_web_md\n","!pip -q install paramiko\n","\n","import datetime, pickle, os, codecs, re, string\n","import json\n","import random\n","import numpy as np\n","import keras\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import *\n","from keras import regularizers\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import backend as K\n","from keras.utils import CustomObjectScope\n","from keras.engine.topology import Layer\n","\n","#\n","from keras.engine import InputSpec\n","\n","from keras import initializers\n","\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import string\n","from spacy.lang.en import English\n","import gensim, nltk, logging\n","\n","from nltk.corpus import stopwords\n","from nltk import tokenize\n","from nltk.stem import SnowballStemmer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import en_core_web_sm\n","\n","from IPython.display import HTML, display\n","\n","import tensorflow as tf\n","\n","from numpy.random import seed\n","# from tensorflow import set_random_seed\n","os.environ['PYTHONHASHSEED'] = str(1024)\n","tf.random.set_seed(1024)\n","# set_random_seed(1024)\n","seed(1024)\n","np.random.seed(1024)\n","random.seed(1024)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.7.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (49.1.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n","\u001b[38;5;2mâ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5exIp4egmwwA","colab_type":"text"},"source":["# Text preprocessing"]},{"cell_type":"code","metadata":{"id":"zY1TBBBbqEbp","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452204,"user_tz":-330,"elapsed":24595,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["def clean_str(string):\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","\n","    cleanr = re.compile('<.*?>')\n","\n","    # string = re.sub(r'\\d+', '', string)\n","    string = re.sub(cleanr, '', string)\n","    # string = re.sub(\"'\", '', string)\n","    # string = re.sub(r'\\W+', ' ', string)\n","    string = string.replace('_', '')\n","\n","\n","    return string.strip().lower()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLle9TkY1T_X","colab_type":"text"},"source":["# Use pre-trained word embeddings"]},{"cell_type":"code","metadata":{"id":"oLodIgWw14kC","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452206,"user_tz":-330,"elapsed":24582,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["def load_subword_embedding_300d(word_index):\n","    print('load_subword_embedding...')\n","    embeddings_index = {}\n","    f = codecs.open(word_embedding_path + \"wiki-news-300d-1M-subword.vec\", encoding='utf-8')\n","    for line in tqdm(f):\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('found %s word vectors' % len(embeddings_index))\n","    \n","    #embedding matrix\n","    print('preparing embedding matrix...')\n","    words_not_found = []\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    \n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if (embedding_vector is not None) and len(embedding_vector) > 0:\n","            \n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            words_not_found.append(word)\n","    \n","    return embedding_matrix"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voCjs6ewiSoF","colab_type":"text"},"source":["# Plot word embedding chart"]},{"cell_type":"code","metadata":{"id":"0rXXQmEMibaF","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452208,"user_tz":-330,"elapsed":24565,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["def plot_with_labels(low_dim_embs, labels, filename= word_embedding_path + 'tsne.png'):    \n","    plt.figure(figsize=(18, 18))\n","    for i, label in enumerate(labels):\n","        x, y = low_dim_embs[i, :]\n","        plt.scatter(x, y)\n","        plt.annotate(label,\n","                 xy=(x, y),\n","                 xytext=(5, 2),\n","                 textcoords='offset points',\n","                 ha='right',\n","                 va='bottom')\n","    plt.savefig(filename)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTR1_43CqGSW","colab_type":"text"},"source":["# Normalize texts"]},{"cell_type":"code","metadata":{"id":"ePLZEjNxqPKe","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452767,"user_tz":-330,"elapsed":25105,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["nlp = en_core_web_sm.load()\n","\n","\n","puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â¢',  '~', '@', 'Â£', \n"," 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â', 'Â°', 'â¬', 'â¢', 'âº',  'â¥', 'â', 'Ã', 'Â§', 'â³', 'â²', 'Ã', 'â', 'Â½', 'Ã ', 'â¦', \n"," 'â', 'â', 'â', 'â', 'â', 'Ã¢', 'âº', 'â', 'Â¢', 'Â²', 'Â¬', 'â', 'Â¶', 'â', 'Â±', 'Â¿', 'â¾', 'â', 'Â¦', 'â', 'â', 'Â¥', 'â', 'â', 'â¹', 'â', \n"," 'â', 'ï¼', 'Â¼', 'â', 'â¼', 'âª', 'â ', 'â ', 'â', 'â', 'Â¨', 'â', 'â«', 'â', 'Ã©', 'Â¯', 'â¦', 'Â¤', 'â²', 'Ã¨', 'Â¸', 'Â¾', 'Ã', 'â', 'â', 'â', \n"," 'â', 'ï¼', 'â', 'ã', 'â', 'ï¼', 'Â»', 'ï¼', 'âª', 'â©', 'â', 'Â³', 'ã»', 'â¦', 'â£', 'â', 'â', 'â¬', 'â¤', 'Ã¯', 'Ã', 'Â¹', 'â¤', 'â¡', 'â', '#', 'ââ']\n","\n","\n","def clean_puncts(x):\n","    x = str(x)\n","    for punct in puncts:\n","        x = x.replace(punct, f' {punct} ')\n","    return x\n","\n","def remove_stopwords(text):\n","    text = str(text)    \n","    ## Convert words to lower case and split them\n","    text = text.lower().split()\n","    \n","    ## Remove stop words\n","    stops = set(stopwords.words(\"english\"))\n","    text = [w for w in text if not w in stops and len(w) >= 3]\n","    text = \" \".join(text)\n","    \n","    return text\n","\n","\n","def normalize(text):\n","    text = text.lower().strip()\n","    doc = nlp(text)\n","    filtered_sentences = []\n","    for sentence in doc.sents:                    \n","        sentence = clean_puncts(sentence)\n","        sentence = clean_str(sentence)            \n","        #sentence = remove_stopwords(sentence)                \n","        filtered_sentences.append(sentence)\n","    return filtered_sentences"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu_O63OD2h8P","colab_type":"text"},"source":["# Training word embeddings"]},{"cell_type":"code","metadata":{"id":"HXyS2m7C2pZ7","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452769,"user_tz":-330,"elapsed":25088,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["def create_fasttext(embed_dim, data):\n","    \n","    filename = fasttext_path + 'fasttext_model.txt'\n","    \n","    if not os.path.isfile(filename):    \n","        print('create_fasttext...')\n","        sent_lst = []\n","\n","        for doc in data['text']:\n","            doc = clean_str(doc)\n","            sentences = nltk.tokenize.sent_tokenize(doc)\n","            for sent in sentences:\n","                word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n","                sent_lst.append(word_lst)\n","\n","\n","        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","        fasttext_model = gensim.models.FastText(\n","            word_ngrams=1,\n","            sentences=sent_lst, \n","            size = embed_dim, \n","            workers=os.cpu_count(), \n","            window = 1)\n","        fasttext_model.save(filename)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"724247Q4phPE","colab_type":"text"},"source":["# Load datasets\n","\n","**1)** Yelp Dataset with 500k reviews\n","\n","**2)** IMDb with 50k reviews"]},{"cell_type":"code","metadata":{"id":"0Owr7XARpnO7","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452770,"user_tz":-330,"elapsed":25068,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["def load_data_yelp(path, train_ratio=1, size=400000):\n","    vector_dim = 200\n","    dim = 5\n","    \n","    with open(path) as f:\n","        reviews=f.read().strip().split(\"\\n\")\n","        \n","    df = pd.DataFrame([json.loads(review) for review in reviews])        \n","\n","    text_tokens = []\n","    for row in tqdm(df['text']):    \n","        text_tokens.append(normalize(row))  \n","    \n","    df['text_tokens'] = text_tokens    \n","    del text_tokens\n","    \n","    vector_dim = 200     \n","    if word_embedding_type is 'from_scratch':\n","        create_fasttext(vector_dim, df)\n","        \n","    ###\n","    \n","    X = df['text_tokens'].values\n","    Y = pd.get_dummies(df['stars']).values\n","    \n","    return (X, Y)\n","\n","def load_data_imdb(path, size=49000, train_ratio=1):    \n","    df = pd.read_csv(path, nrows=size, usecols=['text', 'sentiment'])        \n","    \n","    vector_dim = 200 \n","    \n","    if word_embedding_type is 'from_scratch':\n","        # Fasttext\n","        create_fasttext(vector_dim, df)\n","    \n","    ### \n","    \n","    text_tokens = []\n","    for row in tqdm(df['text']):    \n","        text_tokens.append(normalize(row))  \n","    \n","    df['text_tokens'] = text_tokens\n","    \n","    del text_tokens\n","    ###\n","    \n","    X = df['text_tokens'].values\n","    Y = pd.get_dummies(df['sentiment']).values\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsTKHWjKqXmo","colab_type":"text"},"source":["# Attention Layer\n","Check [(Bahdanau et al., 2015)](https://arxiv.org/pdf/1409.0473.pdf)"]},{"cell_type":"code","metadata":{"id":"C_HrvPCJ9HYs","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452771,"user_tz":-330,"elapsed":25047,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["class Attention(Layer):\n","    def __init__(self, **kwargs):\n","        self.init = initializers.get('normal')\n","        self.supports_masking = True\n","        self.attention_dim = 50\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        self.W = K.variable(self.init((input_shape[-1], 1)))\n","        self.b = K.variable(self.init((self.attention_dim, )))\n","        self.u = K.variable(self.init((self.attention_dim, 1)))\n","        self.trainable_weights = [self.W, self.b, self.u]\n","        super(Attention, self).build(input_shape)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def call(self, x, mask=None):\n","        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n","        ait = K.dot(uit, self.u)\n","        ait = K.squeeze(ait, -1)\n","        ait = K.exp(ait)\n","\n","        if mask is not None:\n","            ait *= K.cast(mask, K.floatx())\n","            \n","        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        ait = K.expand_dims(ait)\n","        weighted_input = x * ait\n","        output = K.sum(weighted_input, axis=1)\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmmw7Qfvqx-f","colab_type":"text"},"source":["# Model architecture"]},{"cell_type":"code","metadata":{"id":"jHLATdJqEIiN","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773452771,"user_tz":-330,"elapsed":25034,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["# !rm -rf saved_models\n","# !mkdir saved_models"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6bzUS1dqftf","colab_type":"code","trusted":true,"colab":{},"executionInfo":{"status":"ok","timestamp":1595773453512,"user_tz":-330,"elapsed":25765,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["class HAHNetwork():\n","    def __init__(self):\n","        self.model = None\n","        self.MAX_SENTENCE_LENGTH = 0\n","        self.MAX_SENTENCE_COUNT = 0\n","        self.VOCABULARY_SIZE = 0\n","        self.word_embedding = None\n","        self.model = None\n","        self.word_attention_model = None\n","        self.tokenizer = None\n","        self.class_count = 2\n","\n","    def build_model(self, n_classes=2, embedding_dim=200, embeddings_path=False):\n","        \n","        l2_reg = regularizers.l2(0.001)\n","        \n","        embedding_weights = np.random.normal(0, 1, (len(self.tokenizer.word_index) + 1, embedding_dim))\n","        \n","        if embeddings_path is not None:\n","\n","            if word_embedding_type is 'from_scratch':\n","                # FastText\n","                filename = fasttext_path + 'fasttext_model.txt'                \n","                model =  gensim.models.FastText.load(filename)\n","\n","                embeddings_index = model.wv                    \n","                embedding_matrix = np.zeros( ( len(self.tokenizer.word_index) + 1, embedding_dim) )\n","                for word, i in self.tokenizer.word_index.items():\n","                    try:\n","                        embedding_vector = embeddings_index[word]\n","                        if embedding_vector is not None:\n","                            embedding_matrix[i] = embedding_vector\n","                    except Exception as e:\n","                        #print(str(e))\n","                        continue\n","\n","\n","            else:                \n","                embedding_dim = 300\n","                embedding_matrix = load_subword_embedding_300d(self.tokenizer.word_index)\n","\n","            embedding_weights = embedding_matrix\n","\n","        sentence_in = Input(shape=(self.MAX_SENTENCE_LENGTH,), dtype='int32', name=\"input_1\")\n","        \n","        embedding_trainable = True\n","        \n","        \n","        \n","        if word_embedding_type is 'pre_trained':\n","            embedding_trainable = False\n","        \n","        embedded_word_seq = Embedding(\n","            self.VOCABULARY_SIZE,\n","            embedding_dim,\n","            weights=[embedding_weights],\n","            input_length=self.MAX_SENTENCE_LENGTH,\n","            trainable=embedding_trainable,\n","            #mask_zero=True,\n","            mask_zero=False,\n","            name='word_embeddings',)(sentence_in) \n","        \n","        \n","                     \n","        dropout = Dropout(0.2)(embedded_word_seq)\n","        filter_sizes = [3,4,5]\n","        convs = []\n","        for filter_size in filter_sizes:\n","            conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n","            pool = MaxPool1D(filter_size)(conv)\n","            convs.append(pool)\n","        \n","        concatenate = Concatenate(axis=1)(convs)\n","        \n","        if rnn_type is 'GRU':\n","            #word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True, dropout=0.2))(concatenate)                \n","            dropout = Dropout(0.1)(concatenate)\n","            word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True))(dropout)                \n","        else:\n","            word_encoder = Bidirectional(\n","                LSTM(50, return_sequences=True, dropout=0.2))(embedded_word_seq)\n","            \n","        \n","        dense_transform_word = Dense(\n","            100, \n","            activation='relu', \n","            name='dense_transform_word', \n","            kernel_regularizer=l2_reg)(word_encoder)\n","        \n","        # word attention\n","        attention_weighted_sentence = Model(\n","            sentence_in, Attention(name=\"word_attention\")(dense_transform_word))\n","        \n","        self.word_attention_model = attention_weighted_sentence\n","        \n","        attention_weighted_sentence.summary()\n","\n","        # sentence-attention-weighted document scores\n","        \n","        texts_in = Input(shape=(self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH), dtype='int32', name=\"input_2\")\n","        \n","        attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n","        \n","        \n","        if rnn_type is 'GRU':\n","            #sentence_encoder = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n","            dropout = Dropout(0.1)(attention_weighted_sentences)\n","            sentence_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True))(dropout)\n","        else:\n","            sentence_encoder = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n","        \n","        \n","        dense_transform_sentence = Dense(\n","            100, \n","            activation='relu', \n","            name='dense_transform_sentence',\n","            kernel_regularizer=l2_reg)(sentence_encoder)\n","        \n","        # sentence attention\n","        attention_weighted_text = Attention(name=\"sentence_attention\")(dense_transform_sentence)\n","        \n","        \n","        prediction = Dense(n_classes, activation='softmax')(attention_weighted_text)\n","        \n","        model = Model(texts_in, prediction)\n","        model.summary()\n","        \n","        \n","        optimizer=Adam(lr=learning_rate, decay=0.0001)\n","\n","        model.compile(\n","                      optimizer=optimizer,\n","                      loss='categorical_crossentropy',\n","                      metrics=['accuracy'])\n","\n","        return model\n","\n","\n","    def get_tokenizer_filename(self, saved_model_filename):\n","        return saved_model_filename + '.tokenizer'\n","\n","    def fit_on_texts(self, texts):\n","        self.tokenizer = Tokenizer(filters='\"()*,-/;[\\]^_`{|}~', oov_token='UNK');\n","        all_sentences = []\n","        max_sentence_count = 0\n","        max_sentence_length = 0\n","        for text in texts:\n","            sentence_count = len(text)\n","            if sentence_count > max_sentence_count:\n","                max_sentence_count = sentence_count\n","            for sentence in text:\n","                sentence_length = len(sentence)\n","                if sentence_length > max_sentence_length:\n","                    max_sentence_length = sentence_length\n","                all_sentences.append(sentence)\n","\n","\n","        self.MAX_SENTENCE_COUNT = min(max_sentence_count, 15)\n","        self.MAX_SENTENCE_LENGTH = min(max_sentence_length, 50)\n","        \n","        self.tokenizer.fit_on_texts(all_sentences)\n","        self.VOCABULARY_SIZE = len(self.tokenizer.word_index) + 1\n","        self.create_reverse_word_index()\n","\n","    def create_reverse_word_index(self):\n","        self.reverse_word_index = {value:key for key,value in self.tokenizer.word_index.items()}\n","\n","    def encode_texts(self, texts):\n","        encoded_texts = np.zeros((len(texts), self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH))\n","        for i, text in enumerate(texts):\n","            encoded_text = np.array(pad_sequences(\n","                self.tokenizer.texts_to_sequences(text), \n","                maxlen=self.MAX_SENTENCE_LENGTH))[:self.MAX_SENTENCE_COUNT]\n","            encoded_texts[i][-len(encoded_text):] = encoded_text\n","        return encoded_texts\n","\n","    def save_tokenizer_on_epoch_end(self, path, epoch):\n","        if epoch == 0:\n","            tokenizer_state = {\n","                'tokenizer': self.tokenizer,\n","                'maxSentenceCount': self.MAX_SENTENCE_COUNT,\n","                'maxSentenceLength': self.MAX_SENTENCE_LENGTH,\n","                'vocabularySize': self.VOCABULARY_SIZE\n","            }\n","            pickle.dump(tokenizer_state, open(path, \"wb\" ) )\n","\n","    def train(self, train_x, train_y,\n","              batch_size=16, \n","              epochs=1, \n","              embedding_dim=200, \n","              embeddings_path=False, \n","              saved_model_dir= HANN_path + 'saved_models', \n","              saved_model_filename=None,):\n","        \n","        self.fit_on_texts(train_x)\n","        self.model = self.build_model(\n","            n_classes=train_y.shape[-1], \n","            embedding_dim=200,\n","            embeddings_path=embeddings_path)\n","        encoded_train_x = self.encode_texts(train_x)\n","        callbacks = [\n","            ReduceLROnPlateau(),\n","            LambdaCallback(\n","                on_epoch_end=lambda epoch, logs: self.save_tokenizer_on_epoch_end(\n","                    os.path.join(saved_model_dir, \n","                        self.get_tokenizer_filename(saved_model_filename)), epoch))\n","        ]\n","\n","        if saved_model_filename:\n","            callbacks.append(\n","                ModelCheckpoint(\n","                    filepath=os.path.join(saved_model_dir, saved_model_filename),\n","                    monitor='val_acc',\n","                    save_best_only=True,\n","                    save_weights_only=False,\n","                )\n","            )\n","        history = self.model.fit(\n","                       x=encoded_train_x, \n","                       y=train_y, \n","                       batch_size=batch_size, \n","                       epochs=epochs, \n","                       verbose=1, \n","                       callbacks=callbacks,\n","                       validation_split=0.1,  \n","                       shuffle=True)\n","        \n","        # Plot\n","        print(history.history.keys())\n","        \n","        plt.plot(history.history['acc'])\n","        plt.plot(history.history['val_acc'])\n","        plt.title('model accuracy')\n","        plt.ylabel('accuracy')\n","        plt.xlabel('epoch')\n","        plt.legend(['train', 'test'], loc='upper left')\n","        plt.show()\n","        \n","        plt.plot(history.history['loss'])\n","        plt.plot(history.history['val_loss'])\n","        plt.title('model loss')\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        plt.legend(['train', 'test'], loc='upper left')\n","        plt.show()\n","\n","    def encode_input(self, x, log=False):\n","        x = np.array(x)\n","        if not x.shape:\n","            x = np.expand_dims(x, 0)\n","        texts = np.array([normalize(text) for text in x])\n","        return self.encode_texts(texts)\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obo_fTOvUsi8","colab_type":"text"},"source":["# Load dataset\n","\n","This might take several minutes, depending your dataset"]},{"cell_type":"code","metadata":{"id":"jFwuRFg3qxBW","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"12e05dff-0a75-43b1-cb34-1b900de1d687"},"source":["YELP_DATA_PATH = HANN_path + 'datasets/yelp_reviews_sampling.json'\n","IMDB_DATA_PATH = HANN_path + 'datasets/imdb_reviews.csv'\n","SAVED_MODEL_DIR = HANN_path + 'saved_models'\n","SAVED_MODEL_FILENAME = 'model.h5'\n","\n","if dataset is 'yelp':\n","    (X, Y) = load_data_yelp(path=YELP_DATA_PATH, size=400000)\n","else:\n","    (X, Y) = load_data_imdb(path=IMDB_DATA_PATH, size=49000)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 77%|ââââââââ  | 37589/49000 [26:57<08:57, 21.23it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zv2OyKLKuOtM","colab_type":"code","colab":{}},"source":["with open(datasets_path + 'loaded_imdb.pickle', 'wb') as f:\n","    pickle.dump((X, Y), f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGhw9AgRuR37","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1595772354187,"user_tz":-330,"elapsed":1658,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"7ae08426-bc73-4b79-e68d-f0f6d9e9e7f0"},"source":["with open(datasets_path + 'loaded_imdb.pickle', 'rb') as f:\n","     (X, Y) = pickle.load(f)"],"execution_count":45,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-20bf0f301b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'loaded_imdb.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'datasets_path' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"D9Zcz1wv0wf3","colab_type":"text"},"source":["# Plots Word Embedding chart\n"]},{"cell_type":"code","metadata":{"id":"0TD83uVu04z7","colab_type":"code","trusted":true,"colab":{}},"source":["limit = 200\n","vector_dim = 200\n","\n","# Fasttext\n","filename = fasttext_path + 'fasttext_model.txt'                \n","model =  gensim.models.FastText.load(filename)\n","words = []\n","embedding = np.array([])\n","i = 0\n","for word in model.wv.vocab:\n","    if i == limit: break\n","\n","    words.append(word)\n","    embedding = np.append(embedding, model[word])\n","    i += 1\n","\n","embedding = embedding.reshape(limit, vector_dim)    \n","tsne = TSNE(n_components=2)\n","low_dim_embedding = tsne.fit_transform(embedding)\n","       \n","plot_with_labels(low_dim_embedding, words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOZg-uB4HV1H","colab_type":"text"},"source":["\n","# Taining model"]},{"cell_type":"code","metadata":{"id":"7ZkHlV6rHNkJ","colab_type":"code","trusted":true,"colab":{}},"source":["K.clear_session()\n","model = HAHNetwork()\n","model.train(X, Y, batch_size=64, epochs=8, embeddings_path=True, saved_model_dir=SAVED_MODEL_DIR, saved_model_filename=SAVED_MODEL_FILENAME)"],"execution_count":null,"outputs":[]}]}