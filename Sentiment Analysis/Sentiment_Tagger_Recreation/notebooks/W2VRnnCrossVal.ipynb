{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W2VRnnCrossVal.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"soLpxxAT4sj9","colab_type":"code","outputId":"c362614b-9ac8-49c7-cd7c-89580a081bb1","executionInfo":{"status":"ok","timestamp":1589121511880,"user_tz":-60,"elapsed":14482,"user":{"displayName":"Isuru Udara Liyanage","photoUrl":"","userId":"16467562658879325566"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0U4OmgIZbeIf","colab_type":"code","outputId":"b6af5471-c5e1-4725-c762-99303bbb1ad7","executionInfo":{"status":"ok","timestamp":1589121524058,"user_tz":-60,"elapsed":7847,"user":{"displayName":"Isuru Udara Liyanage","photoUrl":"","userId":"16467562658879325566"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 1.x\n","import pandas as pd\n","import numpy as np\n","import time\n","from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n","\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","from prettytable import PrettyTable\n","from sklearn.svm import SVC\n","import tensorflow as tf\n","import datetime\n","from random import randint\n","from gensim.models import word2vec"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EzFiiGBJcb7v","colab_type":"code","colab":{}},"source":["word2vec_model_name = \"/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/word_embedding/fasttext/commen_docid_removed300_5\"\n","\n","num_features = 300\n","max_sentence_length = 50\n","\n","batchSize = 24\n","lstmUnits = 64\n","numClasses = 2\n","iterations = 1000\n","\n","labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n","data = tf.placeholder(tf.float32, [batchSize, max_sentence_length, num_features])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRcXtoyjcq-_","colab_type":"code","colab":{}},"source":["def main():\n","    start_time = time.time()\n","    run_cross_val()\n","    end_time = time.time()\n","    print(\"Time taken for the process: \" + str(end_time - start_time))\n","    return\n","\n","\n","def convert_to_vectors():\n","    comments = pd.read_csv(\"/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/corpus/analyzed/comments_tagged_remove_all_punc_keep_question.csv\", \";\")\n","    data_vectors, data_labels = comments_to_vectors(comments)\n","\n","    np.save('/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment-tagger/RNN/vectors_from_W2vRnnCrossVal/data_vectors_common_docid_removed.npy', data_vectors)\n","    np.save('/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment-tagger/RNN/vectors_from_W2vRnnCrossVal/data_labels_common_docid_removed.npy', data_labels)\n","\n","\n","def load_vectors():\n","    data_vectors = np.load('/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment-tagger/RNN/vectors_from_W2vRnnCrossVal/data_vectors_common_docid_removed.npy')\n","    data_labels = np.load('/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment-tagger/RNN/vectors_from_W2vRnnCrossVal/data_labels_common_docid_removed.npy')\n","    return data_vectors, data_labels\n","\n","\n","def comments_to_vectors(data):\n","    model = word2vec.Word2Vec.load(word2vec_model_name)\n","    comment_vectors = []\n","    comment_labels = []\n","    for comment in data[\"comment\"]:\n","        comment_vectors.append(get_sentence_vector(model, comment))\n","    for label in data[\"label\"]:\n","        if label == \"POSITIVE\":\n","            comment_labels.append([0, 1])\n","        else:\n","            comment_labels.append([1, 0])\n","    return np.array(comment_vectors), comment_labels\n","\n","\n","def get_sentence_vector(model, sentence):\n","    sentence_vector = np.zeros([max_sentence_length, num_features])\n","    counter = 0\n","    index2word_set = set(model.wv.index2word)\n","    for word in sentence.split():\n","        if word in index2word_set:\n","            sentence_vector[counter] = model[word]\n","            counter += 1\n","            if (counter == max_sentence_length):\n","                break\n","        else:\n","            print(\"word not in word2vec model: \" + word)\n","    return sentence_vector\n","\n","\n","def get_batch(size, data, label):\n","    batch_data = np.empty((size, max_sentence_length, num_features), dtype=float)\n","    batch_label = []\n","    for i in range(size):\n","        random_int = randint(0, len(data) - 1)\n","        batch_data[i] = data[random_int]\n","        batch_label.append(label[random_int])\n","    return batch_data, batch_label\n","\n","\n","def get_batch_order(size, data, label, batch_no):\n","    batch_data = data[batch_no * size : (batch_no + 1) * size]\n","    batch_label = label[batch_no * size : (batch_no + 1) * size]\n","    return batch_data, batch_label\n","\n","\n","def neural_network_model():\n","    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n","\n","    lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=0.75)\n","    value, _ = tf.nn.dynamic_rnn(lstm_cell, data, dtype=tf.float32)\n","\n","    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n","    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n","    value = tf.transpose(value, [1, 0, 2])\n","    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n","    prediction = (tf.matmul(last, weight) + bias)\n","\n","    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    prediction_values = tf.argmax(prediction, 1)\n","\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n","    optimizer = tf.train.AdamOptimizer().minimize(loss)\n","\n","    return loss, accuracy, prediction_values, optimizer\n","\n","\n","def train_neural_network(loss, accuracy, optimizer, train_data, train_labels):\n","    sess = tf.InteractiveSession()\n","    saver = tf.train.Saver()\n","    sess.run(tf.global_variables_initializer())\n","\n","    tf.summary.scalar('Loss', loss)\n","    tf.summary.scalar('Accuracy', accuracy)\n","    merged = tf.summary.merge_all()\n","    logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n","    writer = tf.summary.FileWriter(logdir, sess.graph)\n","\n","    for i in range(iterations):\n","        #Next Batch of reviews\n","        next_batch, next_batch_labels = get_batch(batchSize, train_data, train_labels)\n","        sess.run(optimizer, {data: next_batch, labels: next_batch_labels})\n","\n","        #Write summary to Tensorboard\n","        if (i % 499 == 0):\n","            summary = sess.run(merged, {data: next_batch, labels: next_batch_labels})\n","            writer.add_summary(summary, i)\n","\n","        #Save the network every 10,000 training iterations\n","        if (i % 499 == 0 and i != 0):\n","            save_path = saver.save(sess, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment-tagger/RNN/models/from_fasttext/5/pretrained_lstm.ckpt\", global_step=i)\n","            print(\"saved to %s\" % save_path)\n","    writer.close()\n","\n","\n","def measure_neural_network(accuracy, prediction_values, test_data, test_labels):\n","    sess = tf.InteractiveSession()\n","    saver = tf.train.Saver()\n","    saver.restore(sess, tf.train.latest_checkpoint('/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/Sentiment-tagger/RNN/models/from_fasttext/5/'))\n","\n","    overall_accuracy = 0\n","    all_predictions = []\n","    test_iterations = 20\n","    for i in range(test_iterations):\n","        next_batch, next_batch_labels = get_batch_order(batchSize, test_data, test_labels, i)\n","        accuracy_this_batch = (sess.run(accuracy, {data: next_batch, labels: next_batch_labels})) * 100\n","        predictions_this_batch = sess.run(prediction_values, {data: next_batch, labels: next_batch_labels})\n","        overall_accuracy = overall_accuracy + accuracy_this_batch\n","        all_predictions = all_predictions + predictions_this_batch.tolist()\n","        print(\"Accuracy for this batch:\", accuracy_this_batch)\n","\n","    true_labels = tf.argmax(test_labels, 1).eval()\n","    precision = precision_score(true_labels.tolist()[0:batchSize * test_iterations], all_predictions)\n","    f1 = f1_score(true_labels.tolist()[0:batchSize * test_iterations], all_predictions)\n","    recall = recall_score(true_labels.tolist()[0:batchSize * test_iterations], all_predictions)\n","    overall_accuracy = overall_accuracy / (test_iterations * 100)\n","    print(confusion_matrix(true_labels.tolist()[0:batchSize * test_iterations], all_predictions).ravel())\n","\n","    all_test = true_labels.tolist()[0:batchSize * test_iterations]\n","    return overall_accuracy, precision, recall, f1, all_predictions, all_test\n","\n","\n","def run_cross_val():\n","    all_predictions = []\n","    all_used_test_labels = []\n","    # w2v_model_path = \"../../../corpus/analyzed/saved_models/\"\n","    comments = pd.read_csv(\"/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/corpus/analyzed/comments_tagged_remove_all_punc_keep_question.csv\", \";\")\n","    pretty_table = PrettyTable([\"Algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"])\n","\n","    # convert_to_vectors()\n","    data_vectors, data_labels = load_vectors()\n","\n","    print(\"Running tesnsorflow simulation.....\")\n","\n","\n","    i = 1\n","    kf = KFold(n_splits=10)\n","    kf.get_n_splits(comments)\n","    for train_index, test_index in kf.split(data_vectors):\n","        train_data_comments, test_data_comments = data_vectors[train_index], data_vectors[test_index]\n","        train_data_labels, test_data_labels = data_labels[train_index], data_labels[test_index]\n","\n","\n","        tf.reset_default_graph()\n","\n","        global labels\n","        global data\n","        labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n","        data = tf.placeholder(tf.float32, [batchSize, max_sentence_length, num_features])\n","\n","        loss, accuracy, prediction_values, optimizer = neural_network_model()\n","        train_neural_network(loss, accuracy, optimizer, train_data_comments, train_data_labels)\n","        accuracy, precision, recall, f1, predictions, used_test_labels = measure_neural_network(accuracy, prediction_values, test_data_comments, test_data_labels)\n","\n","        all_predictions = all_predictions + predictions\n","        all_used_test_labels = all_used_test_labels + used_test_labels\n","\n","        i = i + 1\n","        evaluation_metrics(used_test_labels, predictions, pretty_table, \"iteration\" + str(i))\n","\n","    evaluation_metrics(all_used_test_labels, all_predictions, pretty_table, \"final\")\n","    print(pretty_table)\n","    print_confusion_matrix(all_used_test_labels, all_predictions)\n","\n","\n","\n","def fit_models(vectorizer, train_data_comments, test_data_comments, train_data_labels, test_data_labels):\n","    pretty_table = PrettyTable([\"Algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"])\n","\n","    vectorized_train_comments = vectorizer.fit_transform(train_data_comments)\n","    vectorized_test_comments = vectorizer.transform(test_data_comments)\n","\n","    model = SVC(C=1, kernel='linear')\n","    model = model.fit(vectorized_train_comments, train_data_labels)\n","    predictions = model.predict(vectorized_test_comments)\n","    evaluation_metrics(test_data_labels, predictions, pretty_table, \"SVM\")\n","    print_confusion_matrix(test_data_labels, predictions)\n","\n","    print(pretty_table)\n","    print(\"\")\n","    return predictions\n","\n","\n","def evaluation_metrics(true_sentiment, predicted_sentiment, pretty_table, algorithm):\n","    label_binarizer = preprocessing.LabelBinarizer()\n","    label_binarizer.fit(['NEGATIVE', 'POSITIVE'])\n","    test_labels = label_binarizer.transform(true_sentiment)\n","    predict_labels = label_binarizer.transform(predicted_sentiment)\n","    accuracy_str = str(accuracy_score(true_sentiment, predicted_sentiment))\n","    precision_str = str(precision_score(true_sentiment, predicted_sentiment))\n","    recall_str = str(recall_score(true_sentiment, predicted_sentiment))\n","    f1_score_str = str(f1_score(true_sentiment, predicted_sentiment))\n","    pretty_table.add_row([algorithm, accuracy_str, precision_str, recall_str, f1_score_str])\n","    return\n","\n","\n","def print_confusion_matrix(label, prediction):\n","    cf_matrix = confusion_matrix(label, prediction)\n","    print(cf_matrix.ravel())\n","    print(cf_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIl_US7gfF4_","colab_type":"code","colab":{}},"source":["convert_to_vectors()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifDJVxECcut4","colab_type":"code","colab":{}},"source":["main()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVBRDU3wiN7N","colab_type":"text"},"source":["Results from 100 epochs\n","\n","+-------------+--------------------+--------------------+--------------------+--------------------+\n","|  Algorithm  |      Accuracy      |     Precision      |       Recall       |      F1_Score      |\n","+-------------+--------------------+--------------------+--------------------+--------------------+\n","|  iteration2 | 0.7479166666666667 | 0.8456375838926175 |       0.5625       | 0.6756032171581771 |\n","|  iteration3 |      0.74375       | 0.8705882352941177 | 0.5943775100401606 | 0.7064439140811456 |\n","|  iteration4 | 0.8083333333333333 | 0.9336283185840708 | 0.7326388888888888 | 0.8210116731517509 |\n","|  iteration5 | 0.7666666666666667 | 0.9148936170212766 | 0.6417910447761194 | 0.7543859649122806 |\n","|  iteration6 |       0.775        | 0.8571428571428571 | 0.631578947368421  | 0.7272727272727273 |\n","|  iteration7 | 0.7708333333333334 | 0.8963414634146342 |       0.6125       | 0.7277227722772278 |\n","|  iteration8 | 0.8020833333333334 | 0.8812785388127854 | 0.7366412213740458 | 0.8024948024948024 |\n","|  iteration9 | 0.7854166666666667 | 0.9380952380952381 | 0.686411149825784  | 0.7927565392354124 |\n","| iteration10 | 0.8041666666666667 | 0.8243243243243243 | 0.6421052631578947 | 0.7218934911242604 |\n","| iteration11 | 0.8083333333333333 | 0.7380952380952381 | 0.6118421052631579 | 0.6690647482014388 |\n","|    final    |      0.78125       | 0.878393665158371  | 0.6503350083752094 | 0.7473532242540905 |\n","+-------------+--------------------+--------------------+--------------------+--------------------+\n","[2197  215  835 1553]\n","[[2197  215]\n"," [ 835 1553]]"]}]}