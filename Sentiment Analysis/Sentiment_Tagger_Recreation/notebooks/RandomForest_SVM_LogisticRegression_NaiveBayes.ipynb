{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RandomForest_SVM_LogisticRegression_NaiveBayes.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GVLzHtm588Pk","colab_type":"text"},"source":["Name in Repo : SentimentAnalyzerW2V2.py"]},{"cell_type":"markdown","metadata":{"id":"AQtzcTtz9E3z","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"-xI-4pZ_phP-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1584283423537,"user_tz":-330,"elapsed":26583,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"3a6d0634-d352-4479-8a8f-1167bc5b6d3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J0IcwB109ElJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import math\n","from gensim.models import word2vec\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score, f1_score, precision_score\n","from gensim.models.fasttext import FastText"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmvIoHON8hg7","colab_type":"code","colab":{}},"source":["comments = pd.read_csv(\"/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/corpus/analyzed/comments_all_remove_all_punc_keep_question.csv\", \";\")\n","trainData, testData = train_test_split(comments, test_size=0.2, random_state=0)\n","no_of_train_samples = trainData.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmLDkPiC-tZ1","colab_type":"code","colab":{}},"source":["def main():\n","    # generate_word2vec_model()\n","    train_labels = trainData[\"label\"]\n","    test_labels = testData[\"label\"]\n","\n","    train_data_vecs, test_data_vecs = get_train_test_data_vecs(False)\n","    print(\"train data size = %d, test data size = %d\\n\" % (train_data_vecs.size, test_data_vecs.size))\n","    clssify_using_random_forest(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    clssify_using_svm(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    clssify_using_logistic_regression(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    clssify_using_naive_bayes(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","\n","    train_data_vecs, test_data_vecs = get_train_test_data_vecs(True)\n","    print(\"train data size = %d, test data size = %d\\n\" % (train_data_vecs.size, test_data_vecs.size))\n","    clssify_using_random_forest(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    clssify_using_svm(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    clssify_using_logistic_regression(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    clssify_using_naive_bayes(train_data_vecs, test_data_vecs, train_labels, test_labels)\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3AGrsHPj_Rli","colab_type":"code","colab":{}},"source":["def calculate_idf(train_comments):\n","    print(\"calculation idf scores\")\n","    index2word_set = set()\n","    word2doc_frequency = {}\n","    for comment in train_comments:\n","        comment_word_set = set()\n","        for word in comment.split():\n","            if word in index2word_set:\n","                if word not in comment_word_set:\n","                    word2doc_frequency[word] = word2doc_frequency.get(word) + 1\n","            else:\n","                index2word_set.add(word)\n","                word2doc_frequency[word] = 1\n","                comment_word_set.add(word)\n","    return word2doc_frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUeVyd5M_TSv","colab_type":"code","colab":{}},"source":["# split a comment into sentences of words\n","def to_separate_sentences(comment):\n","    sentences = []\n","    raw_sentences = str(comment).split(\".\")\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 2:\n","            sentences.append(raw_sentence.split())\n","    return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHl478nD_WYH","colab_type":"code","colab":{}},"source":["# to word list\n","def to_word_list(comment):\n","    return comment.split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kw-chBw0_Y0X","colab_type":"code","colab":{}},"source":["# make a feature vector from a single comment\n","def make_feature_vec(words, model, num_features):\n","    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n","    nwords = 0.\n","    index2word_set = set(model.wv.index2word)\n","    for word in words:\n","        if word in index2word_set:\n","            nwords = nwords + 1.\n","            feature_vec = np.add(feature_vec, model[word])\n","\n","    # we have some one word comments that is not included in original model, todo expand original model or remove them\n","    if nwords != 0:\n","        feature_vec = np.divide(feature_vec, nwords)\n","\n","    return feature_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2em35k-g_Z4g","colab_type":"code","colab":{}},"source":["# make a tfidf feature vector from a single comment\n","def make_feature_vec_tfidf(words, model, num_features, word2doc_frequency):\n","    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n","    nwords = 0.\n","    index2word_set = set(model.wv.index2word)\n","    for word in words:\n","        if word in index2word_set and word in word2doc_frequency:\n","            nwords = nwords + 1.\n","            feature_vec = np.add(feature_vec, model[word] * math.log10(no_of_train_samples / (word2doc_frequency.get(word))))\n","\n","    # we have some one word comments that is not included in original model, todo expand original model or remove them\n","    if nwords != 0:\n","        feature_vec = np.divide(feature_vec, nwords)\n","\n","    return feature_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUYO2KW-_c41","colab_type":"code","colab":{}},"source":["# get a list of feature vectors for all comments\n","def get_avg_feature_vecs(reviews, model, num_features):\n","    counter = 0\n","    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n","    for review in reviews:\n","        if counter % 1000. == 0.:\n","            print(\"Review %d of %d\" % (counter, len(reviews)))\n","\n","        review_feature_vecs[int(counter)] = make_feature_vec(review, model, num_features)\n","        counter = counter + 1.\n","    return review_feature_vecs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAkVlnPg_evU","colab_type":"code","colab":{}},"source":["# get a list of feature vectors for all comments\n","def get_avg_feature_vecs_tfidf(reviews, model, num_features, word2doc_frequency):\n","    counter = 0\n","    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n","    for review in reviews:\n","        if counter % 1000. == 0.:\n","            print(\"Review %d of %d\" % (counter, len(reviews)))\n","\n","        review_feature_vecs[int(counter)] = make_feature_vec_tfidf(review, model, num_features, word2doc_frequency)\n","        counter = counter + 1.\n","    return review_feature_vecs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Or9wu5ml_fo_","colab_type":"code","colab":{}},"source":["def get_train_test_data_vecs(tfidf):\n","    num_features = 300\n","    # model = word2vec.Word2Vec.load(\"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/Sentiment-tagger/corpus/analyzed/saved_models/Word2Vec/word2vec_model_skipgram_remove_1_300_10\")\n","    model = FastText.load(\"/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/word_embedding/fasttext/from_lankadeepa_comments_all_punct_removed/comments_all_skipgram_remove_puncuation_removed_300_10\")\n","    clean_train_comments = []\n","    clean_test_comments = []\n","\n","    for comment in trainData[\"comment\"]:\n","        clean_train_comments.append(comment.split())\n","    for comment in testData[\"comment\"]:\n","        clean_test_comments.append(comment.split())\n","\n","    if tfidf:\n","        word2doc_frequency = calculate_idf(trainData[\"comment\"])\n","        train_data_vecs = get_avg_feature_vecs_tfidf(clean_train_comments, model, num_features, word2doc_frequency)\n","        test_data_vecs = get_avg_feature_vecs_tfidf(clean_test_comments, model, num_features, word2doc_frequency)\n","    else:\n","        train_data_vecs = get_avg_feature_vecs(clean_train_comments, model, num_features)\n","        test_data_vecs = get_avg_feature_vecs(clean_test_comments, model, num_features)\n","\n","    return train_data_vecs, test_data_vecs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM62F0Zl_l03","colab_type":"code","colab":{}},"source":["def clssify_using_random_forest(train_data_vec, test_data_vec, train_labels, test_labels):\n","    forest = RandomForestClassifier(n_estimators=100)\n","    print(\"Fitting a random forest to labeled training data...\")\n","    forest = forest.fit(train_data_vec, train_labels)\n","    result = forest.predict(test_data_vec)\n","\n","    confusion_matrix = pd.crosstab(test_labels, result, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n","    print(confusion_matrix)\n","\n","    label_binarizer = preprocessing.LabelBinarizer()\n","    label_binarizer.fit(['NEGATIVE', 'POSITIVE'])\n","    test_sentiment = label_binarizer.transform(test_labels)\n","    predict_sentiment = label_binarizer.transform(result)\n","    accuracy_str = str(accuracy_score(test_sentiment, predict_sentiment))\n","    precision_str = str(precision_score(test_sentiment, predict_sentiment))\n","    f1_score_str = str(f1_score(test_sentiment, predict_sentiment))\n","    print(\"Accuracy = %s \\nPrecision = %s \\nF1score = %s \\n\" % (accuracy_str, precision_str, f1_score_str))\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6vHeCs4_n9Z","colab_type":"code","colab":{}},"source":["def clssify_using_svm(train_data_vec, test_data_vec, train_labels, test_labels):\n","    svm = SVC(C=1, kernel='linear')\n","    print(\"Fitting a SVM to labeled training data...\")\n","    svm = svm.fit(train_data_vec, train_labels)\n","    result = svm.predict(test_data_vec)\n","\n","    confusion_matrix = pd.crosstab(test_labels, result, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n","    print(confusion_matrix)\n","\n","    label_binarizer = preprocessing.LabelBinarizer()\n","    label_binarizer.fit(['NEGATIVE', 'POSITIVE'])\n","    test_sentiment = label_binarizer.transform(test_labels)\n","    predict_sentiment = label_binarizer.transform(result)\n","    accuracy_str = str(accuracy_score(test_sentiment, predict_sentiment))\n","    precision_str = str(precision_score(test_sentiment, predict_sentiment))\n","    f1_score_str = str(f1_score(test_sentiment, predict_sentiment))\n","    print(\"Accuracy = %s \\nPrecision = %s \\nF1score = %s \\n\" % (accuracy_str, precision_str, f1_score_str))\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvoQtiVOAHoX","colab_type":"code","colab":{}},"source":["def clssify_using_logistic_regression(train_data_vec, test_data_vec, train_labels, test_labels):\n","    logistic_regression = LogisticRegression()\n","    print(\"Fitting a logistic regression to labeled training data...\")\n","    logistic_regression = logistic_regression.fit(train_data_vec, train_labels)\n","    result = logistic_regression.predict(test_data_vec)\n","\n","    confusion_matrix = pd.crosstab(test_labels, result, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n","    print(confusion_matrix)\n","\n","    label_binarizer = preprocessing.LabelBinarizer()\n","    label_binarizer.fit(['NEGATIVE', 'POSITIVE'])\n","    test_sentiment = label_binarizer.transform(test_labels)\n","    predict_sentiment = label_binarizer.transform(result)\n","    accuracy_str = str(accuracy_score(test_sentiment, predict_sentiment))\n","    precision_str = str(precision_score(test_sentiment, predict_sentiment))\n","    f1_score_str = str(f1_score(test_sentiment, predict_sentiment))\n","    print(\"Accuracy = %s \\nPrecision = %s \\nF1score = %s \\n\" % (accuracy_str, precision_str, f1_score_str))\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7om7v01mAJx9","colab_type":"code","colab":{}},"source":["def clssify_using_naive_bayes(train_data_vec, test_data_vec, train_labels, test_labels):\n","    naive_bayes = GaussianNB()\n","    print(\"Fitting a naive bayes to labeled training data...\")\n","    naive_bayes = naive_bayes.fit(train_data_vec, train_labels)\n","    result = naive_bayes.predict(test_data_vec)\n","\n","    confusion_matrix = pd.crosstab(test_labels, result, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n","    print(confusion_matrix)\n","\n","    label_binarizer = preprocessing.LabelBinarizer()\n","    label_binarizer.fit(['NEGATIVE', 'POSITIVE'])\n","    test_sentiment = label_binarizer.transform(test_labels)\n","    predict_sentiment = label_binarizer.transform(result)\n","    accuracy_str = str(accuracy_score(test_sentiment, predict_sentiment))\n","    precision_str = str(precision_score(test_sentiment, predict_sentiment))\n","    f1_score_str = str(f1_score(test_sentiment, predict_sentiment))\n","    print(\"Accuracy = %s \\nPrecision = %s \\nF1score = %s \\n\" % (accuracy_str, precision_str, f1_score_str))\n","    return\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34divETuAMAE","colab_type":"code","colab":{}},"source":["main()"],"execution_count":null,"outputs":[]}]}