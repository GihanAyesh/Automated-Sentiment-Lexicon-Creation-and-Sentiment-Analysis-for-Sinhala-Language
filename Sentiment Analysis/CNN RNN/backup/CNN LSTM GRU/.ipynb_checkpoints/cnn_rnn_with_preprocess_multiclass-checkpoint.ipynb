{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6nTBbEWxpxA"
   },
   "source": [
    "# References\n",
    "https://github.com/ultimate010/crnn \n",
    "{Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts}\n",
    "https://keras.io/examples/imdb_bidirectional_lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zR8-aK009p9"
   },
   "source": [
    "# Folder Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1836,
     "status": "ok",
     "timestamp": 1594403128777,
     "user": {
      "displayName": "Lahiru Senevirathne",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64",
      "userId": "12627549003426465017"
     },
     "user_tz": -330
    },
    "id": "2DEOa06miLE5"
   },
   "outputs": [],
   "source": [
    "first_time = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_code_for_sentiment_analysis.ipynb\n",
      " cnn_rnn_by_piyumal.ipynb\n",
      "cnn_rnn_with_preprocess_backup.ipynb\n",
      "cnn_rnn_with_preprocess_binary.ipynb\n",
      "cnn_rnn_with_preprocess_multiclass.ipynb\n",
      "desktop.ini\n",
      "hyperparameter tuning to cnn_rnn_with_preprocess.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path =  '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/'\n",
    "folder_path = '../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2387,
     "status": "ok",
     "timestamp": 1594403129386,
     "user": {
      "displayName": "Lahiru Senevirathne",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64",
      "userId": "12627549003426465017"
     },
     "user_tz": -330
    },
    "id": "gWPD3dgEqB5d"
   },
   "outputs": [],
   "source": [
    "lankadeepa_data_path = folder_path + 'corpus/new/preprocess_from_isuru/lankadeepa_tagged_comments.csv'\n",
    "gossip_lanka_data_path = folder_path + 'corpus/new/preprocess_from_unicode_values/gossip_lanka_tagged_comments.csv'\n",
    "embedding_size = 300\n",
    "embedding_type = 'fasttext'\n",
    "embedding_type1 = 'fastText'\n",
    "context = 5\n",
    "word_embedding_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(embedding_size)+\"/\"+embedding_type1+\"_\"+str(embedding_size)+\"_\"+str(context)\n",
    "word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(embedding_size)+\"/keyed_vectors/keyed.kv\"\n",
    "embedding_matrix_path = folder_path + 'Sentiment Analysis/CNN RNN/embedding_matrix/'+embedding_type+'_lankadeepa_gossiplanka_'+str(embedding_size)+'_'+str(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip fasttetx_lankadeepa_gossiplanka_300_5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BYvE-D5QAc0"
   },
   "source": [
    "# Imports Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZ9YDtD6qn4t"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "if (first_time):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import cumsum\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout, Activation, Flatten, \\\n",
    "    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n",
    "    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras import callbacks\n",
    "from keras.utils import generic_utils\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXiAlHRZ7X_p"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqqHP9rwpbdt"
   },
   "outputs": [],
   "source": [
    "lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
    "gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
    "gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmw4wplLKgXx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['docid', 'comment', 'label'], dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lankadeepa_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJhs5dtuLDZA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100204</td>\n",
       "      <td>නියම සිංහල මහත්මයෙක්.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100204</td>\n",
       "      <td>අන්න මිනිස්සු...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100204</td>\n",
       "      <td>හොරා හොද මිනිහෙක් නම් අනේ මහත්තයෝ සමාවෙන්න කිය...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100204</td>\n",
       "      <td>තව ඩිංගෙන් ලබු ගෙඩියත් කොස් ගෙඩි සහ පොල් ගෙඩි ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100204</td>\n",
       "      <td>බතල වැලට කොස්ගෙඩි අටට උසාවි ගිහින් රිමන්ඩ් කරන...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>254158</td>\n",
       "      <td>කුස්සියේ කක්කුස්සියේ වෙනසක් නැතිව ඇති ආපන ශාලා...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>254158</td>\n",
       "      <td>ඔයාල දන්නේ නැද්ද කක්කුස්සියටත් කුස්සියටත් දෙකට...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>254158</td>\n",
       "      <td>එක ප්‍රශ්නයක් නම් තියෙනවා අහන්න පිරිමි වැසිකිල...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>254158</td>\n",
       "      <td>නිරෝගිමත් පරපුරක් රටට හිමිවේවි</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>254158</td>\n",
       "      <td>වැසිකිළියේ තියෙන කරාමයෙන් එන්නෙත් කුසිසියේ කරා...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3829 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       docid                                            comment  label\n",
       "0     100204                              නියම සිංහල මහත්මයෙක්.    4.0\n",
       "1     100204                                   අන්න මිනිස්සු...    4.0\n",
       "2     100204  හොරා හොද මිනිහෙක් නම් අනේ මහත්තයෝ සමාවෙන්න කිය...    3.0\n",
       "3     100204  තව ඩිංගෙන් ලබු ගෙඩියත් කොස් ගෙඩි සහ පොල් ගෙඩි ...    2.0\n",
       "4     100204  බතල වැලට කොස්ගෙඩි අටට උසාවි ගිහින් රිමන්ඩ් කරන...    2.0\n",
       "...      ...                                                ...    ...\n",
       "3824  254158  කුස්සියේ කක්කුස්සියේ වෙනසක් නැතිව ඇති ආපන ශාලා...    2.0\n",
       "3825  254158  ඔයාල දන්නේ නැද්ද කක්කුස්සියටත් කුස්සියටත් දෙකට...    2.0\n",
       "3826  254158  එක ප්‍රශ්නයක් නම් තියෙනවා අහන්න පිරිමි වැසිකිල...    3.0\n",
       "3827  254158                     නිරෝගිමත් පරපුරක් රටට හිමිවේවි    4.0\n",
       "3828  254158  වැසිකිළියේ තියෙන කරාමයෙන් එන්නෙත් කුසිසියේ කරා...    NaN\n",
       "\n",
       "[3829 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lankadeepa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qm-gaV94U4Ob"
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([lankadeepa_data,gossipLanka_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kxm16KdZVIWo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    4302\n",
       "4.0    1665\n",
       "3.0    1201\n",
       "5.0     995\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Shq4w6GcqbIq"
   },
   "source": [
    "# preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9ynks11qZ-e"
   },
   "outputs": [],
   "source": [
    "# edit this later \n",
    "def text_preprocessing(train_data,test_data):\n",
    "  train_data_texts = train_data['comment']\n",
    "  train_data_labels = train_data['label']\n",
    "  test_data_texts = test_data['comment']\n",
    "  test_data_labels = test_data['label']\n",
    "\n",
    "\n",
    "  comment_texts = []\n",
    "  comment_labels = []\n",
    "\n",
    "  train_text = []\n",
    "  test_text = []\n",
    "  train_labels=[]\n",
    "  test_labels=[]\n",
    "\n",
    "  for label in train_data_labels:\n",
    "    if label == \"POSITIVE\":\n",
    "      train_labels.append(1)\n",
    "    else:\n",
    "      train_labels.append(0)\n",
    "  comment_labels.append(train_labels)\n",
    "\n",
    "  for label in test_data_labels:\n",
    "    if label == \"POSITIVE\":\n",
    "      test_labels.append(1)\n",
    "    else:\n",
    "      test_labels.append(0)\n",
    "  comment_labels.append(test_labels)\n",
    "  \n",
    "\n",
    "  for comment in train_data_texts:\n",
    "    lines = []\n",
    "    try:\n",
    "      words = comment.split()\n",
    "      lines += words\n",
    "    except:\n",
    "      continue\n",
    "    train_text.append(lines)\n",
    "  comment_texts.append(train_text)\n",
    "\n",
    "  for comment in test_data_texts:\n",
    "    lines = []\n",
    "    try:\n",
    "      words = comment.split()\n",
    "      lines += words\n",
    "    except:\n",
    "      continue\n",
    "    test_text.append(lines)\n",
    "  comment_texts.append(test_text)\n",
    "\n",
    "\n",
    "  return comment_texts,comment_labels\n",
    "\n",
    "# edit this later \n",
    "def text_preprocessing_1(data):\n",
    "  comments = data['comment']\n",
    "  labels = data['label']\n",
    "\n",
    "  comments_splitted = []\n",
    "  labels_encoded = []\n",
    "\n",
    "  for label in labels:\n",
    "    if label == \"POSITIVE\":\n",
    "      labels_encoded.append(1)\n",
    "    else:\n",
    "      labels_encoded.append(0)\n",
    "\n",
    "  for comment in comments:\n",
    "    lines = []\n",
    "    try:\n",
    "      words = comment.split()\n",
    "      lines += words\n",
    "    except:\n",
    "      continue\n",
    "    comments_splitted.append(lines)\n",
    "  return comments_splitted,labels_encoded\n",
    "\n",
    "\n",
    "def text_preprocessing_2(data):\n",
    "  comments = data['comment']\n",
    "  labels = data['label']\n",
    "\n",
    "  comments_splitted = []\n",
    "\n",
    "  for comment in comments:\n",
    "    lines = []\n",
    "    try:\n",
    "      words = comment.split()\n",
    "      lines += words\n",
    "    except:\n",
    "      continue\n",
    "    comments_splitted.append(lines)\n",
    "\n",
    "  return comments_splitted,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZoOqtodrUY1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42189\n"
     ]
    }
   ],
   "source": [
    "comment_texts, comment_labels = text_preprocessing_2(all_data)\n",
    "\n",
    "# prepare tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(comment_texts)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMhZbzGur4kt"
   },
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(comment_texts)\n",
    "# print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ho5NTzHtsKoN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   228   109  9473]\n",
      " [    0     0     0 ...     0   828 15117]\n",
      " [    0     0     0 ...  9474  1924  1227]\n",
      " ...\n",
      " [    0     0     0 ...  1426   269  1740]\n",
      " [    0     0     0 ...  2013    55   203]\n",
      " [    0     0     0 ...   475    72  1545]]\n"
     ]
    }
   ],
   "source": [
    "max_length = len(max(encoded_docs, key=len))\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length)\n",
    "print(padded_docs)\n",
    "\n",
    "comment_labels = np.array(comment_labels)\n",
    "padded_docs = np.array(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Drgv_J0ZUV8j"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.2, random_state=0)\n",
    "# X_train = np.array(X_train)\n",
    "# X_test = np.array(X_test)\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)\n",
    "# comment_labels = np.array(comment_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91fqgdk67oaZ"
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPm_9b6Y8EPi"
   },
   "source": [
    "## Generate Embedding Metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOB0YKL-fNW5"
   },
   "outputs": [],
   "source": [
    "def generate_embedding_metrix():\n",
    "  if (embedding_type == 'fasttext'):\n",
    "    word_embedding_model = FastText.load(word_embedding_path)\n",
    "  else:\n",
    "    word_embedding_model = word2vec.Word2Vec.load(word_embedding_path)\n",
    "    \n",
    "  word_vectors = word_embedding_model.wv\n",
    "  word_vectors.save(word_embedding_keydvectors_path)\n",
    "  word_vectors = KeyedVectors.load(word_embedding_keydvectors_path, mmap='r')\n",
    "\n",
    "  embeddings_index = dict()\n",
    "  for word, vocab_obj in word_vectors.vocab.items():\n",
    "    embeddings_index[word]=word_vectors[word]\n",
    "\n",
    "  # create a weight matrix for words in training docs\n",
    "  embedding_matrix = zeros((vocab_size, embedding_size))\n",
    "  for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "\n",
    "  pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVYEOwiN8H3s"
   },
   "source": [
    "## Load Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNSwoRM292-u"
   },
   "outputs": [],
   "source": [
    "def load_word_embedding_atrix():\n",
    "  f = open(embedding_matrix_path, 'rb')\n",
    "  embedding_matrix= np.array(pickle.load(f))\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrmGuYXf82E_"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hx80tScO_ddi"
   },
   "source": [
    "## RNN(LSTM/GRU) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9elMDrR_laX"
   },
   "outputs": [],
   "source": [
    "def build_RNN_model():\n",
    "    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n",
    "    embedding  = Embedding(max_features, embedding_dims,\n",
    "                  weights=[embedding_matrix], input_length=maxlen,\n",
    "                  name='embedding' ,trainable=False)(main_input)\n",
    "\n",
    "    embedding = Dropout(0.50)(embedding)\n",
    "\n",
    "    x = RNN(rnn_output_size)(embedding)\n",
    "\n",
    "    x = Dense(hidden_dims, activation='relu', init='he_normal',\n",
    "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
    "              name='mlp')(x)\n",
    "\n",
    "    x = Dropout(0.10, name='drop')(x)\n",
    "\n",
    "    output = Dense(6, init='he_normal',\n",
    "                   activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(input=main_input, output=output)\n",
    "    model.compile(loss={'output':'sparse_categorical_crossentropy'},\n",
    "                optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
    "                metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt3TZsP84gRK"
   },
   "source": [
    "## CNN+RNN(LSTM /GRU) model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3v2boPz4frA"
   },
   "outputs": [],
   "source": [
    "def build_CNN_RNN_model():\n",
    "    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n",
    "    embedding  = Embedding(max_features, embedding_dims,\n",
    "                  weights=[embedding_matrix], input_length=maxlen,\n",
    "                  name='embedding' ,trainable=False)(main_input)\n",
    "\n",
    "    embedding = Dropout(0.50)(embedding)\n",
    "\n",
    "    conv4 = Convolution1D(nb_filter=nb_filter,\n",
    "                          filter_length=4,\n",
    "                          border_mode='valid',\n",
    "                          activation='relu',\n",
    "                          subsample_length=1,\n",
    "                          name='conv4')(embedding)\n",
    "    maxConv4 = MaxPooling1D(pool_length=2,\n",
    "                             name='maxConv4')(conv4)\n",
    "\n",
    "    conv5 = Convolution1D(nb_filter=nb_filter,\n",
    "                          filter_length=5,\n",
    "                          border_mode='valid',\n",
    "                          activation='relu',\n",
    "                          subsample_length=1,\n",
    "                          name='conv5')(embedding)\n",
    "    maxConv5 = MaxPooling1D(pool_length=2,\n",
    "                            name='maxConv5')(conv5)\n",
    "\n",
    "    x = keras.layers.Concatenate(axis=1)([maxConv4, maxConv5])\n",
    "\n",
    "    x = Dropout(0.15)(x)\n",
    "\n",
    "    x = RNN(rnn_output_size)(x)\n",
    "\n",
    "\n",
    "    x = Dense(hidden_dims, activation='relu', init='he_normal',\n",
    "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
    "              name='mlp')(x)\n",
    "\n",
    "    x = Dropout(0.10, name='drop')(x)\n",
    "\n",
    "    output = Dense(1, init='he_normal',\n",
    "                   activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(input=main_input, output=output)\n",
    "    model.compile(loss={'output':'binary_crossentropy'},\n",
    "                optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7QWHo7RBFYw"
   },
   "source": [
    "## CNN+BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3yWVn8nB2Q6"
   },
   "outputs": [],
   "source": [
    "def build_CNN_BiLSTM():\n",
    "  # main model\n",
    "  input = Input(shape=(maxlen,))\n",
    "  embedding = Embedding(max_features,300,weights=[embedding_matrix],input_length=maxlen)(input)\n",
    "\n",
    "  conv4 = Convolution1D(nb_filter=nb_filter,\n",
    "                          filter_length=4,\n",
    "                          border_mode='valid',\n",
    "                          activation='relu',\n",
    "                          subsample_length=1,\n",
    "                          name='conv4')(embedding)\n",
    "  maxConv4 = MaxPooling1D(pool_length=2,\n",
    "                             name='maxConv4')(conv4)\n",
    "\n",
    "  conv5 = Convolution1D(nb_filter=nb_filter,\n",
    "                          filter_length=5,\n",
    "                          border_mode='valid',\n",
    "                          activation='relu',\n",
    "                          subsample_length=1,\n",
    "                          name='conv5')(embedding)\n",
    "  maxConv5 = MaxPooling1D(pool_length=2,\n",
    "                            name='maxConv5')(conv5)\n",
    "\n",
    "\n",
    "  x = keras.layers.Concatenate(axis=1)([maxConv4, maxConv5])\n",
    "\n",
    "  x = Dropout(0.15)(x)\n",
    "\n",
    "  model =  Bidirectional (LSTM (300,return_sequences=True,dropout=0.8),merge_mode='concat')(x)\n",
    "  model = TimeDistributed(Dense(300,activation='relu'))(model)\n",
    "  model = Flatten()(model)\n",
    "  model = Dense(300,activation='relu')(model)\n",
    "  output = Dense(2,activation='softmax')(model)\n",
    "  model = Model(input,output)\n",
    "  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5nll2IhjLhr"
   },
   "source": [
    "## BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMs9GE8Dlpiw"
   },
   "outputs": [],
   "source": [
    "def build_BiLSTM_1_1():\n",
    "  input = Input(shape=(maxlen,))\n",
    "  embedding = Embedding(max_features,embedding_dims,weights=[embedding_matrix],input_length=maxlen)(input)\n",
    "\n",
    "  model =  Bidirectional (LSTM (300,return_sequences=True,dropout=drop_out_value),merge_mode='concat')(embedding)\n",
    "  model = TimeDistributed(Dense(300,activation='relu'))(model)\n",
    "  model = Flatten()(model)\n",
    "  model = Dense(300,activation='relu')(model)\n",
    "  output = Dense(2,activation='softmax')(model)\n",
    "  model = Model(input,output)\n",
    "  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# final model\n",
    "def build_BiLSTM_1_2():\n",
    "  input = Input(shape=(maxlen,))\n",
    "  embedding = Embedding(max_features,embedding_dims,weights=[embedding_matrix],input_length=maxlen)(input)\n",
    "\n",
    "  model =  Bidirectional (LSTM (300,return_sequences=True,dropout=drop_out_value,kernel_regularizer=l2(0.01)),merge_mode='concat')(embedding)\n",
    "  model = TimeDistributed(Dense(300,activation='relu'))(model)\n",
    "  model = Flatten()(model)\n",
    "  # model = Dense(300,activation='relu')(model)\n",
    "  output = Dense(1,activation='sigmoid')(model)\n",
    "  model = Model(input,output)\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def build_BiLSTM_1_3():\n",
    "  input = Input(shape=(maxlen,))\n",
    "  embedding = Embedding(max_features,embedding_dims,weights=[embedding_matrix],input_length=maxlen)(input)\n",
    "\n",
    "  model =  LSTM (512,return_sequences=True,dropout=drop_out_value,kernel_regularizer=l2(0.01))(embedding)\n",
    "  model = LSTM (256,return_sequences=True,dropout=drop_out_value,kernel_regularizer=l2(0.01))(model)\n",
    "  model = TimeDistributed(Dense(300,activation='relu'))(model)\n",
    "  model = Flatten()(model)\n",
    "  # model = Dense(300,activation='relu')(model)\n",
    "  output = Dense(1,activation='sigmoid')(model)\n",
    "  model = Model(input,output)\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# BiLSTM with dropout regularization\n",
    "def build_BiLSTM_2_1():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, embedding_dims, input_length=maxlen,weights=[embedding_matrix]))\n",
    "  model.add(Bidirectional(LSTM (300,dropout=drop_out_value),merge_mode='concat'))\n",
    "  # model.add(Dropout(drop_out_value))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# BiLSTM with l2 regularization\n",
    "def build_BiLSTM_2_2():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, embedding_dims, input_length=maxlen,weights=[embedding_matrix]))\n",
    "  model.add(Bidirectional(LSTM (300,kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)),merge_mode='concat'))\n",
    "  model.add(Dropout(drop_out_value))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC0ZRhKe5a6C"
   },
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhOLwIC-5aiD"
   },
   "outputs": [],
   "source": [
    "def Train_Test_Model(model,X_train, X_test, y_train, y_test):\n",
    "\n",
    "  print('Training and Testing...')\n",
    "  test_accs = []\n",
    "  first_run = True\n",
    "\n",
    "\n",
    "  acc=[]\n",
    "  val_acc=[]\n",
    "  loss=[]\n",
    "  val_loss=[]\n",
    "  best_val_acc = 0\n",
    "  best_test_acc = 0\n",
    "  for j in range(nb_epoch):\n",
    "      a = time.time()\n",
    "      his = model.fit(X_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      validation_data=[X_test, y_test],\n",
    "                      shuffle=True,\n",
    "                      epochs=1, verbose=verbosity)\n",
    "      acc+=his.history['accuracy']\n",
    "      val_acc+=his.history['val_accuracy']\n",
    "      loss+=his.history['loss']\n",
    "      val_loss+=his.history['val_loss']\n",
    "      # print('Epoch %d/%d\\t%s' % (j + 1, nb_epoch, str(his.history)))\n",
    "      if his.history['val_accuracy'][0] >= best_val_acc:\n",
    "          score, test_acc = model.evaluate(X_test, y_test,\n",
    "                                      batch_size=batch_size,\n",
    "                                      verbose=2)\n",
    "          best_val_acc = his.history['val_accuracy'][0]\n",
    "          best_test_acc = test_acc\n",
    "          print('Got best epoch  best val acc is %f test acc is %f' %\n",
    "                (best_val_acc, best_test_acc))\n",
    "          if len(test_accs) > 0:\n",
    "              print('Current avg test acc:', str(np.mean(test_accs)))\n",
    "      b = time.time()\n",
    "      cost = b - a\n",
    "      left = (nb_epoch - j - 1)\n",
    "      print('One round cost %ds, %d round %ds %dmin left' % (cost, left,\n",
    "                                                            cost * left,\n",
    "                                                            cost * left / 60.0))\n",
    "      test_accs.append(best_test_acc)\n",
    "      predictions = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "  print('Avg test acc:', str(np.mean(test_accs)))\n",
    "  return y_test,predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4DuyDZMkcE3"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TMmqz8BkTQR"
   },
   "outputs": [],
   "source": [
    "def Do_Cross_Validation(model,X,y):\n",
    "\n",
    "  # Define per-fold score containers\n",
    "  acc_per_fold = []\n",
    "  loss_per_fold = []\n",
    "\n",
    "  kfold = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "  fold_no = 1\n",
    "  inputs = X\n",
    "  targets = y\n",
    "  for train, test in kfold.split(inputs, targets):\n",
    "    model = model\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    his = model.fit(inputs[train], targets[train],\n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=True,\n",
    "                  epochs=nb_epoch, \n",
    "                  verbose=verbosity,\n",
    "                  validation_split=validation_split)\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "  # == Provide average scores ==\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print('Score per fold')\n",
    "  for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print('Average scores for all folds:')\n",
    "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "  print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6CXqtIDR4jS"
   },
   "source": [
    "# Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvl487sr8seD"
   },
   "outputs": [],
   "source": [
    "def Plot_graphs():\n",
    "\n",
    "  epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "  #------------------------------------------------\n",
    "  # Plot training and validation accuracy per epoch\n",
    "  #------------------------------------------------\n",
    "  plt.plot(epochs, acc, 'r')\n",
    "  plt.plot(epochs, val_acc, 'b')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  #------------------------------------------------\n",
    "  # Plot training and validation loss per epoch\n",
    "  #------------------------------------------------\n",
    "  plt.plot(epochs, loss, 'r')\n",
    "  plt.plot(epochs, val_loss, 'b')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "\n",
    "  # Expected Output\n",
    "  # A chart where the validation loss does not increase sharply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RS6GyZ8-mk7"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85wsS8Mn8oeI"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../Sentiment Analysis/CNN RNN/embedding_matrix/fasttext_lankadeepa_gossiplanka_300_5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-51d09f109c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# embedding_matrix = generate_embedding_metrix()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word_embedding_atrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-ac35247f9d85>\u001b[0m in \u001b[0;36mload_word_embedding_atrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_word_embedding_atrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0membedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../Sentiment Analysis/CNN RNN/embedding_matrix/fasttext_lankadeepa_gossiplanka_300_5'"
     ]
    }
   ],
   "source": [
    "# embedding_matrix = generate_embedding_metrix()\n",
    "embedding_matrix = load_word_embedding_atrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uueiXUkkWuKl"
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # 64, 128\n",
    "nb_filter = 200\n",
    "filter_length = 4 # test with 2,3,4,5\n",
    "hidden_dims = nb_filter * 2\n",
    "nb_epoch = 2\n",
    "RNN = LSTM \n",
    "rnn_output_size = embedding_size \n",
    "folds = 3\n",
    "maxlen = 210 #test with other values\n",
    "max_features = embedding_matrix.shape[0] #vocab_size\n",
    "embedding_dims = embedding_size\n",
    "drop_out_value = 0.5 #0.8 #0.3\n",
    "verbosity = 1\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PBWe0V1-mK1"
   },
   "outputs": [],
   "source": [
    "model = build_RNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMA3XMXFSj-1"
   },
   "outputs": [],
   "source": [
    "labels, predictions = Train_Test_Model(model,X_train, X_test, y_train, y_test)\n",
    "# Do_Cross_Validation(model,padded_docs,comment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wswC-_eIEo52"
   },
   "outputs": [],
   "source": [
    "# precision, recall, fscore, support = precision_recall_fscore_support(labels, predictions)\n",
    "# print('precision: {}'.format(precision))\n",
    "# print('recall: {}'.format(recall))\n",
    "# print('fscore: {}'.format(fscore))\n",
    "# print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmYPCmuCjbaa"
   },
   "outputs": [],
   "source": [
    "# predictions[2]\n",
    "# y_test[2]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0BYvE-D5QAc0",
    "xXiAlHRZ7X_p",
    "Shq4w6GcqbIq",
    "91fqgdk67oaZ",
    "sPm_9b6Y8EPi",
    "wVYEOwiN8H3s",
    "hx80tScO_ddi",
    "Pt3TZsP84gRK",
    "k7QWHo7RBFYw",
    "P5nll2IhjLhr"
   ],
   "name": "cnn_rnn_with_preprocess_multiclass.ipynb",
   "provenance": [
    {
     "file_id": "153374pBL1OZuAJMWKK_ae0D9_DUq4P0P",
     "timestamp": 1590420423516
    }
   ],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
