{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_rnn_with_preprocess_multiclass.ipynb","provenance":[{"file_id":"153374pBL1OZuAJMWKK_ae0D9_DUq4P0P","timestamp":1590420423516}],"collapsed_sections":["0BYvE-D5QAc0","xXiAlHRZ7X_p","Shq4w6GcqbIq","91fqgdk67oaZ","sPm_9b6Y8EPi","wVYEOwiN8H3s","hx80tScO_ddi","Pt3TZsP84gRK","k7QWHo7RBFYw","P5nll2IhjLhr"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"t6nTBbEWxpxA","colab_type":"text"},"source":["# References\n","https://github.com/ultimate010/crnn \n","{Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts}\n","https://keras.io/examples/imdb_bidirectional_lstm/"]},{"cell_type":"markdown","metadata":{"id":"3zR8-aK009p9","colab_type":"text"},"source":["# Folder Paths"]},{"cell_type":"code","metadata":{"id":"2DEOa06miLE5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594403128777,"user_tz":-330,"elapsed":1836,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["first_time = True"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWPD3dgEqB5d","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594403129386,"user_tz":-330,"elapsed":2387,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["folder_path =  '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/'\n","lankadeepa_data_path = folder_path + 'corpus/new/preprocess_from_isuru/lankadeepa_tagged_comments.csv'\n","gossip_lanka_data_path = folder_path + 'corpus/new/preprocess_from_unicode_values/gossip_lanka_tagged_comments.csv'\n","embedding_size = 400\n","embedding_type = 'word2vec'\n","embedding_type1 = 'word2vec'\n","context = 5\n","word_embedding_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(embedding_size)+\"/\"+embedding_type1+\"_\"+str(embedding_size)+\"_\"+str(context)\n","word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(embedding_size)+\"/keyed_vectors/keyed.kv\"\n","embedding_matrix_path = folder_path + 'Sentiment Analysis/CNN RNN/embedding_matrix/'+embedding_type+'_lankadeepa_gossiplanka_'+str(embedding_size)+'_'+str(context)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0BYvE-D5QAc0","colab_type":"text"},"source":["# Imports Statements"]},{"cell_type":"code","metadata":{"id":"gZ9YDtD6qn4t","colab_type":"code","colab":{}},"source":["if (first_time):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","import collections\n","import pickle\n","import re\n","import random\n","import sys\n","import os \n","import time\n","\n","import gensim\n","from gensim.models.keyedvectors import KeyedVectors\n","from gensim.models.fasttext import FastText\n","from gensim.models import word2vec\n","\n","from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n","\n","from __future__ import print_function\n","\n","import pandas as pd\n","import numpy as np\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","from numpy import cumsum\n","\n","import keras\n","from keras.models import Sequential,Model\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dropout, Activation, Flatten, \\\n","    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n","    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D\n","from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n","from keras.regularizers import l2, l1_l2\n","from keras.constraints import maxnorm\n","from keras import callbacks\n","from keras.utils import generic_utils\n","from keras.optimizers import Adadelta\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXiAlHRZ7X_p","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"GqqHP9rwpbdt","colab_type":"code","colab":{}},"source":["lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n","gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n","gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmw4wplLKgXx","colab_type":"code","colab":{}},"source":["lankadeepa_data.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJhs5dtuLDZA","colab_type":"code","colab":{}},"source":["lankadeepa_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qm-gaV94U4Ob","colab_type":"code","colab":{}},"source":["all_data = pd.concat([lankadeepa_data,gossipLanka_data])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kxm16KdZVIWo","colab_type":"code","colab":{}},"source":["all_data['label'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Shq4w6GcqbIq","colab_type":"text"},"source":["# preprocess Data"]},{"cell_type":"code","metadata":{"id":"y9ynks11qZ-e","colab_type":"code","colab":{}},"source":["# edit this later \n","def text_preprocessing(train_data,test_data):\n","  train_data_texts = train_data['comment']\n","  train_data_labels = train_data['label']\n","  test_data_texts = test_data['comment']\n","  test_data_labels = test_data['label']\n","\n","\n","  comment_texts = []\n","  comment_labels = []\n","\n","  train_text = []\n","  test_text = []\n","  train_labels=[]\n","  test_labels=[]\n","\n","  for label in train_data_labels:\n","    if label == \"POSITIVE\":\n","      train_labels.append(1)\n","    else:\n","      train_labels.append(0)\n","  comment_labels.append(train_labels)\n","\n","  for label in test_data_labels:\n","    if label == \"POSITIVE\":\n","      test_labels.append(1)\n","    else:\n","      test_labels.append(0)\n","  comment_labels.append(test_labels)\n","  \n","\n","  for comment in train_data_texts:\n","    lines = []\n","    try:\n","      words = comment.split()\n","      lines += words\n","    except:\n","      continue\n","    train_text.append(lines)\n","  comment_texts.append(train_text)\n","\n","  for comment in test_data_texts:\n","    lines = []\n","    try:\n","      words = comment.split()\n","      lines += words\n","    except:\n","      continue\n","    test_text.append(lines)\n","  comment_texts.append(test_text)\n","\n","\n","  return comment_texts,comment_labels\n","\n","# edit this later \n","def text_preprocessing_1(data):\n","  comments = data['comment']\n","  labels = data['label']\n","\n","  comments_splitted = []\n","  labels_encoded = []\n","\n","  for label in labels:\n","    if label == \"POSITIVE\":\n","      labels_encoded.append(1)\n","    else:\n","      labels_encoded.append(0)\n","\n","  for comment in comments:\n","    lines = []\n","    try:\n","      words = comment.split()\n","      lines += words\n","    except:\n","      continue\n","    comments_splitted.append(lines)\n","  return comments_splitted,labels_encoded\n","\n","\n","def text_preprocessing_2(data):\n","  comments = data['comment']\n","  labels = data['label']\n","\n","  comments_splitted = []\n","\n","  for comment in comments:\n","    lines = []\n","    try:\n","      words = comment.split()\n","      lines += words\n","    except:\n","      continue\n","    comments_splitted.append(lines)\n","\n","  return comments_splitted,labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZoOqtodrUY1","colab_type":"code","colab":{}},"source":["comment_texts, comment_labels = text_preprocessing_2(all_data)\n","\n","# prepare tokenizer\n","\n","t = Tokenizer()\n","t.fit_on_texts(comment_texts)\n","vocab_size = len(t.word_index) + 1\n","print(vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMhZbzGur4kt","colab_type":"code","colab":{}},"source":["encoded_docs = t.texts_to_sequences(comment_texts)\n","print(encoded_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ho5NTzHtsKoN","colab_type":"code","colab":{}},"source":["max_length = len(max(encoded_docs, key=len))\n","padded_docs = pad_sequences(encoded_docs, maxlen=max_length)\n","print(padded_docs)\n","\n","comment_labels = np.array(comment_labels)\n","padded_docs = np.array(padded_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Drgv_J0ZUV8j","colab_type":"code","colab":{}},"source":["\n","X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.2, random_state=0)\n","# X_train = np.array(X_train)\n","# X_test = np.array(X_test)\n","# y_train = np.array(y_train)\n","# y_test = np.array(y_test)\n","# comment_labels = np.array(comment_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91fqgdk67oaZ","colab_type":"text"},"source":["# Word Embedding"]},{"cell_type":"markdown","metadata":{"id":"sPm_9b6Y8EPi","colab_type":"text"},"source":["## Generate Embedding Metrix"]},{"cell_type":"code","metadata":{"id":"kOB0YKL-fNW5","colab_type":"code","colab":{}},"source":["def generate_embedding_metrix():\n","  if (embedding_type == 'fasttext'):\n","    word_embedding_model = FastText.load(word_embedding_path)\n","  else:\n","    word_embedding_model = word2vec.Word2Vec.load(word_embedding_path)\n","    \n","  word_vectors = word_embedding_model.wv\n","  word_vectors.save(word_embedding_keydvectors_path)\n","  word_vectors = KeyedVectors.load(word_embedding_keydvectors_path, mmap='r')\n","\n","  embeddings_index = dict()\n","  for word, vocab_obj in word_vectors.vocab.items():\n","    embeddings_index[word]=word_vectors[word]\n","\n","  # create a weight matrix for words in training docs\n","  embedding_matrix = zeros((vocab_size, embedding_size))\n","  for word, i in t.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","      embedding_matrix[i] = embedding_vector\n","\n","  pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n","  return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVYEOwiN8H3s","colab_type":"text"},"source":["## Load Embedding Matrix"]},{"cell_type":"code","metadata":{"id":"YNSwoRM292-u","colab_type":"code","colab":{}},"source":["def load_word_embedding_atrix():\n","  f = open(embedding_matrix_path, 'rb')\n","  embedding_matrix= np.array(pickle.load(f))\n","  return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrmGuYXf82E_","colab_type":"text"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"hx80tScO_ddi","colab_type":"text"},"source":["## RNN(LSTM/GRU) model"]},{"cell_type":"code","metadata":{"id":"K9elMDrR_laX","colab_type":"code","colab":{}},"source":["def build_RNN_model():\n","    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n","    embedding  = Embedding(max_features, embedding_dims,\n","                  weights=[embedding_matrix], input_length=maxlen,\n","                  name='embedding' ,trainable=False)(main_input)\n","\n","    embedding = Dropout(0.50)(embedding)\n","\n","    x = RNN(rnn_output_size)(embedding)\n","\n","    x = Dense(hidden_dims, activation='relu', init='he_normal',\n","              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n","              name='mlp')(x)\n","\n","    x = Dropout(0.10, name='drop')(x)\n","\n","    output = Dense(6, init='he_normal',\n","                   activation='softmax', name='output')(x)\n","\n","    model = Model(input=main_input, output=output)\n","    model.compile(loss={'output':'sparse_categorical_crossentropy'},\n","                optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n","                metrics=[\"accuracy\"])\n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pt3TZsP84gRK","colab_type":"text"},"source":["## CNN+RNN(LSTM /GRU) model "]},{"cell_type":"code","metadata":{"id":"m3v2boPz4frA","colab_type":"code","colab":{}},"source":["def build_CNN_RNN_model():\n","    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n","    embedding  = Embedding(max_features, embedding_dims,\n","                  weights=[embedding_matrix], input_length=maxlen,\n","                  name='embedding' ,trainable=False)(main_input)\n","\n","    embedding = Dropout(0.50)(embedding)\n","\n","    conv4 = Convolution1D(nb_filter=nb_filter,\n","                          filter_length=4,\n","                          border_mode='valid',\n","                          activation='relu',\n","                          subsample_length=1,\n","                          name='conv4')(embedding)\n","    maxConv4 = MaxPooling1D(pool_length=2,\n","                             name='maxConv4')(conv4)\n","\n","    conv5 = Convolution1D(nb_filter=nb_filter,\n","                          filter_length=5,\n","                          border_mode='valid',\n","                          activation='relu',\n","                          subsample_length=1,\n","                          name='conv5')(embedding)\n","    maxConv5 = MaxPooling1D(pool_length=2,\n","                            name='maxConv5')(conv5)\n","\n","    x = keras.layers.Concatenate(axis=1)([maxConv4, maxConv5])\n","\n","    x = Dropout(0.15)(x)\n","\n","    x = RNN(rnn_output_size)(x)\n","\n","\n","    x = Dense(hidden_dims, activation='relu', init='he_normal',\n","              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n","              name='mlp')(x)\n","\n","    x = Dropout(0.10, name='drop')(x)\n","\n","    output = Dense(1, init='he_normal',\n","                   activation='sigmoid', name='output')(x)\n","\n","    model = Model(input=main_input, output=output)\n","    model.compile(loss={'output':'binary_crossentropy'},\n","                optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n","                metrics=[\"accuracy\"])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7QWHo7RBFYw","colab_type":"text"},"source":["## CNN+BiLSTM"]},{"cell_type":"code","metadata":{"id":"W3yWVn8nB2Q6","colab_type":"code","colab":{}},"source":["def build_CNN_BiLSTM():\n","  # main model\n","  input = Input(shape=(maxlen,))\n","  embedding = Embedding(max_features,300,weights=[embedding_matrix],input_length=maxlen)(input)\n","\n","  conv4 = Convolution1D(nb_filter=nb_filter,\n","                          filter_length=4,\n","                          border_mode='valid',\n","                          activation='relu',\n","                          subsample_length=1,\n","                          name='conv4')(embedding)\n","  maxConv4 = MaxPooling1D(pool_length=2,\n","                             name='maxConv4')(conv4)\n","\n","  conv5 = Convolution1D(nb_filter=nb_filter,\n","                          filter_length=5,\n","                          border_mode='valid',\n","                          activation='relu',\n","                          subsample_length=1,\n","                          name='conv5')(embedding)\n","  maxConv5 = MaxPooling1D(pool_length=2,\n","                            name='maxConv5')(conv5)\n","\n","\n","  x = keras.layers.Concatenate(axis=1)([maxConv4, maxConv5])\n","\n","  x = Dropout(0.15)(x)\n","\n","  model =  Bidirectional (LSTM (300,return_sequences=True,dropout=0.8),merge_mode='concat')(x)\n","  model = TimeDistributed(Dense(300,activation='relu'))(model)\n","  model = Flatten()(model)\n","  model = Dense(300,activation='relu')(model)\n","  output = Dense(2,activation='softmax')(model)\n","  model = Model(input,output)\n","  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5nll2IhjLhr","colab_type":"text"},"source":["## BiLSTM "]},{"cell_type":"code","metadata":{"id":"GMs9GE8Dlpiw","colab_type":"code","colab":{}},"source":["def build_BiLSTM_1_1():\n","  input = Input(shape=(maxlen,))\n","  embedding = Embedding(max_features,embedding_dims,weights=[embedding_matrix],input_length=maxlen)(input)\n","\n","  model =  Bidirectional (LSTM (300,return_sequences=True,dropout=drop_out_value),merge_mode='concat')(embedding)\n","  model = TimeDistributed(Dense(300,activation='relu'))(model)\n","  model = Flatten()(model)\n","  model = Dense(300,activation='relu')(model)\n","  output = Dense(2,activation='softmax')(model)\n","  model = Model(input,output)\n","  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n","  return model\n","\n","# final model\n","def build_BiLSTM_1_2():\n","  input = Input(shape=(maxlen,))\n","  embedding = Embedding(max_features,embedding_dims,weights=[embedding_matrix],input_length=maxlen)(input)\n","\n","  model =  Bidirectional (LSTM (300,return_sequences=True,dropout=drop_out_value,kernel_regularizer=l2(0.01)),merge_mode='concat')(embedding)\n","  model = TimeDistributed(Dense(300,activation='relu'))(model)\n","  model = Flatten()(model)\n","  # model = Dense(300,activation='relu')(model)\n","  output = Dense(1,activation='sigmoid')(model)\n","  model = Model(input,output)\n","  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n","  return model\n","\n","def build_BiLSTM_1_3():\n","  input = Input(shape=(maxlen,))\n","  embedding = Embedding(max_features,embedding_dims,weights=[embedding_matrix],input_length=maxlen)(input)\n","\n","  model =  LSTM (512,return_sequences=True,dropout=drop_out_value,kernel_regularizer=l2(0.01))(embedding)\n","  model = LSTM (256,return_sequences=True,dropout=drop_out_value,kernel_regularizer=l2(0.01))(model)\n","  model = TimeDistributed(Dense(300,activation='relu'))(model)\n","  model = Flatten()(model)\n","  # model = Dense(300,activation='relu')(model)\n","  output = Dense(1,activation='sigmoid')(model)\n","  model = Model(input,output)\n","  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n","  return model\n","\n","# BiLSTM with dropout regularization\n","def build_BiLSTM_2_1():\n","  model = Sequential()\n","  model.add(Embedding(max_features, embedding_dims, input_length=maxlen,weights=[embedding_matrix]))\n","  model.add(Bidirectional(LSTM (300,dropout=drop_out_value),merge_mode='concat'))\n","  # model.add(Dropout(drop_out_value))\n","  model.add(Dense(1, activation='sigmoid'))\n","\n","  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","  return model\n","\n","# BiLSTM with l2 regularization\n","def build_BiLSTM_2_2():\n","  model = Sequential()\n","  model.add(Embedding(max_features, embedding_dims, input_length=maxlen,weights=[embedding_matrix]))\n","  model.add(Bidirectional(LSTM (300,kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)),merge_mode='concat'))\n","  model.add(Dropout(drop_out_value))\n","  model.add(Dense(1, activation='sigmoid'))\n","\n","  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WC0ZRhKe5a6C","colab_type":"text"},"source":["# Train and Test"]},{"cell_type":"code","metadata":{"id":"LhOLwIC-5aiD","colab_type":"code","colab":{}},"source":["def Train_Test_Model(model,X_train, X_test, y_train, y_test):\n","\n","  print('Training and Testing...')\n","  test_accs = []\n","  first_run = True\n","\n","\n","  acc=[]\n","  val_acc=[]\n","  loss=[]\n","  val_loss=[]\n","  best_val_acc = 0\n","  best_test_acc = 0\n","  for j in range(nb_epoch):\n","      a = time.time()\n","      his = model.fit(X_train, y_train,\n","                      batch_size=batch_size,\n","                      validation_data=[X_test, y_test],\n","                      shuffle=True,\n","                      epochs=1, verbose=verbosity)\n","      acc+=his.history['accuracy']\n","      val_acc+=his.history['val_accuracy']\n","      loss+=his.history['loss']\n","      val_loss+=his.history['val_loss']\n","      # print('Epoch %d/%d\\t%s' % (j + 1, nb_epoch, str(his.history)))\n","      if his.history['val_accuracy'][0] >= best_val_acc:\n","          score, test_acc = model.evaluate(X_test, y_test,\n","                                      batch_size=batch_size,\n","                                      verbose=2)\n","          best_val_acc = his.history['val_accuracy'][0]\n","          best_test_acc = test_acc\n","          print('Got best epoch  best val acc is %f test acc is %f' %\n","                (best_val_acc, best_test_acc))\n","          if len(test_accs) > 0:\n","              print('Current avg test acc:', str(np.mean(test_accs)))\n","      b = time.time()\n","      cost = b - a\n","      left = (nb_epoch - j - 1)\n","      print('One round cost %ds, %d round %ds %dmin left' % (cost, left,\n","                                                            cost * left,\n","                                                            cost * left / 60.0))\n","      test_accs.append(best_test_acc)\n","      predictions = model.predict(X_test, batch_size=batch_size, verbose=0)\n","\n","  print('Avg test acc:', str(np.mean(test_accs)))\n","  return y_test,predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u4DuyDZMkcE3","colab_type":"text"},"source":["# Cross Validation"]},{"cell_type":"code","metadata":{"id":"4TMmqz8BkTQR","colab_type":"code","colab":{}},"source":["def Do_Cross_Validation(model,X,y):\n","\n","  # Define per-fold score containers\n","  acc_per_fold = []\n","  loss_per_fold = []\n","\n","  kfold = KFold(n_splits=folds, shuffle=True)\n","\n","  fold_no = 1\n","  inputs = X\n","  targets = y\n","  for train, test in kfold.split(inputs, targets):\n","    model = model\n","\n","    # Generate a print\n","    print('------------------------------------------------------------------------')\n","    print(f'Training for fold {fold_no} ...')\n","\n","    # Fit data to model\n","    his = model.fit(inputs[train], targets[train],\n","                  batch_size=batch_size,\n","                  shuffle=True,\n","                  epochs=nb_epoch, \n","                  verbose=verbosity,\n","                  validation_split=validation_split)\n","    \n","    # Generate generalization metrics\n","    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n","    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n","    acc_per_fold.append(scores[1] * 100)\n","    loss_per_fold.append(scores[0])\n","\n","    # Increase fold number\n","    fold_no = fold_no + 1\n","\n","  # == Provide average scores ==\n","  print('------------------------------------------------------------------------')\n","  print('Score per fold')\n","  for i in range(0, len(acc_per_fold)):\n","    print('------------------------------------------------------------------------')\n","    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n","  print('------------------------------------------------------------------------')\n","  print('Average scores for all folds:')\n","  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","  print(f'> Loss: {np.mean(loss_per_fold)}')\n","  print('------------------------------------------------------------------------')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6CXqtIDR4jS","colab_type":"text"},"source":["# Plot Graphs"]},{"cell_type":"code","metadata":{"id":"fvl487sr8seD","colab_type":"code","colab":{}},"source":["def Plot_graphs():\n","\n","  epochs=range(len(acc)) # Get number of epochs\n","\n","  #------------------------------------------------\n","  # Plot training and validation accuracy per epoch\n","  #------------------------------------------------\n","  plt.plot(epochs, acc, 'r')\n","  plt.plot(epochs, val_acc, 'b')\n","  plt.title('Training and validation accuracy')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","  plt.figure()\n","\n","  #------------------------------------------------\n","  # Plot training and validation loss per epoch\n","  #------------------------------------------------\n","  plt.plot(epochs, loss, 'r')\n","  plt.plot(epochs, val_loss, 'b')\n","  plt.title('Training and validation loss')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Loss\")\n","  plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","  plt.figure()\n","\n","\n","  # Expected Output\n","  # A chart where the validation loss does not increase sharply!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RS6GyZ8-mk7","colab_type":"text"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"85wsS8Mn8oeI","colab_type":"code","colab":{}},"source":["embedding_matrix = generate_embedding_metrix()\n","# embedding_matrix = load_word_embedding_atrix()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uueiXUkkWuKl","colab_type":"code","colab":{}},"source":["batch_size = 32 # 64, 128\n","nb_filter = 200\n","filter_length = 4 # test with 2,3,4,5\n","hidden_dims = nb_filter * 2\n","nb_epoch = 15\n","RNN = LSTM \n","rnn_output_size = embedding_size \n","folds = 3\n","maxlen = 210 #test with other values\n","max_features = embedding_matrix.shape[0] #vocab_size\n","embedding_dims = embedding_size\n","drop_out_value = 0.5 #0.8 #0.3\n","verbosity = 1\n","validation_split = 0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PBWe0V1-mK1","colab_type":"code","colab":{}},"source":["model = build_RNN_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMA3XMXFSj-1","colab_type":"code","colab":{}},"source":["labels, predictions = Train_Test_Model(model,X_train, X_test, y_train, y_test)\n","# Do_Cross_Validation(model,padded_docs,comment_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wswC-_eIEo52","colab_type":"code","colab":{}},"source":["# precision, recall, fscore, support = precision_recall_fscore_support(labels, predictions)\n","# print('precision: {}'.format(precision))\n","# print('recall: {}'.format(recall))\n","# print('fscore: {}'.format(fscore))\n","# print('support: {}'.format(support))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmYPCmuCjbaa","colab_type":"code","colab":{}},"source":["# predictions[2]\n","# y_test[2]"],"execution_count":null,"outputs":[]}]}